{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89eb9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d00b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.21.4)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: datasets==1.0.2 in /opt/conda/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.26.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.21.4)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (6.0.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (0.3.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (4.62.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers==4.24.0 in /opt/conda/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.10.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (1.21.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.0.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformer-utils in /opt/conda/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (1.9.1+cu111)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (0.11.2)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.24.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.62.3)\n",
      "Requirement already satisfied: colorcet in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (3.0.1)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /opt/conda/lib/python3.9/site-packages (from colorcet->transformer-utils) (0.4.8)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.21.4)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.3.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (3.4.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->transformer-utils) (4.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (3.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.10.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (3.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (8.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->seaborn->transformer-utils) (2021.3)\n",
      "Requirement already satisfied: param>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from pyct>=0.4.4->colorcet->transformer-utils) (1.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (1.26.12)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn->transformer-utils) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (0.13.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.11.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "!pip install datasets==1.0.2\n",
    "!pip install transformers==4.24.0\n",
    "!pip install transformer-utils\n",
    "!pip install packaging\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c13816af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset\n",
    "\n",
    ")\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import EncoderDecoderModel\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f147dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = \"gogamza/kobart-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38bc4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_textfile_path = \"data/train_text.csv\"\n",
    "train_summaryfile_path = \"data/train_summary.csv\"\n",
    "\n",
    "with open(train_textfile_path, encoding=\"utf-8\") as f:\n",
    "            train_textlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "with open(train_summaryfile_path, encoding=\"utf-8\") as f:\n",
    "            train_sumlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "860b4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_textfile_path = \"data/val_text.csv\"\n",
    "val_summaryfile_path = \"data/val_summary.csv\"\n",
    "\n",
    "with open(val_textfile_path, encoding=\"utf-8\") as f:\n",
    "            val_textlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "with open(val_summaryfile_path, encoding=\"utf-8\") as f:\n",
    "            val_sumlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "906c546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(zip(train_textlines, train_sumlines), columns=['Text', 'Summary'])\n",
    "val_df = pd.DataFrame(zip(val_textlines, val_sumlines), columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20431a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop([train_df.index[0]])\n",
    "val_df = val_df.drop([val_df.index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "806cbf78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ 아 그 왕칫솔 또 사려...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>잠도안오네ㅐ얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "1  그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일...   \n",
       "2  kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...   \n",
       "3  아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...   \n",
       "4  칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ 아 그 왕칫솔 또 사려...   \n",
       "5  잠도안오네ㅐ얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...   \n",
       "\n",
       "                                             Summary  \n",
       "1                 비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다  \n",
       "2                 비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다  \n",
       "3  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  \n",
       "4                칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다  \n",
       "5                   잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df57e1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Text, Summary]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['Text'].str.contains(r'[鈐]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91df577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train_df) // 4\n",
    "train_data = Dataset.from_pandas(train_df[:train_len]) \n",
    "val_len = len(val_df) // 2\n",
    "val_data = Dataset.from_pandas(val_df[:val_len])\n",
    "test_data=Dataset.from_pandas(val_df[val_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95d24c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None), '__index_level_0__': Value(dtype='int64', id=None)}, num_rows: 69998)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None), '__index_level_0__': Value(dtype='int64', id=None)}, num_rows: 17502)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None), '__index_level_0__': Value(dtype='int64', id=None)}, num_rows: 17502)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80744009",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 128\n",
    "max_target = 128\n",
    "batch_size = 4\n",
    "\n",
    "#\"digit82/kobart-summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7124da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "  #get all the dialogues\n",
    "  inputs = [dialogue for dialogue in data_to_process['Text']]\n",
    "  #tokenize the dialogues\n",
    "  model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n",
    "  #tokenize the summaries\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(data_to_process['Summary'], max_length=max_target, padding='max_length', truncation=True)\n",
    "    \n",
    "  #set labels\n",
    "  model_inputs['labels'] = targets['input_ids']\n",
    "  #return the tokenized data\n",
    "  #input_ids, attention_mask and labels\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bf1281d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def02c9761864548ac6d133d5f4f2ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c445102c3c034fedaa7bb87aeb440138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e15e98da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "353d3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set special tokens\n",
    "# #from transformers import EncoderDecoderConfig\n",
    "# model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "# model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 128 # 256은 쿠다 메모리 오류 생김\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 2\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7817dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "    rouge_output2 = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    rouge_outputL = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rougeL\"])[\"rougeL\"].mid\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"rouge1_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge1_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge1_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "        \n",
    "        \"rouge2_precision\": round(rouge_output2.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output2.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output2.fmeasure, 4), \n",
    "        \n",
    "        \"rougeL_precision\": round(rouge_outputL.precision, 4),\n",
    "        \"rougeL_recall\": round(rouge_outputL.recall, 4),\n",
    "        \"rougeL_fmeasure\": round(rouge_outputL.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03edd664",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results221115_2\",\n",
    "    num_train_epochs=3,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    "    max_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39deb606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \\n또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\\ntokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
    "\"\"\"\n",
    "DataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \n",
    "또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\n",
    "tokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66d3c501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41f390de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: __index_level_0__, token_type_ids. If __index_level_0__, token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 69998\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 123859968\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/Korean_Conversation_Summary/wandb/run-20221116_071958-2duflkh5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/2duflkh5\" target=\"_blank\">results221115_2</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.934900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.972600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=5.9537754821777344, metrics={'train_runtime': 59.9427, 'train_samples_per_second': 26.692, 'train_steps_per_second': 1.668, 'total_flos': 121947291648000.0, 'train_loss': 5.9537754821777344, 'epoch': 0.02})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d06fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97662fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /aiffel/.cache/huggingface/hub/models--gogamza--kobart-base-v2/snapshots/d9a1f640896cef8dcfd693b1bc57510a2b09a18f/config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /aiffel/.cache/huggingface/hub/models--gogamza--kobart-base-v2/snapshots/d9a1f640896cef8dcfd693b1bc57510a2b09a18f/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at gogamza/kobart-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 128 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)# 여기에 기본 kobart가져오기?\n",
    "import random\n",
    "from random import randrange\n",
    "test_samples = test_data.select(range(16))# 0, len(test_data), 200\n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1] # 여기에 체크포인트 가져오기 \n",
    "# 연구해봐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c9367f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_0 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 잼을 팔면 자면 바쳐지지만 자격이 안 된다鈐긴다.\n",
      " 알아서\n",
      "\n",
      "Target summary \n",
      " 곧 유료화되는 웹툰을 보고 재미있으면 말해주겠다\n",
      "\n",
      "Text 웹툰볼그야ㅋㅋ 안자면바야지 곧유료화되는것들 먼저봐야지 이런영웅은싫어 괜찮다던데 17일인가유료화 그게머지 첨들어보는데 나도몰랏는데 인기글에서봤어 웅ㅋㅋㅋ 보고 재밋으면말해준다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 번개 장터에 가서 물어본다고 한다鈐빔\n",
      "\n",
      "Target summary \n",
      " 번개장터인데 피큐어 상세 샷을 고양이 위에 올려놓고 찍어서 귀엽다고 장사 잘 되겠다고 한다\n",
      "\n",
      "Text 시스템 사진 이거바 번개장터인데 이사람 피규어상세샷 고양이 위에 올려노코찍음 ㅋㅋㅋㅋㅋ기엽닼ㅋㅋㅋㅋㅋ 털바바ㅋㅋㅋ고양이맞네 ㅣㄱㄱㄱㄱ짱귀욥다 zzzzzzxxx저걸 가만히 있어줬네 잘때 찍었나봐ㅋㅋ 사진완전 잘찍었네ㅋㅋㅋㅋ장사잘되겠다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_2 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 단점 술이 빨리 안 팔려서인지 모르겠다鈐애플탯î\n",
      "\n",
      "Target summary \n",
      " 오늘 달력을 주문해서 내일 제작이 끝나니 나오면 바로 가져다주기로 약속하고 있다\n",
      "\n",
      "Text 달력 빨리 나오면 내일 갖다주께 ㅋㅋㅋㅋㅋㅋㅋㅋ오오 언제 줄지 모르겠음 오늘 주문해서 제작은 내일끝날건디 금요일아직 이틀남음 바로 주시려나 아 오늘 수욜이네 ㄲㅋㅋㅋㅋ 수요일밖에 안되다니ㅜㅜㅜ 구니까ㅜㅜ\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_3 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 이런 것을 아는 애가 젤 나음이라고 한다볶았다볶군지알거워서 알거같다고 한다 나눌 훨\n",
      "\n",
      "Target summary \n",
      " 트롯이다에 나오는 애들이 느끼하다고 하면서 어린아이들이 제일 낫다고 하고 있다\n",
      "\n",
      "Text 트롯이다보니 나오는 애들 죄다 박현빈st로 끼떨어 늬끼하게ㅠㅠㅠ 차라리 보기시러ㅜ그런거 초딩 노래잘하는 애가 젤나음 나누군지알거같아 걔 알거같애ㅋㅋㅋㅋㅋㅋ 어린애들이 젤 나음 ㅡㅡ\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_4 \n",
      "Summary before \n",
      " 이게뭐야 이게 먼저야 내 방에 향수로 도배된거같아 진짜 끔\n",
      "\n",
      "Summary after \n",
      " 내 방에 내 방향을 향수로 도배된거 같다고 해서 지금 한차례 붓고 있다고 한다鈐鈐긴다.\n",
      " 가입자\n",
      "\n",
      "Target summary \n",
      " 캔들을 많이 만들고 있어서 방이 향수로 도배된 공방이 된 것 같다\n",
      "\n",
      "Text 이게뭐야 이게 먼저야 내 방에 향수로 도배된거같아 진짜 끔찍 캔들너무많아 지금 공방됐어 지금 한차례 붓고왔어 몰드에 아니 무슨 캔들을 그렇게 또 정도가 없이 만들고 있어 몰라 지금 개웃겨 지금 한 아이디어스냐구요 캔들만드는거만 3일넘게 하고있는거같아\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_5 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 커피를 걸을지 아직도 안 걸리고 있다鈐)보다\n",
      "\n",
      "Target summary \n",
      " 오늘 마을을 한 바퀴 돌며 걷는 것에 대해 이야기한다\n",
      "\n",
      "Text 과연오늘 얼마나걸을지 아직도 걷는중 한참남았눙 ㅎㅎ 그렇게 머루 긴코스 강남읍한바퀴 마을한바퀴여 ㅎㅎ 마을이 넘커\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_6 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 셔츠 잼을 고칠 수 있다 )보다ィ\n",
      "\n",
      "Target summary \n",
      " 넷플릭스 영화를 시간 때우기 용으로 추천한다\n",
      "\n",
      "Text 시스템 사진 넷플ㅋㅋㅋㅋㅋ추천이요ㅋㅋㅋㅋ 그냥 킬링타임용으로 볼만한듯ㅋㅋㅋㅋ 명대사라 캡쳐해봄ㅋㅋㅋㅋㅋㅋㅋ 헤어질때 저말하면 개웃길듯ㅋㅋㅋㅋㅋㅋㅋㅋㅋ 기타 ㅋㅋㅋㅋ 황금같은 가임기 ㅋㅋㅋㅋㅋㅋㅋ 저런말은 어떻게 생각하는거여ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 그니까 ㅋㅋㅋㅋㅋㅋㅋㅋㅋ 방금 대사 나오는데 저 주인공이 남자들은 강한 여자를 좋아한다고 하지만 막상 원하는건 치어리더야 라는데\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_7 \n",
      "Summary before \n",
      " 본방ᄏᄏᄏᄏ 진짜 무섭제 ᄏᄏᄏᄏᄏ어 진짜\n",
      "\n",
      "Summary after \n",
      " 사수을을 사주겠다한다한다鈐묏\n",
      "\n",
      "Target summary \n",
      " 숨을 참고 볼 만큼 악귀가 너무 무섭고 몰입도가 굉장히 높다\n",
      "\n",
      "Text 본방사수한다 시스템 사진 같이 달리자 와 방금 숨 참고 봤네 ㅋㅋㅋㅋ악귀 진짜 무섭제ㅠㅠㅠ ㅋㅋㅋㅋ어 진짜 목소리 바뀔때 완전 무서워 하 재밌엉 ㅠㅠ 몰입도 장난아니다ㅠ\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_8 \n",
      "Summary before \n",
      " 대신 일 많이 해서 몸이 자유로워지고 의욕이 생긴대 대신 일 많이 해서 몸이\n",
      "\n",
      "Summary after \n",
      " 일 때문에 마시고 일 시작하나보다와서 일하기 전에 일해본다고 한다긴다.\n",
      "鈐 같다\"고\n",
      "\n",
      "Target summary \n",
      " 일을 많이 해서 몸이 피곤한 대신 생각이 자유로워지고 의욕이 생긴다고 하자 좋은 카드라고 한다\n",
      "\n",
      "Text 생각이 자유로워지고 의욕이 생긴대 대신 일 많이 해서 몸이 피곤한가보다 그런가보다 오 미루던 일을 드디어 하는구나 오 좋은 카드네 이제 일 시작하나보다 내년에 본격적으로 혼자 열심히 해야지\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_9 \n",
      "Summary before \n",
      " 놀았음 허ᅡ장실 가야지 쉬루 ᄏᄏᄏᄏᄏᄏ\n",
      "\n",
      "Summary after \n",
      " 쉬고 싶고 떨리는 것 같다고 한다ィfont 것이기빔\n",
      "\n",
      "Target summary \n",
      " 엑소의 노래를 듣다가 익숙한 휘파람 소리가 들려 이어폰을 빼고 들었다\n",
      "\n",
      "Text 놀았음 허ㅏ장실 가야지 쉬루 ㅋㅋㅋㅋㅋㅋㅋ 개웃기다 뭐하곡 놀게 있니 엑소노래 하나 나와서 노래듣는데 익숙한 휘바람이 들리는거야 그래서 바로이어폰 빼고 들넜지 노래들어서 굿 좋겠당ㅋㅋㅋㅋ나있을떈 안나오던뎅\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_10 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 전하의 전하에게 전하는 것이 아니라 전하를 전하라고 한다)보다\n",
      "\n",
      "Target summary \n",
      " 영상을 보며 좋아하는 캐릭터에 대하여 이야기한다\n",
      "\n",
      "Text 좀비는 현실이 아니니까 괜찮을거야 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 전하가 사라져 죽고싶지않으면 어서 찾아내게 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 내가 거기서 가장 좋아하는 캐릭터는 범팔이야 범며들었엌ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ내가 빨리 보면서 내 최애도 찾아볼게 심장이 두근두근 이름 야 만약 오늘 이거 같이 봤잖아 나 찐으루 오늘 음료 두번 쏟았다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_11 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 복불복이라 생각한다鈐떄\n",
      "\n",
      "Target summary \n",
      " 나무 특성상 복불복이라고 생각했지만 이런 사진과는 또 비슷해서 괜찮은 것 같으면서도 진한 것 같기도 하다\n",
      "\n",
      "Text 나무 특성상 복불복이라 생각햇지만 시스템 사진 이런거랑은 또 비슷하거든 ㅎ 흠저렇게 보면 또 괜찮은데 그렇긴한데 생각보다 진한거 같기도하고\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_12 \n",
      "Summary before \n",
      " 리는데 아 오늘 궁금  그날 가서 봐야할듯 ᄏᄏᄏ 죠\n",
      "\n",
      "Summary after \n",
      " 셔츠을을 안 쌌다고 한다더라.\n",
      "鈐묏\n",
      "\n",
      "Target summary \n",
      " 오늘은 안 눌러진다고 그날 가서 봐야 할듯하다고 이야기하고 있다\n",
      "\n",
      "Text 시스템 기타 오 뭘까 안눌리더라 다른건 눌리는데 아 오늘 궁금 ㅜㅜ 그날 가서 봐야할듯 ㅋㅋ 죠르디 소환했으니까 당일날 확인해보자 아ㅋㅋㅋㅋ 오늘인줄 순간\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_13 \n",
      "Summary before \n",
      " 이럴 때 이럴 이럴 이럴 이럴 이럴 이럴 때 산으로 들어가서 쉬는 건데 마저\n",
      "\n",
      "Summary after \n",
      " 나처럼 귀농하고 싶고 싶다고 한다)보다 저렴하게 살 것 같다 운영할鈐\n",
      "\n",
      "Target summary \n",
      " 이런 시기에 산으로 들어가서 귀농을 하고 싶다\n",
      "\n",
      "Text 이럴 때 산으로 들어가서 쉬는 건데 마저 나 요즘 귀농하고 싶자너 ㅋ 동숲으로 습득한 스킬로 님 뭔가 잘할 듯 ㅇㅈ ㅎㅎ 동숲스킬 무시 못하지 ㅋ\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_14 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 유배배오면 우리처럼 빔을 만들자며 운동 헬스도 하고싶다고 하고 싶다고 한다반병도 한다鈐ィ\n",
      "\n",
      "Target summary \n",
      " 헬스에 다니자고 했는데 검도를 배우고 싶다고 한다\n",
      "\n",
      "Text 이름 유배오면 우리같이 헹스 다미자 운동 헬스요 나 합기듀 하고싶어 아니아니 검도 이야 이름 이한테 배워 머리 ㅋㅋㅋㅋㅋㅋㅋㅋㅌㅋㅋㅋ 아맞다이 가이 검도인가 후덜덜\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_15 \n",
      "Summary before \n",
      " ᄏᄏ 설마 내한공연 오는 가수들 공연 ᄏᄏᄏᄏᄏᄏ\n",
      "\n",
      "Summary after \n",
      " 이동 이동 단지에 대해 이야기한다한다 훨 복무\n",
      "\n",
      "Target summary \n",
      " 내한공연 오는 가수들 공연에서 떼를 지어 노래하는 영상을 맨날 본 거 또 보고 있는데 거기에 있는 거 마냥 벅차오르고 가슴이 웅장해진다\n",
      "\n",
      "Text 어제 하루종일 떼창 영상 봣음ㅋㅋ 설마 내한공연 오는 가수들 공연 ㅋㅋㅋㅋㅋㅋ 당연ㅋㅋㅋㅋ본 거 또보고 또봐도 맨날 재밋음ㅋㅋㅋ 나도 좋아해ㅋㅋㅋㅋㅋ 가슴이 웅장해짐요 내가 거기에 있는거마냥ㅋㅋㅋㅋ 벅차오름 인정\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(summaries_after_tuning)):\n",
    "    print('idx_{} '.format(i))\n",
    "    print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fb797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15f2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4330d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3806cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e594f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03111e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10077c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda1410e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d769ac71",
   "metadata": {},
   "source": [
    "- 웬지 Text Summary를 따로 LineByLineTextDataset을 해줘야할 것 같다.\n",
    "- 그리고 특수 기호가 많이 들어가 있어 이부분도 제거해줘야할 것 같다.\n",
    "- 결론은 전처리를 적용하고 각 train, val별로 Text, Summary따로 LineByLineTextdataset을 적용해줘야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ae92f",
   "metadata": {},
   "source": [
    "## 데이터 전처리 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f79690",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_df.csv')\n",
    "val_df = pd.read_csv('data/val_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6878bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True, drop=True)\n",
    "val_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727e407b",
   "metadata": {},
   "source": [
    "## train, val별로 Text, summary 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9670ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df['Text'].to_csv('data/train_text.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c6969df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df['Summary'].to_csv('data/train_summary.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "690eeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_df['Text'].to_csv('data/val_text.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce8b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_df['Summary'].to_csv('data/val_summary.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4962c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = \"gogamza/kobart-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cc6eb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일이야 16일 아 이름 아 너 나한테 돈 보내주면 지금 할 수 잇옹 얼마야 최종 결제액이 잠시만 인당 952 900 합쳐서 1 905 800 근데 나중에 특가 뜰 수도 있으려나 좀 더 두고볼까 뜨기야 뜨겠지 웅웅 보니까 아시아나는 특가 이벤트 꽤 하는 것 같아서 일단 두고보장 그래 구럼 일단 자자'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('data/train_text.csv')\n",
    "test['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d34ae7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_text_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/train_text.csv\",\n",
    "    block_size=128, # how many lines to read at a time \n",
    ")\n",
    "\n",
    "train_summary_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/train_summary.csv\",\n",
    "    block_size=128, # how many lines to read at a time \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d757329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/val_text.csv\",\n",
    "    block_size=128, # how many lines to read at a time \n",
    ")\n",
    "\n",
    "val_summary_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/val_summary.csv\",\n",
    "    block_size=128, # how many lines to read at a time \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99441ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Text'] = train_text_dataset\n",
    "train_df['Summary'] = train_summary_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52e3a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['Text'] = val_text_dataset\n",
    "val_df['Summary'] = val_summary_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408aec26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd8593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92569c18",
   "metadata": {},
   "outputs": [],
   "source": [
    " def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\n",
    "        warnings.warn(\n",
    "            DEPRECATION_WARNING.format(\n",
    "                \"https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\"\n",
    "            ),\n",
    "            FutureWarning,\n",
    "        )\n",
    "        if os.path.isfile(file_path) is False:\n",
    "            raise ValueError(f\"Input file path {file_path} not found\")\n",
    "        # Here, we do not cache the features, operating under the assumption\n",
    "        # that we will soon use fast multithreaded tokenizers from the\n",
    "        # `tokenizers` repo everywhere =)\n",
    "        logger.info(f\"Creating features from dataset file at {file_path}\")\n",
    "\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\n",
    "        self.examples = batch_encoding[\"input_ids\"]\n",
    "        self.examples = [{\"input_ids\": torch.tensor(e, dtype=torch.long)} for e in self.examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c620c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af34762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encode 토크나이저 \n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f934ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "  #get all the dialogues\n",
    "  inputs = [dialogue for dialogue in data_to_process['Text']]\n",
    "  #tokenize the dialogues\n",
    "  model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n",
    "  #tokenize the summaries\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(data_to_process['Summary'], max_length=max_target, padding='max_length', truncation=True)\n",
    "    \n",
    "  #set labels\n",
    "  model_inputs['labels'] = targets['input_ids']\n",
    "  #return the tokenized data\n",
    "  #input_ids, attention_mask and labels\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71e13543",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1090/2008327742.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_tokenize_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_tokenize_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "train_tokenize_data = train_df.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_df.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51eec85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c2894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b7f0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3394bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "    rouge_output2 = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    rouge_outputL = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rougeL\"])[\"rougeL\"].mid\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"rouge1_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge1_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge1_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "        \n",
    "        \"rouge2_precision\": round(rouge_output2.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output2.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output2.fmeasure, 4), \n",
    "        \n",
    "        \"rougeL_precision\": round(rouge_outputL.precision, 4),\n",
    "        \"rougeL_recall\": round(rouge_outputL.recall, 4),\n",
    "        \"rougeL_fmeasure\": round(rouge_outputL.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75e57a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=True,mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfaf85dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4771781",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results221115\",\n",
    "    num_train_epochs=1,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    "    max_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a42a30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=val_df,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5e48d47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 279992\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 123859968\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639d8cbe88d74474b433b9c71eabddbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669601433265294, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/Korean_Conversation_Summary/wandb/run-20221115_140502-3kicxeb9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/3kicxeb9\" target=\"_blank\">results221115</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "128448",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 128448",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1090/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m         )\n\u001b[0;32m-> 1501\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1502\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 128448"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d976ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec12f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9c75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6764d63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = \"gogamza/kobart-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f386584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('data/train_df.csv', index = False)\n",
    "# val_df.to_csv('data/val_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb76865b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['웅', '영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙...</td>\n",
       "      <td>팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['너는 잘가라....회사.... 선택 잘해..', '알겠어 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 많...</td>\n",
       "      <td>이제 이력서를 쓰고 영어도 해야 한다고 해서 첫 회사를 잘 들어가라고 했다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['느낌상 대통령까지는 아니고 오시면 여사님정도오시지않을까 ㅋㅋㅋ', '그러면서',...</td>\n",
       "      <td>느낌상 대통령까지는 아니고 오시면 여사님 정도 오시지 않을까라며 이에 대해 이야기하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['숨만수이ㅓ도 숨만쉬어도 100 이내', '한달안에 일 무조건 해야대', '아 딱...</td>\n",
       "      <td>한 달 안에 무조건 일을 시작해서 돈을 벌어야 한다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['목요일은 외근이구 금요일은 출장!!!', '금요일이 당진이양?', '아닝아닝 1...</td>\n",
       "      <td>목요일에 외근이고 금요일에 출장인데 당진은 10일에 간다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  ['웅', '영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙...   \n",
       "1  ['너는 잘가라....회사.... 선택 잘해..', '알겠어 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 많...   \n",
       "2  ['느낌상 대통령까지는 아니고 오시면 여사님정도오시지않을까 ㅋㅋㅋ', '그러면서',...   \n",
       "3  ['숨만수이ㅓ도 숨만쉬어도 100 이내', '한달안에 일 무조건 해야대', '아 딱...   \n",
       "4  ['목요일은 외근이구 금요일은 출장!!!', '금요일이 당진이양?', '아닝아닝 1...   \n",
       "\n",
       "                                             Summary  \n",
       "0     팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다.  \n",
       "1         이제 이력서를 쓰고 영어도 해야 한다고 해서 첫 회사를 잘 들어가라고 했다.  \n",
       "2  느낌상 대통령까지는 아니고 오시면 여사님 정도 오시지 않을까라며 이에 대해 이야기하...  \n",
       "3                      한 달 안에 무조건 일을 시작해서 돈을 벌어야 한다.  \n",
       "4                   목요일에 외근이고 금요일에 출장인데 당진은 10일에 간다.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train_df.csv')\n",
    "val_df = pd.read_csv('data/val_df.csv')\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "354668d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/train_df.csv\",\n",
    "    block_size=128, # how many lines to read at a time \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "105b0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/val_df.csv\",\n",
    "    block_size=128, # how many lines to read at a time \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e7b7d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([15054, 23795,   315,   243,   282,   316, 20461, 15562,   320])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552bbf14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61904ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d13e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27e3c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=True,mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe7efeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07de73eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "705899db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set special tokens\n",
    "# #from transformers import EncoderDecoderConfig\n",
    "# model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "# model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 128 # 256은 쿠다 메모리 오류 생김\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 2\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b84a932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "    rouge_output2 = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    rouge_outputL = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rougeL\"])[\"rougeL\"].mid\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"rouge1_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge1_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge1_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "        \n",
    "        \"rouge2_precision\": round(rouge_output2.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output2.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output2.fmeasure, 4), \n",
    "        \n",
    "        \"rougeL_precision\": round(rouge_outputL.precision, 4),\n",
    "        \"rougeL_recall\": round(rouge_outputL.recall, 4),\n",
    "        \"rougeL_fmeasure\": round(rouge_outputL.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d37668c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results221115\",\n",
    "    num_train_epochs=1,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    "    max_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b947037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16438a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 280026\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 123859968\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/Korean_Conversation_Summary/wandb/run-20221115_084328-3ce5qj5e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/3ce5qj5e\" target=\"_blank\">results221115</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:48, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.403900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=4.868830413818359, metrics={'train_runtime': 56.3023, 'train_samples_per_second': 28.418, 'train_steps_per_second': 1.776, 'total_flos': 121947291648000.0, 'train_loss': 4.868830413818359, 'epoch': 0.01})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7afc326",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_df = pd.read_csv('data/val_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "087a7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "\n",
    "\n",
    "test_val_df=Dataset.from_pandas(test_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82a91ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /aiffel/.cache/huggingface/hub/models--gogamza--kobart-base-v2/snapshots/d9a1f640896cef8dcfd693b1bc57510a2b09a18f/config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /aiffel/.cache/huggingface/hub/models--gogamza--kobart-base-v2/snapshots/d9a1f640896cef8dcfd693b1bc57510a2b09a18f/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at gogamza/kobart-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 128 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_target = 128\n",
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)# 여기에 기본 kobart가져오기?\n",
    "import random\n",
    "from random import randrange\n",
    "test_samples = test_val_df.select(range(16))# 0, len(test_data), 200\n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1] # 여기에 체크포인트 가져오기 \n",
    "# 연구해봐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ff3ada7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_0 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " \"['아니다' '아니 이거 이거 또 또래 많이가도 되게 되게 많이 많이해서',''웅 그래서거거도아거니고 3일가는데 좀하는데 좀가서 좀해 뭐 뭐다 뭐한다구 뭐야',''', '이거 뭐 해봤자 3일 다 합해도 몇만원인데 몇몇몇 몇이인데인데 하는데 그거 그거인데는데는데 뭐 하나인데이고인데 너무해 그때는 2일인데 그 때에서 자꾸 자꾸 자라고... 넘' '' '아래래 뭐하는데하는데 자꾸자라고에서 계속 자려고하는데 거기서 거기서에서 걔 뭐 많이하는데 그럼\n",
      "\n",
      "Target summary \n",
      " 팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다.\n",
      "\n",
      "Text ['웅', '영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소로 엄청 싸워서', '웅 흥 4개월가는거도아니고 3일가는데 좀해주지 거 얼마한다구', '내말이... 아니 해봤자 3일 다 합해도 몇만원 차이인데 너무해 그때는 2일이었는데도 만원 더 싼데에서 자꾸 자라고... 넘 시러..', '일단']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " \"[',''',',' '''', '이력서 쓰고 영어도 해야해..', '여.....', '우리  똥.................................................'... '........', '나''...'...' '...''.....'......'..... '좋은 시절 다 지났다... '...'다'' ''그' '나'너는는......... '....회사....'.... 그럼... 그럼.....\"... 나..!!',...' ᄏᄏ ', '아'나 나...... \\ᄏ'ᄏ...ᄏ 'ᄏ\n",
      "\n",
      "Target summary \n",
      " 이제 이력서를 쓰고 영어도 해야 한다고 해서 첫 회사를 잘 들어가라고 했다.\n",
      "\n",
      "Text ['너는 잘가라....회사.... 선택 잘해..', '알겠어 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 많이 힘들구나... 나도 이제 이력서쓰고 영어도 해야해..', '우리 똥갱애지.... 화이팅.... 첫 회사 잘들어가야한다!', '좋은 시절 다 지났다..']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_2 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " \"['그럼요 오시면 여사님정도오시지않을까 ᄏᄏ','' 여기있었구낭',' '내가 옆에서 옆에서 서포트 한다고 해서 그래서 그래서.... 그래서 뭐뭐뭐가 ᄒᄒ',', '어르르들랑 설명 설명 다들 다 하는데', '그럼 그럼 다 다 하시면 되겠넹', '나''', '아'다''   '니까니까 다하시면', '이니까' '아래다다 하시시면', '오시면 다다 다 하셨으면',다'다' ''네 오빠 오빠 다오시면 그럼 그럼 나사님은정도 오시시\n",
      "\n",
      "Target summary \n",
      " 느낌상 대통령까지는 아니고 오시면 여사님 정도 오시지 않을까라며 이에 대해 이야기하고 있다.\n",
      "\n",
      "Text ['느낌상 대통령까지는 아니고 오시면 여사님정도오시지않을까 ㅋㅋㅋ', '그러면서', '샘 여기있었구낭ㅋ', '종이강사랑 자기가 옆에서 서포트 한다는 식으로 ㅋㅋㅋ', '벌써 시나리오가 ㅋㅋㅋ', '어르신들 못하는 설명 자기가 한다는 식으로 ㅋㅋㅋㅋ', '그럼 본인이 다 하시면 되겠넹', '그니까요 ㅋㅋㅋ']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_3 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " \"[' '''', '아',',''', '그럼 그럼 그럼',' '그냥 뭐야', '이거거네네 그럼 뭐하는데 '돈은 있는거야 그거 그거할꺼?', '어', '일 무조건 일해야하는데 일 해야 하는데''아,,', '여 '다','다'' '아 '아아아', '나']]''이''['##이름만.......대이이니까..', '한달안에 일 무조건 하면',,'',다' '냥..' '이렇게해도  냥냥 일만 하면...', '안', 이거\n",
      "\n",
      "Target summary \n",
      " 한 달 안에 무조건 일을 시작해서 돈을 벌어야 한다.\n",
      "\n",
      "Text ['숨만수이ㅓ도 숨만쉬어도 100 이내', '한달안에 일 무조건 해야대', '아 딱한달', '그냥 아무코토 하면 안돼', '돈은 있는거네 무슨일할꺼?', '웅 그니까 15일 이후로는 무조건 일해야데 몰라 아직,,', '하 고민이당']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_4 \n",
      "Summary before \n",
      " '' '''''''', '아닝아닝 10일이 당진이\n",
      "\n",
      "Summary after \n",
      " \"['아'', ''' '이름?', '아닝아닝 10일이 당진이야', '그럼 그럼은 어디가가...',', '이''', '오',' '나', 오홍?']', '나'  '다'' '아', '여''이'다'''아''이' '주' '이', '일요일은 외근이이구 ', '내 '거', '지'..', '어느이거이니까...은은이네',럼은 '..'이 ''이이',, '아니아너' = =', '201 '금요??\n",
      "\n",
      "Target summary \n",
      " 목요일에 외근이고 금요일에 출장인데 당진은 10일에 간다.\n",
      "\n",
      "Text ['목요일은 외근이구 금요일은 출장!!!', '금요일이 당진이양?', '아닝아닝 10일이 당진이야', '그럼 금요일은 어디로가...?', '의왕! 철기연으로 가', '오홍?']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_5 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " \"[' '',''' '아니깐 나 진짜 진짜 회사 다니기 싫으면 면접 들어가고 싶어서 그래서 그랴 그그리고 또 그  쁘ᄏᄏ',','' 그럼 그럼 그한테 물어보는게 ᄏ 되게 빠르르'', '아 '아' '그럼 그렇게 그렇게 따지듯 할 필요가 있 있네 그럼 그거 그거 이거 되게 되게 그렇게 쫌 이렇게 해서 걔걔  되 되어서', '나' 이거''아'이거 이거 그거 그걸 그걸 되게 많이 많이 물어보면 되네 ᄋᄏ ', '그럼 나' 그 '그럼 그럼 진짜로 더 많이\n",
      "\n",
      "Target summary \n",
      " 그만 둘 전임자에게 면접 자리에서 따지듯이 문의하는 면접자에 대해 이야기한다.\n",
      "\n",
      "Text ['참 나ㅡㅡ 아니 회사 다니기 싫으면 면접 오질 말던갘ㅋㅋㅋ 어이가 없네ㅋㅋㅋ 그리고 전임자한테 왜 그랴 그만두는 애를ㅋㅋㅋ 회사에 물어봐야짘ㅋㅋㅋ', '사실 전임자한테 물어보는게 빠르긴하짘ㅋㅋㅋㅋㅋㅋㅋㅋ 치고 빠지기는', '아니 그건 그런데 그렇게 따지듯 할 필요가 있냐곸ㅋㅋㅋ', 'ㅋㅋㅋㅋㅋㅋㅋ 그러게 ㅋㅋㅋㅋ', '따로 살짝 물어보면 되짘ㅋㅋㅋ', '나도 면접 들어가고 싶어서 들어간게 아니야 얘야', '그리고 보통 전임자도 지가 빠질라고 얘기 잘 안해주던뎈ㅋㅋㅋ 면접자리면 다 있는데섴ㅋㅋㅋ']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_6 \n",
      "Summary before \n",
      " '메일 주소 알려주세여 제가 점심먹고와서 보낼게여', '\n",
      "\n",
      "Summary after \n",
      " \"[''메일 주소 알려주세여, 제가 점심먹고와서 보낼게여',''',',' '#@이름#씨 원티비 Q티비에비 엣 사진 사진사진 # #씨 보네여' 나임'네', '아 '아' '아 '' 사진'', '이름하고하고서 할거야', '그럼..고서서할게  쪄쪄서 빨리 빨리빨리빨리야야서 보내주거거여서''이름먹고 또 또래서 낼게 여', '여 ''와'응응이로 빨리서아서 또 빨리\n",
      "\n",
      "Target summary \n",
      " 점심을 먹고 와서 알려준 메일 주소로 롯데 원티비 큐에이(QA) 담당자에게 메일을 보냈다.\n",
      "\n",
      "Text ['메일 주소 알려주세여 제가 점심먹고와서 보낼게여', 'ㄴ넵넵!! #@시스템#사진# #@URL# 롯데 원티비 QA담당님입니다!', '네에 #@이름#씨 보냇어여', '넵 감사합니다!']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_7 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " \"[ '아니 그럼 그럼제제 그제인계받지 인수인계를받고받고어케...', '아..ᄒ 제송함다 제가 뭘 너무 너무 되게 되게네네여', '앗 나너너 나에있어?? 나아너에 나..거네그럼 나 나 너 나나너무나나에에거거진거 나랑거 ','''진저거랑진틀틀럼거서 그런가?']',', '이 ''' '아 '아니 그럼 그럼 낼인데인데 개강 첫날이라서서......그래서?',\n",
      "\n",
      "Target summary \n",
      " 일주일 동안 인수인계 받을 예정인데 과 사무실보다는 사랑방처럼 학생들이 많이 찾아온다.\n",
      "\n",
      "Text ['그럼 낼부터 출근해?', '아니이번주동안인수인계받지 인수인계를하루받고어케일해', '아..ㅎ 제송함다 제가 뭘 너무 몰랏네여 헤', '앗 나너무싸가지없게말햇더? 쏘리 근데오늘학생들엄청찾아왓다햇자나 그래서인수인계받은것도없어^^ 과사무실이아니라사랑방인줄', '오늘 개강 첫날이라서 그런가?']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_8 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " \"['나도 일한다는데 시급 1.5배 맞나요', ''''' 뭐야 이런건 요구해야지 근로계약서 안써지 뭐 뭐 그런건 줘야함', '아직도 안썼냐 그럼 그럼요'거네',' '아니 그럼 안그래도 계약서도 쓰고 보너스도 더 더할라구여',', '이 '럼 그럼 그 똘똘이'  걔네''아아아 아주 아주 쫌이거니구만']', '그럼 뭐 젠 젤 벅 '거 나 나 이거 너 나있다구요~',\n",
      "\n",
      "Target summary \n",
      " 근로계약서를 써야한다고 하며 크리스마스 때 일하는 것의 시급을 1.5배해달라고 할 것을 이야기하고 있다.\n",
      "\n",
      "Text ['나 크리스마스떄도 일한다는데 시급 1.5배 맞나요', '어 1.5배 요구해', '어후 근로계약서 써야딩..', '하모 그런건 요구해야지 근로계약서 안써도 그런건 줘야함', '아직도 안썼냐ㅎㅎ 근로계약서 빨리 써라', '넹넹 안그래도 계약서도 쓰고 보너스도 더 달라할라구여', '막내 아주 똘똘하구만']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_9 \n",
      "Summary before \n",
      " '고용노동부랑 전화하는데', 'ᄋᄋᄋ', '이중취업 하셨나\n",
      "\n",
      "Summary after \n",
      " \"['노동부랑 전화하는데', 'ᄋᄋ', '이중취업 하셨나요? 이러는거야 맨'''',',' '제가 선생님 실업급여 가능하신지 자격 먼저 먼저 확인할게요', '아래서 선생님님 이중취업이신가요.? 아래서 무슨 말씀이시죠...?']'' [' '우리' '이'다' '('(' '....''이랑 통화하는데' ', '우리''이거니까니까',  '다'' '아 '아이고' 그럼', 뭐'래', '여' 이거 '이건 뭐 하셨어요', '그 '이론 많이 많이 하셨\n",
      "\n",
      "Target summary \n",
      " 고용노동부와 전화하는데 이중 취업을 했는지 묻고 실업급여 자격을 확인하겠다고 말했다.\n",
      "\n",
      "Text ['고용노동부랑 전화하는데', 'ㅇㅇㅇ', '이중취업 하셨나요? 이러는거야 맨첨에', '헤에', '제가 선생님 실업급여 가능하신지 자격 먼저 확인할게여~ 이러더니 아니 그런데 선생님 이중취업이신가요.? 아래서 무슨 말씀이시죠...?']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_10 \n",
      "Summary before \n",
      " .  돈이 얼마노.. 근데 4대 보험 포함이란 말이 저게 세전금액이라는\n",
      "\n",
      "Summary after \n",
      " \"[' \"[ '학교에서 6개월짜리 합격하면 조캣다.. 합격 하면거거  럼....','''',', '최' '..'' '아래.. 180 받는 독서실 되겐네,,', '아 '저거..겅고문 보니가..... ᄅᄋᄋ 수요조사고고 6월에 서류 내고 면접가지 본대.. 하.. ', '그럼 필기 없는게...',' '그런......이런게 뭐.....', '이 '럼... 그럼...??', '나 '적적성 없네~~',, '면접이라면\n",
      "\n",
      "Target summary \n",
      " 학교에서 4대 보험 포함이고 세금 떼도 최소 180만 원은 될 거 같은 6개월짜리 합격했으면 좋겠지만 서류 내고 면접까지 본다고 한다.\n",
      "\n",
      "Text ['학교에서 6개월짜리 합격했으면 조캣다.. 합격하면 맛난거 쏩니다.. 샹 돈이 얼마노.. 근데 4대보험 포함이란 말이 저게 세전금액이라는건가? 세금 떼도 짭잘 하겠느..', '최소 180은 되겠누.. 180 받는 독서실 되겐네,,', '저거 겅고문 보니가 이건 ㄹㅇ 수요조사고 6월에 서류 내고 면접가지 본대.. 하 면접스터디 각 ㅜ', '그래도 필기 없는게 어디 ㅎ', '인적성 없노~!', 'Pt면접이라면...?!']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_11 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " \"['아침에 안나가면 이 자리를 안 나가면 나랑거거야',''ᄋᄋᄅ'' '아래래서 나 이거야 진짜 진짜', '아니 진짜' '할 수 있을 때 해야 됨', '그럼럼 쫌 더있 때야  쪄서서 이러고 진짜 놔서 이거 이거진거인데인데 이거 진짜로인데 껴놨잖아'',' '나 '아침 일어나지럼 나 진짜 갑자기 갑자기 진짜 이게 뭐야 이거지' 나 나래 쳤잖아', '나 ''래 '다'다'' '그래야에 깬거래\n",
      "\n",
      "Target summary \n",
      " 오늘 안 나가면 좋은 자리를 뺏길 것 같았다며 피곤해도 가기를 잘 한 것 같다고 한다.\n",
      "\n",
      "Text ['아침에 깬거거든 피곤해서 갈까말까 고민햇는데 지금 이거 계약기간이라 개바빠서 내가오늘 안나가면 이 자리를 뺏길거같은거야', 'ㅇㅇㅇㅇ 그리고 이런 좋은자리는', '그래서 길게보자 이러고 진짜', '할 수 있을 때 해야 됨', '나 진짜 잘려햇는데 진짜 갑자기 맘바껴서 일어났잖아']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_12 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " \"[' \"['''' '', '거봐서 그거거 나랑그거랑 나 나 바로출근 가능하다했거 쫌' '아 '아'',' 'ᄏᄏ'ᄏ  ᄋᄏ',', '이름#' 뭐? 근데 이거 뭐뭐?? 그럼 나한테도 얘기했어어?', '나'그럼 그럼 그럼 나 좀 뭐 뭐야?'그럼', '그 '럼럼 그럼 또 좀 나야지', 그럼 '야 '다'', '아 좀 놀아야지....', '어 '어거진 ᅲ','와 '내가 인애\n",
      "\n",
      "Target summary \n",
      " 업무 설명 겸 부른 느낌이었고 소장님이 인생 계획을 물어보셨지만 대답을 하지 못했다.\n",
      "\n",
      "Text ['잘받더#@이름#?', '머르개써 근데뭔가 이미정해져4고 그젼에 업무설명겸 부른느낌 나 바로출근 가능하다했믄데 내일부터 나오라하진않겎지 덜덜쓰 내일 일잇는거까머금', 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 뭐물어봐써? 소장님이랑ㅇ도 얘기했어?', '야 #@이름#아 좀 놀아야지ㅠ', '웅 소장님이 인셍계획 물어봄 ; 대답못함 ㅠ;;;', 'ㅋㅋㅋㄴㅋㅋㅋㅋㅋㅋㅋ 뭔 ㅠ 하루살이들 한테 인생계획;;']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_13 \n",
      "Summary before \n",
      " '무무슨부서야', '협력센터데 뭐 병원에 환자의뢰하고이런\n",
      "\n",
      "Summary after \n",
      " \"['''''', '응응'데 뭐 뭐뭐뭐?','  '걔네네 뭐든 입사든든에 뭐든지 입사하면',', 일단 일단 뭐든가든가든'다'', '다 '다워야하니까']' ['ews'...' '다''s'?' 나'>'자료'..' '나't'네' 있'햇' '개 '...''무슨부서야', '우리''이' 뭐네'뭐 그네', 뭐 근데 뭐', '아 '뭐 뭐에', '그'그 뭐 저기 뭐야', '이런저런\n",
      "\n",
      "Target summary \n",
      " 본인의 기존 분야와 다른 부서로 입사하게 되었다.\n",
      "\n",
      "Text ['무슨부서야', '협력센터데 뭐 병원에 환자의뢰하고이런거같아', '오  어렵군 너분야랑은 좀 다르지??', '글치 근데뭐 어디든 입사하면 새로배워야하니까']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_14 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " \"['#@이름# 3d 이미지 하나만 만들어 줄수있어',''' 오빠 오빠한테 부탁하지마.. 오빠가 하구있있정 어거거지루 ᄒᄒ 그림판으로 될수 있을거 같음 오빠 컴터 잘해!',', '무슨 3 4일을 오빠 아버지한테 시켜',' '네', '아''그렸다', '그림징', 오빠는 '디자이너 뽑으으라...'' 나한테...', '오 오빠, '에다 ᄅᄅ이틀 샵## 쫌 하나, 하나 하나도만만\n",
      "\n",
      "Target summary \n",
      " 오빠에게 쓰리디(3D) 이미지 작업 일을 부탁하고 있다.\n",
      "\n",
      "Text ['#@이름# 3d 이미지 하나만 만들어 줄수있엉?', '회사일 나한테 부탁하지마.. 오빠관련된거 아니면', '오빠가 하구있정 어거지루 ㅎㅎ 그림판으로 될수있을거 같엥 오빠 컴터 잘해!', '무슨 3d일을 오빠한테 시켜', '잘그렸징', '디자이너 뽑으라해']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_15 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " \"['''''',' 다' '다'다''  '네',','래 '래 ᄅ', '원래래'!!'~'다']']] [' '일당''일'일당'...' '', '여', '그 '렁'?''아 '.... ᄃᄃ', '.. '르르 '다. '앗 쫌 ᄋ..네'네다' 엥'그걸' '아 '아' 이거 '쪄쪄 쪄', 이상 빕 '에 m2 'ᄃ'ᄅ ', '아 냥 '...' 쟈 ᅡ\n",
      "\n",
      "Target summary \n",
      " 일당은 원래 십만 원인데 허브는 조금 더 줘서 11만 원이다.\n",
      "\n",
      "Text ['일당 얼마여', '딱 그런듯 나 ㅁㄹ', '엥', '원래 십만원 11만원이다~생각했는디 허브는 쫌더준대']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(summaries_after_tuning)):\n",
    "    print('idx_{} '.format(i))\n",
    "    print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de51186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e529a889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  ['그럼 날짜는 가격 큰 변동 없으면 6.28-7.13로 확정할까?', '우리 비행...   \n",
       "1  ['Kf마스크만 5부제 하는거지?', '응. 면마스크는 아무때나 사도될껀?', '면...   \n",
       "2  ['아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애'...   \n",
       "3  ['칫솔사야하는데 쓱으로 살까?', '뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ', '아 그...   \n",
       "4  ['잠도안오네ㅐ얼릉 고구마츄 먹고싶단', '그게 그렇게 맛있었어??? 아주 여보 빼...   \n",
       "\n",
       "                                             Summary  \n",
       "0               비행기 표 가격에 대해 이야기하며, 특가 이벤트를 기다리고 있다.  \n",
       "1                비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다.  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  \n",
       "3            칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계(쓱) 가자고 했다.  \n",
       "4                  잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다.  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b2d35cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['웅', '영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙...</td>\n",
       "      <td>팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['너는 잘가라....회사.... 선택 잘해..', '알겠어 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 많...</td>\n",
       "      <td>이제 이력서를 쓰고 영어도 해야 한다고 해서 첫 회사를 잘 들어가라고 했다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['느낌상 대통령까지는 아니고 오시면 여사님정도오시지않을까 ㅋㅋㅋ', '그러면서',...</td>\n",
       "      <td>느낌상 대통령까지는 아니고 오시면 여사님 정도 오시지 않을까라며 이에 대해 이야기하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['숨만수이ㅓ도 숨만쉬어도 100 이내', '한달안에 일 무조건 해야대', '아 딱...</td>\n",
       "      <td>한 달 안에 무조건 일을 시작해서 돈을 벌어야 한다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['목요일은 외근이구 금요일은 출장!!!', '금요일이 당진이양?', '아닝아닝 1...</td>\n",
       "      <td>목요일에 외근이고 금요일에 출장인데 당진은 10일에 간다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  ['웅', '영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙...   \n",
       "1  ['너는 잘가라....회사.... 선택 잘해..', '알겠어 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 많...   \n",
       "2  ['느낌상 대통령까지는 아니고 오시면 여사님정도오시지않을까 ㅋㅋㅋ', '그러면서',...   \n",
       "3  ['숨만수이ㅓ도 숨만쉬어도 100 이내', '한달안에 일 무조건 해야대', '아 딱...   \n",
       "4  ['목요일은 외근이구 금요일은 출장!!!', '금요일이 당진이양?', '아닝아닝 1...   \n",
       "\n",
       "                                             Summary  \n",
       "0     팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다.  \n",
       "1         이제 이력서를 쓰고 영어도 해야 한다고 해서 첫 회사를 잘 들어가라고 했다.  \n",
       "2  느낌상 대통령까지는 아니고 오시면 여사님 정도 오시지 않을까라며 이에 대해 이야기하...  \n",
       "3                      한 달 안에 무조건 일을 시작해서 돈을 벌어야 한다.  \n",
       "4                   목요일에 외근이고 금요일에 출장인데 당진은 10일에 간다.  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "704cf729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_len = len(train_df) // 4\n",
    "train_data = Dataset.from_pandas(train_df[:train_len]) \n",
    "val_len = len(val_df) // 2\n",
    "val_data = Dataset.from_pandas(val_df[:val_len])\n",
    "test_data=Dataset.from_pandas(val_df[val_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24dbd396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 69998)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 17502)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 17502)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53f2b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 128\n",
    "max_target = 128\n",
    "batch_size = 4\n",
    "\n",
    "#\"digit82/kobart-summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36b6315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "  #get all the dialogues\n",
    "  inputs = [dialogue for dialogue in data_to_process['Text']]\n",
    "  #tokenize the dialogues\n",
    "  model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n",
    "  #tokenize the summaries\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(data_to_process['Summary'], max_length=max_target, padding='max_length', truncation=True)\n",
    "    \n",
    "  #set labels\n",
    "  model_inputs['labels'] = targets['input_ids']\n",
    "  #return the tokenized data\n",
    "  #input_ids, attention_mask and labels\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de93cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d59dc792c8a4733b8cad96f9122c3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d4ac2acc79476b909d3ed9e335ceed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9697e69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "883d9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set special tokens\n",
    "# #from transformers import EncoderDecoderConfig\n",
    "# model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "# model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 128 # 256은 쿠다 메모리 오류 생김\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 2\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87bf555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "    rouge_output2 = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    rouge_outputL = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rougeL\"])[\"rougeL\"].mid\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"rouge1_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge1_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge1_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "        \n",
    "        \"rouge2_precision\": round(rouge_output2.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output2.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output2.fmeasure, 4), \n",
    "        \n",
    "        \"rougeL_precision\": round(rouge_outputL.precision, 4),\n",
    "        \"rougeL_recall\": round(rouge_outputL.recall, 4),\n",
    "        \"rougeL_fmeasure\": round(rouge_outputL.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33d3f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results221115\",\n",
    "    num_train_epochs=1,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    "    max_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d87b935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \\n또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\\ntokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
    "\"\"\"\n",
    "DataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \n",
    "또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\n",
    "tokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04b112f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "497ff02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 69998\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 123859968\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/Korean_Conversation_Summary/wandb/run-20221115_075519-3sr32gmm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/3sr32gmm\" target=\"_blank\">results221115</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.974600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=5.930811614990234, metrics={'train_runtime': 56.9912, 'train_samples_per_second': 28.075, 'train_steps_per_second': 1.755, 'total_flos': 121947291648000.0, 'train_loss': 5.930811614990234, 'epoch': 0.02})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93ba218c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17502\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='1094' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  37/1094 01:23 < 40:39, 0.43 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_266/2732109216.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     def predict(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2796\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2797\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2973\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2974\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2975\u001b[0m             \u001b[0minputs_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_inputs_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mgeneration_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         generated_tokens = self.model.generate(\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mgeneration_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m             )\n\u001b[1;32m   1576\u001b[0m             \u001b[0;31m# 12. run beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m             return self.beam_search(\n\u001b[0m\u001b[1;32m   1578\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2764\u001b[0m             )  # (batch_size * num_beams, vocab_size)\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2766\u001b[0;31m             \u001b[0mnext_token_scores_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2767\u001b[0m             \u001b[0mnext_token_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_token_scores_processed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeam_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/generation_logits_process.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/generation_logits_process.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbanned_tokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbanned_batch_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbanned_tokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "245a7fe1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /aiffel/.cache/huggingface/hub/models--gogamza--kobart-base-v2/snapshots/d9a1f640896cef8dcfd693b1bc57510a2b09a18f/config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /aiffel/.cache/huggingface/hub/models--gogamza--kobart-base-v2/snapshots/d9a1f640896cef8dcfd693b1bc57510a2b09a18f/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at gogamza/kobart-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 128 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)# 여기에 기본 kobart가져오기?\n",
    "import random\n",
    "from random import randrange\n",
    "test_samples = test_data.select(range(16))# 0, len(test_data), 200\n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1] # 여기에 체크포인트 가져오기 \n",
    "# 연구해봐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7fc46ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_0 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 셔츠을을 사고 싶다고 한다.\n",
      "\n",
      "Target summary \n",
      " 곧 유료화되는 웹툰을 보고 재미있으면 말해주겠다.\n",
      "\n",
      "Text ['웹툰볼그야??ㅋㅋ', '안자면바야지 곧유료화되는것들 먼저봐야지 이런영웅은싫어 괜찮다던데? 17일인가유료화', '그게머지 첨들어보는데', '나도몰랏는데 인기글에서봤어', '웅ㅋㅋㅋ', '보고 재밋으면말해준다~~']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 고양이이 위에 올려놓고 노코샷을 하고 있다.\n",
      "\n",
      "Target summary \n",
      " 번개장터인데 피큐어 상세 샷을 고양이 위에 올려놓고 찍어서 귀엽다고 장사 잘 되겠다고 한다.\n",
      "\n",
      "Text ['#@시스템#사진# 이거바 번개장터인데 이사람 피규어상세샷 고양이 위에 올려노코찍음', 'ㅋㅋㅋㅋㅋ기엽닼ㅋㅋㅋㅋㅋ 털바바ㅋㅋㅋ고양이맞네', 'ㅣㄱㄱㄱㄱ짱귀욥다', 'Zzzzzzxxx저걸 가만히 있어줬네', '잘때 찍었나봐ㅋㅋ', '사진완전 잘찍었네ㅋㅋㅋㅋ장사잘되겠다']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_2 \n",
      "Summary before \n",
      " '달력 빨리 나오면 내일 갖다주께', 'ᄏᄏᄏᄏᄏᄏᄏ\n",
      "\n",
      "Summary after \n",
      " 빨리 나오면 빨리 빨리 나와서 빨리 나올 것 같다고 한다.\n",
      "\n",
      "Target summary \n",
      " 오늘 달력을 주문해서 내일 제작이 끝나니 나오면 바로 가져다주기로 약속하고 있다.\n",
      "\n",
      "Text ['달력 빨리 나오면 내일 갖다주께', 'ㅋㅋㅋㅋㅋㅋㅋㅋ오오', '언제 줄지 모르겠음 오늘 주문해서 제작은 내일끝날건디', '금요일아직 이틀남음', '바로 주시려나 아 오늘 수욜이네', 'ㄲㅋㅋㅋㅋ 수요일밖에 안되다니ㅜㅜㅜ', '구니까ㅜㅜ']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_3 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 젤트롯이다보니 나오는 애들 죄다 박현빈st로 끼워서 끼떨어 늬엄을 튕겨서 잤다.\n",
      "\n",
      "Target summary \n",
      " 트롯이다에 나오는 애들이 느끼하다고 하면서 어린아이들이 제일 낫다고 하고 있다.\n",
      "\n",
      "Text ['트롯이다보니 나오는 애들 죄다 박현빈st로 끼떨어 늬끼하게ㅠㅠㅠ', '차라리', '보기시러ㅜ그런거', '초딩 노래잘하는 애가 젤나음', '나누군지알거같아 걔 알거같애ㅋㅋㅋㅋㅋㅋ', '어린애들이 젤 나음 ㅡㅡ']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_4 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 내 방에 방에 향수로 도배된거 같다고 해서 지금 한차례 붓고 왔어 몰드라고 한다.\n",
      "\n",
      "Target summary \n",
      " 캔들을 많이 만들고 있어서 방이 향수로 도배된 공방이 된 것 같다.\n",
      "\n",
      "Text ['이게뭐야~ 이게 먼저야 내 방에 향수로 도배된거같아 진짜 끔찍 캔들너무많아 지금 공방됐어 지금 한차례 붓고왔어 몰드에..', '아니 무슨 캔들을 그렇게 또 정도가 없이 만들고 있어', '몰라 지금 개웃겨 지금 한', '아이디어스냐구요~', '캔들만드는거만 3일넘게 하고있는거같아']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_5 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 한복을은 채 한 바퀴를 걸을 수 있다.\"\"\n",
      "\n",
      "\n",
      "Target summary \n",
      " 오늘 마을을 한 바퀴 돌며 걷는 것에 대해 이야기한다.\n",
      "\n",
      "Text ['과연오늘 얼마나걸을지..', '아직도 걷는중~', '한참남았눙..', 'ㅎㅎ 그렇게 머루~', '긴코스.. 강남읍한바퀴', '마을한바퀴여~', 'ㅎㅎ 마을이 넘커..']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_6 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 킬링타임으로 볼 수 있다.\n",
      "\n",
      "Target summary \n",
      " 넷플릭스 영화를 시간 때우기(킬링타임) 용으로 추천한다.\n",
      "\n",
      "Text ['#@시스템#사진# 넷플ㅋㅋㅋㅋㅋ추천이요ㅋㅋㅋㅋ 그냥 킬링타임용으로 볼만한듯ㅋㅋㅋㅋ 명대사라 캡쳐해봄ㅋㅋㅋㅋㅋㅋㅋ 헤어질때 저말하면 개웃길듯ㅋㅋㅋㅋㅋㅋㅋㅋㅋ#@기타#ㅋㅋㅋㅋ', '황금같은 가임기 ㅋㅋㅋㅋㅋㅋㅋ', '저런말은 어떻게 생각하는거여ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ', '그니까 ㅋㅋㅋㅋㅋㅋㅋㅋㅋ', '방금 대사 나오는데 저 주인공이 남자들은 강한 여자를 좋아한다고 하지만 막상 원하는건 치어리더야 라는데']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_7 \n",
      "Summary before \n",
      " ['본방사수한다...', '#@시스템#사진# 같이 달리\n",
      "\n",
      "Summary after \n",
      " 본방사수한다...\n",
      "\n",
      "Target summary \n",
      " 숨을 참고 볼 만큼 악귀가 너무 무섭고 몰입도가 굉장히 높다.\n",
      "\n",
      "Text ['본방사수한다...', '#@시스템#사진# 같이 달리자~~~', '와 방금 숨 참고 봤네', 'ㅋㅋㅋㅋ악귀 진짜 무섭제ㅠㅠㅠ', 'ㅋㅋㅋㅋ어...진짜 목소리 바뀔때 완전 무서워', '하 재밌엉 ㅠㅠ', '몰입도 장난아니다ㅠ']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_8 \n",
      "Summary before \n",
      " 생각이 자유로워지고 의욕이 생긴대 대신 일 많이 해서 몸이 피곤한가\n",
      "\n",
      "Summary after \n",
      " 생각이 자유로워지고 의욕이 생긴다고 하지만 일 많이 해서 몸이 피곤한가보다보다가 일 때문에 오 미루던 일을 하고 싶다고 하니 일 시작하나보다 더 빨리 해야겠다고 생각하고 있다.\n",
      "\n",
      "Target summary \n",
      " 일을 많이 해서 몸이 피곤한 대신 생각이 자유로워지고 의욕이 생긴다고 하자, 좋은 카드라고 한다.\n",
      "\n",
      "Text ['생각이 자유로워지고 의욕이 생긴대 대신 일 많이 해서 몸이 피곤한가보다', '그런가보다 오 미루던 일을 드디어 하는구나', '오 좋은 카드네 이제 일 시작하나보다 내년에 본격적으로', '혼자 열심히 해야지']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_9 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 장실 가야지 쉬고 있다.개가 웃기다 뭐하곡 놀고 있는데 익숙한 휘바람이 들리는 것 같다고 해서 굿을 하고 있다.\n",
      "\n",
      "Target summary \n",
      " 엑소의 노래를 듣다가 익숙한 휘파람 소리가 들려 이어폰을 빼고 들었다.\n",
      "\n",
      "Text ['놀았음 허ㅏ장실 가야지 쉬루', 'ㅋㅋㅋㅋㅋㅋㅋ 개웃기다 뭐하곡 놀게 있니??', '엑소노래 하나 나와서 노래듣는데 익숙한 휘바람이 들리는거야 그래서 바로이어폰 빼고 들넜지 노래들어서 굿', '좋겠당ㅋㅋㅋㅋ나있을떈 안나오던뎅']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_10 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 고기를비는 현실이 아니니까 괜찮을 거라 한다.\n",
      "\n",
      "Target summary \n",
      " 영상을 보며 좋아하는 캐릭터에 대하여 이야기한다.\n",
      "\n",
      "Text ['좀비는 현실이 아니니까 괜찮을거야^___^', 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 전하가 사라져... 죽고싶지않으면 어서 찾아내게!', 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 내가 거기서 가장 좋아하는 캐릭터는 범팔이야 범며들었엌ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ', 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ내가 빨리 보면서 내 최애도 찾아볼게 심장이 두근두근 #@이름#야 만약 오늘 이거 같이 봤잖아? 나 찐으루 오늘 음료 두번 쏟았다.....']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_11 \n",
      "Summary before \n",
      " [ [ [ [ [ [''나무 특성상 복불복이라 생각햇지만 #\n",
      "\n",
      "Summary after \n",
      " 나무 특성상 특성상 복불복이라 생각햇지만 그렇다고 하면 또 다르게 보면 또 괜찮은거 같기도 하고, 또 그렇게 보면 더 진한거 같은데 이렇게 보면 진하고 진해서 진할 것 같다고 한다.\n",
      "\n",
      "Target summary \n",
      " 나무 특성상 복불복이라고 생각했지만 이런 사진과는 또 비슷해서 괜찮은 것 같으면서도 진한 것 같기도 하다.\n",
      "\n",
      "Text ['나무 특성상 복불복이라 생각햇지만 #@시스템#사진# 이런거랑은 또 비슷하거든 ㅎ', '흠..저렇게 보면 또 괜찮은데', '그렇긴한데', '생각보다 진한거 같기도하고']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_12 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 죠르디 소환했으니까 당일날 확인해보자\"고 하자\"고 하니 당일 날 확인하라고 한다.\n",
      "\n",
      "Target summary \n",
      " 오늘은 안 눌러진다고 그날 가서 봐야 할듯하다고 이야기하고 있다.\n",
      "\n",
      "Text ['#@시스템#기타#', '오 뭘까', '안눌리더라 다른건 눌리는데', '아 오늘', '궁금 ㅜㅜ', '그날 가서 봐야할듯 ㅋㅋ 죠르디 소환했으니까 당일날 확인해보자', '아ㅋㅋㅋㅋ 오늘인줄 순간']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_13 \n",
      "Summary before \n",
      " ''하고 싶자너 ᄏ', '동숲으로 습득한 스킬로 \n",
      "\n",
      "Summary after \n",
      " 산으로 들어가서 쉬는 건데..', '자국 때문에 귀농하고 싶다고 한다.\n",
      "\n",
      "Target summary \n",
      " 이런 시기에 산으로 들어가서 귀농을 하고 싶다.\n",
      "\n",
      "Text ['이럴 때 산으로 들어가서 쉬는 건데..', '마저... 나 요즘 귀농하고 싶자너 ㅋ', '동숲으로 습득한 스킬로 님 뭔가 잘할 듯', 'ㅇㅈ ㅎㅎ 동숲스킬 무시 못하지 ㅋ']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_14 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 유배오면 우리같이 헹스 다미자 운동, 헬스도 하고싶어하지 않아 고민이다.\n",
      "\n",
      "Target summary \n",
      " 헬스에 다니자고 했는데 검도를 배우고 싶다고 한다.\n",
      "\n",
      "Text ['#@이름# 유배오면 우리같이 헹스 다미자 운동', '헬스요...? 나 합기듀 하고싶어 아니아니 검도 이야', '#@이름#이한테 배워', '머리~', 'ㅋㅋㅋㅋㅋㅋㅋㅋㅌㅋㅋㅋ', '아맞다이 가이 검도인가 후덜덜']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_15 \n",
      "Summary before \n",
      " \n",
      "\n",
      "Summary after \n",
      " 하루종일 떼창 영상 봣다.\n",
      "\n",
      "Target summary \n",
      " 내한공연 오는 가수들 공연에서 떼를 지어 노래(떼창)하는 영상을 맨날 본 거 또 보고 있는데 거기에 있는 거 마냥 벅차오르고 가슴이 웅장해진다.\n",
      "\n",
      "Text ['어제 하루종일 떼창 영상 봣음ㅋㅋ', '설마 내한공연 오는 가수들 공연?ㅋㅋㅋㅋㅋㅋ', '당연ㅋㅋㅋㅋ본 거 또보고 또봐도 맨날 재밋음ㅋㅋㅋ', '나도 좋아해ㅋㅋㅋㅋㅋ 가슴이 웅장해짐요', '내가 거기에 있는거마냥ㅋㅋㅋㅋ 벅차오름', '인정']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(summaries_after_tuning)):\n",
    "    print('idx_{} '.format(i))\n",
    "    print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "905bd914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/Korean_Conversation_Summary\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2022-11-15 14:23:02--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/AMAZON_FASHION_5.json.gz\n",
      "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
      "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... failed: Connection timed out.\n",
      "Retrying.\n",
      "\n",
      "--2022-11-15 14:25:12--  (try: 2)  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/AMAZON_FASHION_5.json.gz\n",
      "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... ^C\n"
     ]
    }
   ],
   "source": [
    "%cd /aiffel/aiffel/Korean_Conversation_Summary\n",
    "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/AMAZON_FASHION_5.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc58d7ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'AMAZON_FASHION_5.json.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1090/3321195047.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AMAZON_FASHION_5.json.gz'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AMAZON_FASHION_5.json.gz'"
     ]
    }
   ],
   "source": [
    "import gzip, json, pandas as pd\n",
    "\n",
    "data = []\n",
    "with gzip.open('AMAZON_FASHION_5.json.gz') as f:\n",
    "    for l in f:\n",
    "        data.append(json.loads(l.strip()))\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9dce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76275bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
