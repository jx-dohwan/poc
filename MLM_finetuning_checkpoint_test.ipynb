{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d037907",
   "metadata": {},
   "source": [
    "# MLM_finetuning_checkpoint_test\n",
    "- 다른 ipynb 파일에서 전처리를 진행후 생성된 csv 파일로 본 학습이 진행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4a339",
   "metadata": {},
   "source": [
    "## 1. Import 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8568795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3278e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.21.4)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: datasets==1.0.2 in /opt/conda/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.26.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (0.3.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.0.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (4.62.3)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.21.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers==4.24.0 in /opt/conda/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (1.21.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2021.10.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformer-utils in /opt/conda/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (1.9.1+cu111)\n",
      "Requirement already satisfied: colorcet in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (3.0.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.24.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.62.3)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (0.11.2)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /opt/conda/lib/python3.9/site-packages (from colorcet->transformer-utils) (0.4.8)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.3.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.7.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.21.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->transformer-utils) (4.0.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.13.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (3.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2021.11.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (8.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->seaborn->transformer-utils) (2021.3)\n",
      "Requirement already satisfied: param>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from pyct>=0.4.4->colorcet->transformer-utils) (1.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (1.26.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn->transformer-utils) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (0.13.5)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.11.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "!pip install datasets==1.0.2\n",
    "!pip install transformers==4.24.0\n",
    "!pip install transformer-utils\n",
    "!pip install packaging\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34c401a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mecab 설치\n",
    "# !curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d40fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    "    BartTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    EarlyStoppingCallback,\n",
    "    PreTrainedTokenizerFast\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import EncoderDecoderModel\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "#from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75481418",
   "metadata": {},
   "source": [
    "## 2. 모델 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa23dd3",
   "metadata": {},
   "source": [
    "### 1) 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc889662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = \"/aiffel/aiffel/Korean_Conversation_Summary/MLM_pretrain_basev2_freezing/checkpoint-175000\"#\"/aiffel/aiffel/Korean_Conversation_Summary/MLM_ft_freez/checkpoint-21500\"#\"/aiffel/aiffel/Korean_Conversation_Summary/MLM_pretrain_basev2_freezing/checkpoint-175000\"#\"/aiffel/aiffel/Korean_Conversation_Summary/MLM_pretrain_basev2_total10ep_3/checkpoint-133500\" #MLM_ft2/checkpoint-4000\" #MLM_pretrain_basev2_total10ep/checkpoint-14000\" #/MLM_ft/checkpoint-4000\"#\"/aiffel/aiffel/Korean_Conversation_Summary/MLM_pretrain_5ep_221120_2/checkpoint-87500\"#\"gogamza/kobart-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28cb7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def freeze_params(model):\n",
    "#     for par in model.parameters():\n",
    "#         par.requires_grad = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d7248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze_params(model.get_encoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a0c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec_layers = model.get_decoder().layers\n",
    "# for i in range(2):\n",
    "#     freeze_params(dec_layers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e379b576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Length of train params in Summarization Model : 259\n"
     ]
    }
   ],
   "source": [
    "for i in model.parameters():\n",
    "    print(i.requires_grad)\n",
    "train_p = [p for p in model.parameters() if p.requires_grad] \n",
    "print(f'Length of train params in Summarization Model : {len(train_p)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e8bd9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in model.parameters():\n",
    "#     print(i.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ef5a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in model.parameters():\n",
    "#     i.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a62b7bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in model.parameters():\n",
    "#     print(i.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b131c",
   "metadata": {},
   "source": [
    "### 2) 데이터 불러오기 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4841706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_category_path = \"data/train_category.csv\"\n",
    "val_category_path = \"data/val_category.csv\"\n",
    "\n",
    "with open(train_category_path, encoding=\"utf-8\") as f:\n",
    "            train_category = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "with open(val_category_path, encoding=\"utf-8\") as f:\n",
    "            val_category = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "265fa230",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_textfile_path = \"data/train_text.csv\"\n",
    "train_summaryfile_path = \"data/train_summary.csv\"\n",
    "\n",
    "with open(train_textfile_path, encoding=\"utf-8\") as f:\n",
    "            train_textlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "with open(train_summaryfile_path, encoding=\"utf-8\") as f:\n",
    "            train_sumlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d11ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_textfile_path = \"data/val_text.csv\"\n",
    "val_summaryfile_path = \"data/val_summary.csv\"\n",
    "\n",
    "with open(val_textfile_path, encoding=\"utf-8\") as f:\n",
    "            val_textlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "with open(val_summaryfile_path, encoding=\"utf-8\") as f:\n",
    "            val_sumlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23616d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_textlines[0]\n",
    "del val_textlines[0]\n",
    "del train_category[0]\n",
    "del val_category[0]\n",
    "del val_sumlines[0]\n",
    "del train_sumlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fd3afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= pd.read_csv(\"data/train_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7081bd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우리 비행포함 15일이야 16일 아 너 나한테 돈 보내주면 지금 할 수 잇옹 얼마야 최종 결제액이 잠시만 인당 952 900 합쳐서 1 905 800 근데 나중에 특가 뜰 수도 있으려나 좀 더 두고볼까 뜨기야 뜨겠지 웅웅 보니까 아시아나는 특가 이벤트 꽤 하는 것 같아서 일단 두고보장 그래 구럼 일단 자자'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4606cbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우리 비행포함 15일이야 16일 아 너 나한테 돈 보내주면 지금 할 수 잇옹 얼마야 최종 결제액이 잠시만 인당 952 900 합쳐서 1 905 800 근데 나중에 특가 뜰 수도 있으려나 좀 더 두고볼까 뜨기야 뜨겠지 웅웅 보니까 아시아나는 특가 이벤트 꽤 하는 것 같아서 일단 두고보장 그래 구럼 일단 자자'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_textlines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8e6fd",
   "metadata": {},
   "source": [
    "### 3) 메타 데이터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4540f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_textlines)):\n",
    "    temp_cat = \"#\"+val_category[i]+\"# \"\n",
    "    val_textlines[i] = temp_cat+val_textlines[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f2ac372",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_textlines)):\n",
    "    temp_cat = \"#\"+train_category[i]+\"# \"\n",
    "    train_textlines[i] = temp_cat+train_textlines[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbfea9f",
   "metadata": {},
   "source": [
    "### 4) DataFrame로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6912ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(zip(train_textlines, train_sumlines), columns=['Text', 'Summary'])\n",
    "val_df = pd.DataFrame(zip(val_textlines, val_sumlines), columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0abc6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True, drop=True)\n",
    "val_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afd42003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#상거래(쇼핑)# 그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#상거래(쇼핑)# kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#상거래(쇼핑)# 아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 ...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#상거래(쇼핑)# 칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#상거래(쇼핑)# 잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  #상거래(쇼핑)# 그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우...   \n",
       "1  #상거래(쇼핑)# kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마...   \n",
       "2  #상거래(쇼핑)# 아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 ...   \n",
       "3  #상거래(쇼핑)# 칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕...   \n",
       "4  #상거래(쇼핑)# 잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보...   \n",
       "\n",
       "                                             Summary  \n",
       "0                 비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다  \n",
       "1                 비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  \n",
       "3                칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다  \n",
       "4                   잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48f14d",
   "metadata": {},
   "source": [
    "## 3. 데이터 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25b1b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mecab 적용 후 문장 len 출력하는 함수 구현 - 전체 데이터\n",
    "\n",
    "# # 한글 데이터 Mecab 적용하기\n",
    "# mecab = Mecab()\n",
    "\n",
    "# def sentence_len_total(data):\n",
    "\n",
    "#     # 빈 리스트 적용\n",
    "#     text_split_text = []\n",
    "#     text_split_summary = []\n",
    "    \n",
    "#     # 반복문으로 Mecab 적용\n",
    "#     for text_sen in data['Text'].iloc[range(0, len(data))]:\n",
    "#         text_split_text.append(mecab.morphs(text_sen))\n",
    "\n",
    "#     for summary_sen in data['Summary'].iloc[range(0, len(data))]:\n",
    "#         text_split_summary.append(mecab.morphs(summary_sen))\n",
    "    \n",
    "#     df_ext = pd.DataFrame(zip(text_split_text,text_split_summary),\\\n",
    "#                           columns=['Text', 'Summary'])\n",
    "\n",
    "    \n",
    "#     # Mecab 적용 후 길이 출력\n",
    "#     text_len = df_ext.Text.map(len)\n",
    "#     headlines_len = df_ext.Summary.map(len)\n",
    "    \n",
    "#     # text_len 사분위수 구하기\n",
    "    \n",
    "#     text_Q1 = text_len.quantile(.25)\n",
    "#     text_Q3 = text_len.quantile(.75)\n",
    "#     text_IQR = text_Q3 - text_Q1\n",
    "#     text_Q2 = text_len.quantile(.5)\n",
    "#     text_Q4 = text_len.quantile(1)\n",
    "#     text_threshold_len_left = text_Q1 - (1.5 * text_IQR)\n",
    "#     text_threshold_len_right = text_Q3 + (1.5 * text_IQR)\n",
    "#     # headlines_len 사분위수 구하기\n",
    "    \n",
    "#     headlines_Q1 = headlines_len.quantile(.25)\n",
    "#     headlines_Q3 = headlines_len.quantile(.75)\n",
    "#     headlines_IQR = headlines_Q3 - headlines_Q1\n",
    "#     headlines_Q2 = headlines_len.quantile(.5)\n",
    "#     headlines_Q4 = headlines_len.quantile(1)\n",
    "#     headlines_threshold_len_left = headlines_Q1 - (1.5 * headlines_IQR)\n",
    "#     headlines_threshold_len_right = headlines_Q3 + (1.5 * headlines_IQR)\n",
    "    \n",
    "#     print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "#     print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "#     print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "    \n",
    "#     print('헤드라인의 최소 길이 : {}'.format(np.min(headlines_len)))\n",
    "#     print('헤드라인의 최대 길이 : {}'.format(np.max(headlines_len)))\n",
    "#     print('헤드라인의 평균 길이 : {}'.format(np.mean(headlines_len)))\n",
    "    \n",
    "\n",
    "    \n",
    "#     print('text_Q1 = {}'.format(text_Q1), 'headlines_Q1 = {}'.format(headlines_Q1))\n",
    "#     print('text_Q3 = {}'.format(text_Q3), 'headlines_Q3 = {}'.format(headlines_Q3))\n",
    "#     print('text_IQR = {}'.format(text_IQR), 'headlines_IQR = {}'.format(headlines_IQR))\n",
    "#     print('text_Q2 = {}'.format(text_Q2), 'headlines_Q2 = {}'.format(headlines_Q2))\n",
    "#     print('text_Q4 = {}'.format(text_Q4), 'headlines_Q4 = {}'.format(headlines_Q4))\n",
    "#     print('텍스트의 왼쪽 울타리 범위 : {}'. format(text_threshold_len_left),\n",
    "#          '텍스트의 오른쪽 울타리 범위 : {}'. format(text_threshold_len_right))\n",
    "#     print('헤드라인의 왼쪽 울타리 범위 : {}'. format(headlines_threshold_len_left),\n",
    "#          '헤드라인의 오른쪽 울타리 범위 : {}'. format(headlines_threshold_len_right))\n",
    "    \n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.boxplot(text_len)\n",
    "#     plt.title('text')\n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.boxplot(headlines_len)\n",
    "#     plt.title('headlines')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.title('text')\n",
    "#     plt.hist(text_len, bins = 40)\n",
    "#     plt.xlabel('length of samples')\n",
    "#     plt.ylabel('number of samples')\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.title('headlines')\n",
    "#     plt.hist(headlines_len, bins = 40)\n",
    "#     plt.xlabel('length of samples')\n",
    "#     plt.ylabel('number of samples')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f8c9af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_len_total(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cab87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_len_total(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7925f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#상거래(쇼핑)# 그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#상거래(쇼핑)# kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#상거래(쇼핑)# 아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 ...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#상거래(쇼핑)# 칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#상거래(쇼핑)# 잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  #상거래(쇼핑)# 그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우...   \n",
       "1  #상거래(쇼핑)# kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마...   \n",
       "2  #상거래(쇼핑)# 아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 ...   \n",
       "3  #상거래(쇼핑)# 칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕...   \n",
       "4  #상거래(쇼핑)# 잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보...   \n",
       "\n",
       "                                             Summary  \n",
       "0                 비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다  \n",
       "1                 비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  \n",
       "3                칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다  \n",
       "4                   잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53502956",
   "metadata": {},
   "source": [
    "## 4. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4690787f",
   "metadata": {},
   "source": [
    "### 1) dataset으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "513a0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_len = len(train_df) \n",
    "train_data = Dataset.from_pandas(train_df[:train_len]) \n",
    "val_len = len(val_df) // 2\n",
    "val_data = Dataset.from_pandas(val_df[:val_len])\n",
    "test_data=Dataset.from_pandas(val_df[val_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7892fc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 279992)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 17502)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 17502)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69934d6e",
   "metadata": {},
   "source": [
    "### 2) EDA 바탕으로 길이 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0648f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 70\n",
    "max_target = 30\n",
    "batch_size = 4\n",
    "ignore_index = -100# tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ee533",
   "metadata": {},
   "source": [
    "### 3) 토큰화 함수 구현 및 토큰화\n",
    "- input_ids, attention_mask, lables토큰만 구현\n",
    "- 추가로 decoder_input_ids, decoder_attention_mask 토큰화도 시도해봤으나 성능에는 차이가 없고 허깅페이스에 그에 관한 설명이 나왔있음\n",
    "    - https://huggingface.co/docs/transformers/glossary#decoder-input-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e1a48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_words = [\n",
    "#     \"#@주소#\",\n",
    "#     \"#@이모티콘#\",\n",
    "#     \"#@이름#\",\n",
    "#     \"#@URL#\",\n",
    "#     \"#@소속#\",\n",
    "#     \"#@기타#\",\n",
    "#     \"#@전번#\",\n",
    "#     \"#@계정#\",\n",
    "#     \"#@url#\",\n",
    "#     \"#@번호#\",\n",
    "#     \"#@금융#\",\n",
    "#     \"#@신원#\",\n",
    "#     \"#@장소#\",\n",
    "#     \"#@시스템#사진#\",\n",
    "#     \"#@시스템#동영상#\",\n",
    "#     \"#@시스템#기타#\",\n",
    "#     \"#@시스템#검색#\",\n",
    "#     \"#@시스템#지도#\",\n",
    "#     \"#@시스템#삭제#\",\n",
    "#     \"#@시스템#파일#\",\n",
    "#     \"#@시스템#송금#\",\n",
    "#     \"#@시스템#\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3832a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_tokenizer = PreTrainedTokenizerFast(\n",
    "#         tokenizer_object=tokenizer,\n",
    "\n",
    "#         additional_special_tokens=special_words,\n",
    "#     )\n",
    "# pretrained_tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac5549bc",
   "metadata": {},
   "outputs": [],
   "source": [
    " def add_ignored_data(inputs, max_len, ignore_index):\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [ignore_index] *(max_len - len(inputs)) # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00d00dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding_data(inputs, max_len):\n",
    "        pad_index = tokenizer.pad_token_id\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [pad_index] *(max_len - len(inputs))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3052528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "    label_id= []\n",
    "    label_ids = []\n",
    "    dec_input_ids = []\n",
    "    inputs = [dialogue for dialogue in data_to_process['Text']]\n",
    "    model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n",
    "\n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_id.append(tokenizer.encode(data_to_process['Summary'][i]))  \n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_id[i].append(tokenizer.eos_token_id)\n",
    "        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))\n",
    "#    for i in range(len(data_to_process['Summary'])):  \n",
    "#        dec_input_id = [tokenizer.eos_token_id]\n",
    "#        dec_input_id += label_ids[i][:-1]\n",
    "#        dec_input_ids.append(add_padding_data(dec_input_id, max_target))  \n",
    "    \n",
    "    model_inputs['labels'] = label_ids\n",
    "#    model_inputs['decoder_input_ids'] = dec_input_ids\n",
    "#    model_inputs['decoder_attention_mask'] = (np.array(dec_input_ids) != tokenizer.pad_token_id).astype(int) \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5e03cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb029f1d71bb4e898696d9e17c16b7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/280 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11cf63cd6c849a4b0b5c48334cce8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f87ff",
   "metadata": {},
   "source": [
    "## 5. 학습을 진행하기 위한 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b9df01",
   "metadata": {},
   "source": [
    "### 1) config 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dde282fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set special tokens\n",
    "#from transformers import EncoderDecoderConfig\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 30 # 256은 쿠다 메모리 오류 생김\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 2\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 2\n",
    "#model.config.suppress_tokens = [23782, 14338, 22554, 234]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d879b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"/aiffel/aiffel/Korean_Conversation_Summary/MLM_pretrain_basev2_freezing/checkpoint-175000\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"kobart_version\": 2.0,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"length_penalty\": 2.0,\n",
       "  \"max_length\": 30,\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 2,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 2,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b17b01",
   "metadata": {},
   "source": [
    "### 2) rounge 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fa4030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     print(\"labels_ids\",labels_ids)\n",
    "#     print(\"labels_ids[labels_ids == -100]\",labels_ids[labels_ids == -100])\n",
    "#     print(\"tokenizer.pad_token_id\",tokenizer.pad_token_id)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "    rouge_output2 = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    rouge_outputL = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rougeL\"])[\"rougeL\"].mid\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"rouge1_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge1_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge1_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "        \n",
    "        \"rouge2_precision\": round(rouge_output2.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output2.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output2.fmeasure, 4), \n",
    "        \n",
    "        \"rougeL_precision\": round(rouge_outputL.precision, 4),\n",
    "        \"rougeL_recall\": round(rouge_outputL.recall, 4),\n",
    "        \"rougeL_fmeasure\": round(rouge_outputL.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860ae15",
   "metadata": {},
   "source": [
    "### 3) arguments 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d96ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"MLM_ft_freez_4\",\n",
    "    num_train_epochs=5,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    logging_steps=500,\n",
    "    save_total_limit=3,\n",
    "    #evaluation_strategy = \"steps\",# step별로 2버 loss가 오르는거 아니면 계속 반복하는듯\n",
    "    #load_best_model_at_end = True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a4724",
   "metadata": {},
   "source": [
    "### 4) data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c61ba86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \\n또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\\ntokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
    "\"\"\"\n",
    "DataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \n",
    "또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\n",
    "tokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf0b9f",
   "metadata": {},
   "source": [
    "### 5) train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b77d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "   # callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774a107",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 279992\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 87500\n",
      "  Number of trainable parameters = 123859968\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/Korean_Conversation_Summary/wandb/run-20221125_024355-10aasw19</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/10aasw19\" target=\"_blank\">MLM_ft_freez_4</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38001' max='87500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38001/87500 2:31:35 < 3:17:28, 4.18 it/s, Epoch 2.17/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.415900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.354900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.309700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.288500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.240500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.199900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.178500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>2.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>2.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>2.859100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>2.850200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>2.871400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-1000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-1000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-1500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-1500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-2000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-2000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-2500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-2500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-3000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-3000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-3500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-3500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-4000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-4000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-4500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-4500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-5000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-5000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-5500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-5500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-6000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-6000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-6500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-6500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-7000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-7000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-7500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-7500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-8000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-8000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-8500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-8500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-9000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-9000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-9500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-9500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-10000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-10000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-10500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-10500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-11000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-11000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-11500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-11500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-12000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-12000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-12500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-12500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-13000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-13000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-13500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-13500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-14000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-14000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-14500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-14500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-15000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-15000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-15500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-15500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-16000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-16000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-16500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-16500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-17000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-17000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-17500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-17500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-18000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-18000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-18500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-18500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-19000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-19000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-19500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-19500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-20000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-20000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-20500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-20500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-20500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-21000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-21000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-21500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-21500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-21500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-22000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-22000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-22500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-22500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-22500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-23000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-23000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-23500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-23500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-23500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-24000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-24000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-24000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-24500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-24500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-24500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-25000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-25000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-25000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-25500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-25500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-25500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-26000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-26000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-26000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-26500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-26500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-26500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-27000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-27000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-27000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-27500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-27500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-27500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-28000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-28000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-28000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-28500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-28500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-28500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-29000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-29000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-29000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-29500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-29500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-29500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-30000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-30000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-30000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-30500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-30500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-30500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-31000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-31000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-31000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-31500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-31500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-31500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-32000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-32000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-32000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-32500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-32500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-32500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-33000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-33000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-33000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-33500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-33500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-33500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-34000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-34000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-34000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-34500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-34500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-34500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-35000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-35000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-35000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-35500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-35500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-35500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-36000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-36000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-36000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-36500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-36500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-36500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-35000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-37000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-37000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-37000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-37500\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-37500/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-37500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_ft_freez_4/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_ft_freez_4/checkpoint-38000\n",
      "Configuration saved in MLM_ft_freez_4/checkpoint-38000/config.json\n",
      "Model weights saved in MLM_ft_freez_4/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_ft_freez_4/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_ft_freez_4/checkpoint-38000/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca5803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1cd53e",
   "metadata": {},
   "source": [
    "## 6. 학습 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d22bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, num_beams=2,no_repeat_ngram_size=2, max_length=40,\n",
    "                            suppress_tokens= [234,23782,14338,240,199,198,161,116, 14338, 239], \n",
    "                            attention_mask=attention_mask, top_p=0.92,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "#model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)# 여기에 기본 kobart가져오기?ㅇ\n",
    "import random\n",
    "from random import randrange\n",
    "ck_num = len(test_data)\n",
    "test_samples = test_data.select(range(0, ck_num, 500))# 0, len(test_data), 200\n",
    "\n",
    "#summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1] # 여기에 체크포인트 가져오기 \n",
    "# 연구해봐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb004e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(summaries_after_tuning)):\n",
    "    print('idx_{} '.format(i))\n",
    "  #  print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd166f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 50000\n",
    "# print(\"ROUGE 1 SCORE: \",rouge.compute(predictions=summaries_after_tuning, references=test_samples[\"Summary\"],\\\n",
    "#                                             rouge_types=[\"rouge1\"])[\"rouge1\"].mid)\n",
    "# print(\"ROUGE 2 SCORE: \",rouge.compute(predictions=summaries_after_tuning, references=test_samples[\"Summary\"], rouge_types=[\"rouge2\"])[\"rouge2\"].mid)\n",
    "# print(\"ROUGE L SCORE: \",rouge.compute(predictions=summaries_after_tuning, references=test_samples[\"Summary\"], rouge_types=[\"rougeL\"])[\"rougeL\"].mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11233f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
