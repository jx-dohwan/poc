{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "191d9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08509e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.21.4)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: datasets==1.0.2 in /opt/conda/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.0.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (4.62.3)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.21.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.3.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (0.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.26.12)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers==4.24.0 in /opt/conda/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (1.21.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2021.11.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.13.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (3.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.0.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformer-utils in /opt/conda/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (1.9.1+cu111)\n",
      "Requirement already satisfied: colorcet in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (3.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.62.3)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.24.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (0.11.2)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /opt/conda/lib/python3.9/site-packages (from colorcet->transformer-utils) (0.4.8)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (3.4.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.21.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->transformer-utils) (4.0.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.13.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (3.4.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (8.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (3.0.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->seaborn->transformer-utils) (2021.3)\n",
      "Requirement already satisfied: param>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from pyct>=0.4.4->colorcet->transformer-utils) (1.12.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn->transformer-utils) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (0.13.5)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.11.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "!pip install datasets==1.0.2\n",
    "!pip install transformers==4.24.0\n",
    "!pip install transformer-utils\n",
    "!pip install packaging\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46aa21d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-19 16:12:18--  https://www.dropbox.com/s/9xls0tgtf3edgns/mecab-0.996-ko-0.9.2.tar.gz?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6017:18::a27d:212\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/dl/9xls0tgtf3edgns/mecab-0.996-ko-0.9.2.tar.gz [following]\n",
      "--2022-11-19 16:12:18--  https://www.dropbox.com/s/dl/9xls0tgtf3edgns/mecab-0.996-ko-0.9.2.tar.gz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucb777575de72a687cf361012ee5.dl.dropboxusercontent.com/cd/0/get/BxCjcXPOYKSg2zdK8yIoUJrBdpVoyOx4OFT9rDTlGXSLhBJESWI2AvtELbb4IBuaUoXVXXTL9G4ZEHMjVpMgFvFTPkkT8Wi8N6Hv0P_sy5EJlcx0UXOhGy5irJXo0N_NtWjjELwj55RUfr0Lo9OCxuidquIkXogMUhPDei1IQKrerIDEJPH-BAY8Pn9FdPpa3fo/file?dl=1# [following]\n",
      "--2022-11-19 16:12:18--  https://ucb777575de72a687cf361012ee5.dl.dropboxusercontent.com/cd/0/get/BxCjcXPOYKSg2zdK8yIoUJrBdpVoyOx4OFT9rDTlGXSLhBJESWI2AvtELbb4IBuaUoXVXXTL9G4ZEHMjVpMgFvFTPkkT8Wi8N6Hv0P_sy5EJlcx0UXOhGy5irJXo0N_NtWjjELwj55RUfr0Lo9OCxuidquIkXogMUhPDei1IQKrerIDEJPH-BAY8Pn9FdPpa3fo/file?dl=1\n",
      "Resolving ucb777575de72a687cf361012ee5.dl.dropboxusercontent.com (ucb777575de72a687cf361012ee5.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
      "Connecting to ucb777575de72a687cf361012ee5.dl.dropboxusercontent.com (ucb777575de72a687cf361012ee5.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1414979 (1.3M) [application/binary]\n",
      "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz?dl=1.2’\n",
      "\n",
      "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2022-11-19 16:12:19 (32.7 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz?dl=1.2’ saved [1414979/1414979]\n",
      "\n",
      "mecab-0.996-ko-0.9.2/\n",
      "mecab-0.996-ko-0.9.2/example/\n",
      "mecab-0.996-ko-0.9.2/example/example.cpp\n",
      "mecab-0.996-ko-0.9.2/example/example_lattice.cpp\n",
      "mecab-0.996-ko-0.9.2/example/example_lattice.c\n",
      "mecab-0.996-ko-0.9.2/example/example.c\n",
      "mecab-0.996-ko-0.9.2/example/thread_test.cpp\n",
      "mecab-0.996-ko-0.9.2/mecab-config.in\n",
      "mecab-0.996-ko-0.9.2/man/\n",
      "mecab-0.996-ko-0.9.2/man/Makefile.am\n",
      "mecab-0.996-ko-0.9.2/man/mecab.1\n",
      "mecab-0.996-ko-0.9.2/man/Makefile.in\n",
      "mecab-0.996-ko-0.9.2/mecab.iss.in\n",
      "mecab-0.996-ko-0.9.2/config.guess\n",
      "mecab-0.996-ko-0.9.2/README\n",
      "mecab-0.996-ko-0.9.2/COPYING\n",
      "mecab-0.996-ko-0.9.2/CHANGES.md\n",
      "mecab-0.996-ko-0.9.2/README.md\n",
      "mecab-0.996-ko-0.9.2/INSTALL\n",
      "mecab-0.996-ko-0.9.2/config.sub\n",
      "mecab-0.996-ko-0.9.2/configure.in\n",
      "mecab-0.996-ko-0.9.2/swig/\n",
      "mecab-0.996-ko-0.9.2/swig/Makefile\n",
      "mecab-0.996-ko-0.9.2/swig/version.h.in\n",
      "mecab-0.996-ko-0.9.2/swig/version.h\n",
      "mecab-0.996-ko-0.9.2/swig/MeCab.i\n",
      "mecab-0.996-ko-0.9.2/aclocal.m4\n",
      "mecab-0.996-ko-0.9.2/LGPL\n",
      "mecab-0.996-ko-0.9.2/Makefile.am\n",
      "mecab-0.996-ko-0.9.2/configure\n",
      "mecab-0.996-ko-0.9.2/tests/\n",
      "mecab-0.996-ko-0.9.2/tests/autolink/\n",
      "mecab-0.996-ko-0.9.2/tests/autolink/unk.def\n",
      "mecab-0.996-ko-0.9.2/tests/autolink/dicrc\n",
      "mecab-0.996-ko-0.9.2/tests/autolink/dic.csv\n",
      "mecab-0.996-ko-0.9.2/tests/autolink/test\n",
      "mecab-0.996-ko-0.9.2/tests/autolink/char.def\n",
      "mecab-0.996-ko-0.9.2/tests/autolink/matrix.def\n",
      "mecab-0.996-ko-0.9.2/tests/autolink/test.gld\n",
      "mecab-0.996-ko-0.9.2/tests/t9/\n",
      "mecab-0.996-ko-0.9.2/tests/t9/unk.def\n",
      "mecab-0.996-ko-0.9.2/tests/t9/ipadic.pl\n",
      "mecab-0.996-ko-0.9.2/tests/t9/dicrc\n",
      "mecab-0.996-ko-0.9.2/tests/t9/dic.csv\n",
      "mecab-0.996-ko-0.9.2/tests/t9/test\n",
      "mecab-0.996-ko-0.9.2/tests/t9/char.def\n",
      "mecab-0.996-ko-0.9.2/tests/t9/matrix.def\n",
      "mecab-0.996-ko-0.9.2/tests/t9/mkdic.pl\n",
      "mecab-0.996-ko-0.9.2/tests/t9/test.gld\n",
      "mecab-0.996-ko-0.9.2/tests/cost-train/\n",
      "mecab-0.996-ko-0.9.2/tests/cost-train/ipa.train\n",
      "mecab-0.996-ko-0.9.2/tests/cost-train/ipa.test\n",
      "mecab-0.996-ko-0.9.2/tests/cost-train/seed/\n",
      "mecab-0.996-ko-0.9.2/tests/cost-train/seed/rewrite.def\n",
      "mecab-0.996-ko-0.9.2/tests/cost-train/seed/feature.def\n",
      "mecab-0.996-ko-0.9.2/tests/cost-train/seed/unk.def\n",
      "mecab-0.996-ko-0.9.2/tests/cost-train/seed/dicrc\n",
      "mecab-0.996-ko-0.9.2/tests/cost-train/seed/dic.csv\n",
      "mecab-0.996-ko-0.9.2/tests/cost-train/seed/char.def\n",
      "mecab-0.996-ko-0.9.2/tests/cost-train/seed/matrix.def\n",
      "mecab-0.996-ko-0.9.2/tests/run-eval.sh\n",
      "mecab-0.996-ko-0.9.2/tests/run-cost-train.sh\n",
      "mecab-0.996-ko-0.9.2/tests/Makefile.am\n",
      "mecab-0.996-ko-0.9.2/tests/katakana/\n",
      "mecab-0.996-ko-0.9.2/tests/katakana/unk.def\n",
      "mecab-0.996-ko-0.9.2/tests/katakana/dicrc\n",
      "mecab-0.996-ko-0.9.2/tests/katakana/dic.csv\n",
      "mecab-0.996-ko-0.9.2/tests/katakana/test\n",
      "mecab-0.996-ko-0.9.2/tests/katakana/char.def\n",
      "mecab-0.996-ko-0.9.2/tests/katakana/matrix.def\n",
      "mecab-0.996-ko-0.9.2/tests/katakana/test.gld\n",
      "mecab-0.996-ko-0.9.2/tests/eval/\n",
      "mecab-0.996-ko-0.9.2/tests/eval/answer\n",
      "mecab-0.996-ko-0.9.2/tests/eval/system\n",
      "mecab-0.996-ko-0.9.2/tests/eval/test.gld\n",
      "mecab-0.996-ko-0.9.2/tests/shiin/\n",
      "mecab-0.996-ko-0.9.2/tests/shiin/unk.def\n",
      "mecab-0.996-ko-0.9.2/tests/shiin/dicrc\n",
      "mecab-0.996-ko-0.9.2/tests/shiin/dic.csv\n",
      "mecab-0.996-ko-0.9.2/tests/shiin/test\n",
      "mecab-0.996-ko-0.9.2/tests/shiin/char.def\n",
      "mecab-0.996-ko-0.9.2/tests/shiin/matrix.def\n",
      "mecab-0.996-ko-0.9.2/tests/shiin/mkdic.pl\n",
      "mecab-0.996-ko-0.9.2/tests/shiin/test.gld\n",
      "mecab-0.996-ko-0.9.2/tests/latin/\n",
      "mecab-0.996-ko-0.9.2/tests/latin/unk.def\n",
      "mecab-0.996-ko-0.9.2/tests/latin/dicrc\n",
      "mecab-0.996-ko-0.9.2/tests/latin/dic.csv\n",
      "mecab-0.996-ko-0.9.2/tests/latin/test\n",
      "mecab-0.996-ko-0.9.2/tests/latin/char.def\n",
      "mecab-0.996-ko-0.9.2/tests/latin/matrix.def\n",
      "mecab-0.996-ko-0.9.2/tests/latin/test.gld\n",
      "mecab-0.996-ko-0.9.2/tests/chartype/\n",
      "mecab-0.996-ko-0.9.2/tests/chartype/unk.def\n",
      "mecab-0.996-ko-0.9.2/tests/chartype/dicrc\n",
      "mecab-0.996-ko-0.9.2/tests/chartype/dic.csv\n",
      "mecab-0.996-ko-0.9.2/tests/chartype/test\n",
      "mecab-0.996-ko-0.9.2/tests/chartype/char.def\n",
      "mecab-0.996-ko-0.9.2/tests/chartype/matrix.def\n",
      "mecab-0.996-ko-0.9.2/tests/chartype/test.gld\n",
      "mecab-0.996-ko-0.9.2/tests/run-dics.sh\n",
      "mecab-0.996-ko-0.9.2/tests/ngram/\n",
      "mecab-0.996-ko-0.9.2/tests/ngram/unk.def\n",
      "mecab-0.996-ko-0.9.2/tests/ngram/dicrc\n",
      "mecab-0.996-ko-0.9.2/tests/ngram/dic.csv\n",
      "mecab-0.996-ko-0.9.2/tests/ngram/test\n",
      "mecab-0.996-ko-0.9.2/tests/ngram/char.def\n",
      "mecab-0.996-ko-0.9.2/tests/ngram/matrix.def\n",
      "mecab-0.996-ko-0.9.2/tests/ngram/test.gld\n",
      "mecab-0.996-ko-0.9.2/tests/Makefile.in\n",
      "mecab-0.996-ko-0.9.2/ltmain.sh\n",
      "mecab-0.996-ko-0.9.2/config.rpath\n",
      "mecab-0.996-ko-0.9.2/config.h.in\n",
      "mecab-0.996-ko-0.9.2/mecabrc.in\n",
      "mecab-0.996-ko-0.9.2/GPL\n",
      "mecab-0.996-ko-0.9.2/Makefile.train\n",
      "mecab-0.996-ko-0.9.2/ChangeLog\n",
      "mecab-0.996-ko-0.9.2/install-sh\n",
      "mecab-0.996-ko-0.9.2/AUTHORS\n",
      "mecab-0.996-ko-0.9.2/doc/\n",
      "mecab-0.996-ko-0.9.2/doc/bindings.html\n",
      "mecab-0.996-ko-0.9.2/doc/posid.html\n",
      "mecab-0.996-ko-0.9.2/doc/unk.html\n",
      "mecab-0.996-ko-0.9.2/doc/learn.html\n",
      "mecab-0.996-ko-0.9.2/doc/format.html\n",
      "mecab-0.996-ko-0.9.2/doc/libmecab.html\n",
      "mecab-0.996-ko-0.9.2/doc/mecab.css\n",
      "mecab-0.996-ko-0.9.2/doc/feature.html\n",
      "mecab-0.996-ko-0.9.2/doc/Makefile.am\n",
      "mecab-0.996-ko-0.9.2/doc/soft.html\n",
      "mecab-0.996-ko-0.9.2/doc/en/\n",
      "mecab-0.996-ko-0.9.2/doc/en/bindings.html\n",
      "mecab-0.996-ko-0.9.2/doc/dic-detail.html\n",
      "mecab-0.996-ko-0.9.2/doc/flow.png\n",
      "mecab-0.996-ko-0.9.2/doc/mecab.html\n",
      "mecab-0.996-ko-0.9.2/doc/index.html\n",
      "mecab-0.996-ko-0.9.2/doc/result.png\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/tab_a.png\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/globals_eval.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Tagger-members.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/functions_vars.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/doxygen.css\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/tab_r.gif\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Lattice.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/functions.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Tagger.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/mecab_8h_source.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/tabs.css\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/nav_f.png\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/tab_b.png\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/globals.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/nav_h.png\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/tab_h.png\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Model.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/globals_func.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/closed.png\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/tab_l.gif\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__path__t-members.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/functions_func.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/globals_type.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Lattice-members.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__node__t.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/namespacemembers_func.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/tab_s.png\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__dictionary__info__t-members.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/namespacemembers_type.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Model-members.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__dictionary__info__t.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/namespaces.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/namespacemembers.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/namespaceMeCab.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__path__t.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/files.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__node__t-members.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/index.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/annotated.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/globals_defs.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/classes.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/mecab_8h-source.html\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/doxygen.png\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/tab_b.gif\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/bc_s.png\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/open.png\n",
      "mecab-0.996-ko-0.9.2/doc/doxygen/mecab_8h.html\n",
      "mecab-0.996-ko-0.9.2/doc/dic.html\n",
      "mecab-0.996-ko-0.9.2/doc/partial.html\n",
      "mecab-0.996-ko-0.9.2/doc/feature.png\n",
      "mecab-0.996-ko-0.9.2/doc/Makefile.in\n",
      "mecab-0.996-ko-0.9.2/missing\n",
      "mecab-0.996-ko-0.9.2/BSD\n",
      "mecab-0.996-ko-0.9.2/NEWS\n",
      "mecab-0.996-ko-0.9.2/mkinstalldirs\n",
      "mecab-0.996-ko-0.9.2/src/\n",
      "mecab-0.996-ko-0.9.2/src/dictionary.h\n",
      "mecab-0.996-ko-0.9.2/src/writer.h\n",
      "mecab-0.996-ko-0.9.2/src/utils.h\n",
      "mecab-0.996-ko-0.9.2/src/string_buffer.cpp\n",
      "mecab-0.996-ko-0.9.2/src/tokenizer.cpp\n",
      "mecab-0.996-ko-0.9.2/src/make.bat\n",
      "mecab-0.996-ko-0.9.2/src/mecab.h\n",
      "mecab-0.996-ko-0.9.2/src/freelist.h\n",
      "mecab-0.996-ko-0.9.2/src/string_buffer.h\n",
      "mecab-0.996-ko-0.9.2/src/learner_tagger.h\n",
      "mecab-0.996-ko-0.9.2/src/dictionary_compiler.cpp\n",
      "mecab-0.996-ko-0.9.2/src/eval.cpp\n",
      "mecab-0.996-ko-0.9.2/src/mecab-system-eval.cpp\n",
      "mecab-0.996-ko-0.9.2/src/darts.h\n",
      "mecab-0.996-ko-0.9.2/src/param.h\n",
      "mecab-0.996-ko-0.9.2/src/char_property.h\n",
      "mecab-0.996-ko-0.9.2/src/learner_node.h\n",
      "mecab-0.996-ko-0.9.2/src/mecab-dict-gen.cpp\n",
      "mecab-0.996-ko-0.9.2/src/mecab-dict-index.cpp\n",
      "mecab-0.996-ko-0.9.2/src/winmain.h\n",
      "mecab-0.996-ko-0.9.2/src/thread.h\n",
      "mecab-0.996-ko-0.9.2/src/context_id.cpp\n",
      "mecab-0.996-ko-0.9.2/src/Makefile.am\n",
      "mecab-0.996-ko-0.9.2/src/connector.h\n",
      "mecab-0.996-ko-0.9.2/src/common.h\n",
      "mecab-0.996-ko-0.9.2/src/dictionary_rewriter.cpp\n",
      "mecab-0.996-ko-0.9.2/src/Makefile.msvc.in\n",
      "mecab-0.996-ko-0.9.2/src/dictionary_rewriter.h\n",
      "mecab-0.996-ko-0.9.2/src/feature_index.h\n",
      "mecab-0.996-ko-0.9.2/src/iconv_utils.cpp\n",
      "mecab-0.996-ko-0.9.2/src/char_property.cpp\n",
      "mecab-0.996-ko-0.9.2/src/mecab-test-gen.cpp\n",
      "mecab-0.996-ko-0.9.2/src/tagger.cpp\n",
      "mecab-0.996-ko-0.9.2/src/mecab-cost-train.cpp\n",
      "mecab-0.996-ko-0.9.2/src/learner.cpp\n",
      "mecab-0.996-ko-0.9.2/src/dictionary.cpp\n",
      "mecab-0.996-ko-0.9.2/src/lbfgs.cpp\n",
      "mecab-0.996-ko-0.9.2/src/ucs.h\n",
      "mecab-0.996-ko-0.9.2/src/writer.cpp\n",
      "mecab-0.996-ko-0.9.2/src/learner_tagger.cpp\n",
      "mecab-0.996-ko-0.9.2/src/lbfgs.h\n",
      "mecab-0.996-ko-0.9.2/src/libmecab.cpp\n",
      "mecab-0.996-ko-0.9.2/src/tokenizer.h\n",
      "mecab-0.996-ko-0.9.2/src/mecab.cpp\n",
      "mecab-0.996-ko-0.9.2/src/utils.cpp\n",
      "mecab-0.996-ko-0.9.2/src/dictionary_generator.cpp\n",
      "mecab-0.996-ko-0.9.2/src/param.cpp\n",
      "mecab-0.996-ko-0.9.2/src/context_id.h\n",
      "mecab-0.996-ko-0.9.2/src/mmap.h\n",
      "mecab-0.996-ko-0.9.2/src/viterbi.h\n",
      "mecab-0.996-ko-0.9.2/src/viterbi.cpp\n",
      "mecab-0.996-ko-0.9.2/src/stream_wrapper.h\n",
      "mecab-0.996-ko-0.9.2/src/feature_index.cpp\n",
      "mecab-0.996-ko-0.9.2/src/nbest_generator.h\n",
      "mecab-0.996-ko-0.9.2/src/ucstable.h\n",
      "mecab-0.996-ko-0.9.2/src/nbest_generator.cpp\n",
      "mecab-0.996-ko-0.9.2/src/iconv_utils.h\n",
      "mecab-0.996-ko-0.9.2/src/connector.cpp\n",
      "mecab-0.996-ko-0.9.2/src/Makefile.in\n",
      "mecab-0.996-ko-0.9.2/src/scoped_ptr.h\n",
      "mecab-0.996-ko-0.9.2/Makefile.in\n",
      "checking for a BSD-compatible install... /usr/bin/install -c\n",
      "checking whether build environment is sane... yes\n",
      "checking for a thread-safe mkdir -p... /usr/bin/mkdir -p\n",
      "checking for gawk... no\n",
      "checking for mawk... mawk\n",
      "checking whether make sets $(MAKE)... yes\n",
      "checking for gcc... gcc\n",
      "checking whether the C compiler works... yes\n",
      "checking for C compiler default output file name... a.out\n",
      "checking for suffix of executables... \n",
      "checking whether we are cross compiling... no\n",
      "checking for suffix of object files... o\n",
      "checking whether we are using the GNU C compiler... yes\n",
      "checking whether gcc accepts -g... yes\n",
      "checking for gcc option to accept ISO C89... none needed\n",
      "checking for style of include used by make... GNU\n",
      "checking dependency style of gcc... none\n",
      "checking for g++... g++\n",
      "checking whether we are using the GNU C++ compiler... yes\n",
      "checking whether g++ accepts -g... yes\n",
      "checking dependency style of g++... none\n",
      "checking how to run the C preprocessor... gcc -E\n",
      "checking for grep that handles long lines and -e... /usr/bin/grep\n",
      "checking for egrep... /usr/bin/grep -E\n",
      "checking whether gcc needs -traditional... no\n",
      "checking whether make sets $(MAKE)... (cached) yes\n",
      "checking build system type... x86_64-unknown-linux-gnu\n",
      "checking host system type... x86_64-unknown-linux-gnu\n",
      "checking how to print strings... printf\n",
      "checking for a sed that does not truncate output... /usr/bin/sed\n",
      "checking for fgrep... /usr/bin/grep -F\n",
      "checking for ld used by gcc... /usr/bin/ld\n",
      "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
      "checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n",
      "checking the name lister (/usr/bin/nm -B) interface... BSD nm\n",
      "checking whether ln -s works... yes\n",
      "checking the maximum length of command line arguments... 1572864\n",
      "checking whether the shell understands some XSI constructs... yes\n",
      "checking whether the shell understands \"+=\"... yes\n",
      "checking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... func_convert_file_noop\n",
      "checking how to convert x86_64-unknown-linux-gnu file names to toolchain format... func_convert_file_noop\n",
      "checking for /usr/bin/ld option to reload object files... -r\n",
      "checking for objdump... objdump\n",
      "checking how to recognize dependent libraries... pass_all\n",
      "checking for dlltool... dlltool\n",
      "checking how to associate runtime and link libraries... printf %s\\n\n",
      "checking for ar... ar\n",
      "checking for archiver @FILE support... @\n",
      "checking for strip... strip\n",
      "checking for ranlib... ranlib\n",
      "checking command to parse /usr/bin/nm -B output from gcc object... ok\n",
      "checking for sysroot... no\n",
      "checking for mt... no\n",
      "checking if : is a manifest tool... no\n",
      "checking for ANSI C header files... yes\n",
      "checking for sys/types.h... yes\n",
      "checking for sys/stat.h... yes\n",
      "checking for stdlib.h... yes\n",
      "checking for string.h... yes\n",
      "checking for memory.h... yes\n",
      "checking for strings.h... yes\n",
      "checking for inttypes.h... yes\n",
      "checking for stdint.h... yes\n",
      "checking for unistd.h... yes\n",
      "checking for dlfcn.h... yes\n",
      "checking for objdir... .libs\n",
      "checking if gcc supports -fno-rtti -fno-exceptions... no\n",
      "checking for gcc option to produce PIC... -fPIC -DPIC\n",
      "checking if gcc PIC flag -fPIC -DPIC works... yes\n",
      "checking if gcc static flag -static works... yes\n",
      "checking if gcc supports -c -o file.o... yes\n",
      "checking if gcc supports -c -o file.o... (cached) yes\n",
      "checking whether the gcc linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes\n",
      "checking whether -lc should be explicitly linked in... no\n",
      "checking dynamic linker characteristics... GNU/Linux ld.so\n",
      "checking how to hardcode library paths into programs... immediate\n",
      "checking whether stripping libraries is possible... yes\n",
      "checking if libtool supports shared libraries... yes\n",
      "checking whether to build shared libraries... yes\n",
      "checking whether to build static libraries... yes\n",
      "checking how to run the C++ preprocessor... g++ -E\n",
      "checking for ld used by g++... /usr/bin/ld -m elf_x86_64\n",
      "checking if the linker (/usr/bin/ld -m elf_x86_64) is GNU ld... yes\n",
      "checking whether the g++ linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes\n",
      "checking for g++ option to produce PIC... -fPIC -DPIC\n",
      "checking if g++ PIC flag -fPIC -DPIC works... yes\n",
      "checking if g++ static flag -static works... yes\n",
      "checking if g++ supports -c -o file.o... yes\n",
      "checking if g++ supports -c -o file.o... (cached) yes\n",
      "checking whether the g++ linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes\n",
      "checking dynamic linker characteristics... (cached) GNU/Linux ld.so\n",
      "checking how to hardcode library paths into programs... immediate\n",
      "checking for library containing strerror... none required\n",
      "checking whether byte ordering is bigendian... no\n",
      "checking for ld used by GCC... /usr/bin/ld -m elf_x86_64\n",
      "checking if the linker (/usr/bin/ld -m elf_x86_64) is GNU ld... yes\n",
      "checking for shared library run path origin... done\n",
      "checking for iconv... yes\n",
      "checking for working iconv... yes\n",
      "checking for iconv declaration... \n",
      "         extern size_t iconv (iconv_t cd, char * *inbuf, size_t *inbytesleft, char * *outbuf, size_t *outbytesleft);\n",
      "checking for ANSI C header files... (cached) yes\n",
      "checking for an ANSI C-conforming const... yes\n",
      "checking whether byte ordering is bigendian... (cached) no\n",
      "checking for string.h... (cached) yes\n",
      "checking for stdlib.h... (cached) yes\n",
      "checking for unistd.h... (cached) yes\n",
      "checking fcntl.h usability... yes\n",
      "checking fcntl.h presence... yes\n",
      "checking for fcntl.h... yes\n",
      "checking for stdint.h... (cached) yes\n",
      "checking for sys/stat.h... (cached) yes\n",
      "checking sys/mman.h usability... yes\n",
      "checking sys/mman.h presence... yes\n",
      "checking for sys/mman.h... yes\n",
      "checking sys/times.h usability... yes\n",
      "checking sys/times.h presence... yes\n",
      "checking for sys/times.h... yes\n",
      "checking for sys/types.h... (cached) yes\n",
      "checking dirent.h usability... yes\n",
      "checking dirent.h presence... yes\n",
      "checking for dirent.h... yes\n",
      "checking ctype.h usability... yes\n",
      "checking ctype.h presence... yes\n",
      "checking for ctype.h... yes\n",
      "checking for sys/types.h... (cached) yes\n",
      "checking io.h usability... no\n",
      "checking io.h presence... no\n",
      "checking for io.h... no\n",
      "checking windows.h usability... no\n",
      "checking windows.h presence... no\n",
      "checking for windows.h... no\n",
      "checking pthread.h usability... yes\n",
      "checking pthread.h presence... yes\n",
      "checking for pthread.h... yes\n",
      "checking for off_t... yes\n",
      "checking for size_t... yes\n",
      "checking size of char... 1\n",
      "checking size of short... 2\n",
      "checking size of int... 4\n",
      "checking size of long... 8\n",
      "checking size of long long... 8\n",
      "checking size of size_t... 8\n",
      "checking for size_t... (cached) yes\n",
      "checking for unsigned long long int... yes\n",
      "checking for stdlib.h... (cached) yes\n",
      "checking for unistd.h... (cached) yes\n",
      "checking for sys/param.h... yes\n",
      "checking for getpagesize... yes\n",
      "checking for working mmap... yes\n",
      "checking for main in -lstdc++... yes\n",
      "checking for pthread_create in -lpthread... yes\n",
      "checking for pthread_join in -lpthread... yes\n",
      "checking for getenv... yes\n",
      "checking for opendir... yes\n",
      "checking whether make is GNU Make... yes\n",
      "checking if g++ supports stl <vector> (required)... yes\n",
      "checking if g++ supports stl <list> (required)... yes\n",
      "checking if g++ supports stl <map> (required)... yes\n",
      "checking if g++ supports stl <set> (required)... yes\n",
      "checking if g++ supports stl <queue> (required)... yes\n",
      "checking if g++ supports stl <functional> (required)... yes\n",
      "checking if g++ supports stl <algorithm> (required)... yes\n",
      "checking if g++ supports stl <string> (required)... yes\n",
      "checking if g++ supports stl <iostream> (required)... yes\n",
      "checking if g++ supports stl <sstream> (required)... yes\n",
      "checking if g++ supports stl <fstream> (required)... yes\n",
      "checking if g++ supports template <class T> (required)... yes\n",
      "checking if g++ supports const_cast<> (required)... yes\n",
      "checking if g++ supports static_cast<> (required)... yes\n",
      "checking if g++ supports reinterpret_cast<> (required)... yes\n",
      "checking if g++ supports namespaces (required) ... yes\n",
      "checking if g++ supports __thread (optional)... yes\n",
      "checking if g++ supports template <class T> (required)... yes\n",
      "checking if g++ supports GCC native atomic operations (optional)... yes\n",
      "checking if g++ supports OSX native atomic operations (optional)... no\n",
      "checking if g++ environment provides all required features... yes\n",
      "configure: creating ./config.status\n",
      "config.status: creating Makefile\n",
      "config.status: creating src/Makefile\n",
      "config.status: creating src/Makefile.msvc\n",
      "config.status: creating man/Makefile\n",
      "config.status: creating doc/Makefile\n",
      "config.status: creating tests/Makefile\n",
      "config.status: creating swig/version.h\n",
      "config.status: creating mecab.iss\n",
      "config.status: creating mecab-config\n",
      "config.status: creating mecabrc\n",
      "config.status: creating config.h\n",
      "config.status: config.h is unchanged\n",
      "config.status: executing depfiles commands\n",
      "config.status: executing libtool commands\n",
      "config.status: executing default commands\n",
      "make  all-recursive\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "Making all in src\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "make[2]: Nothing to be done for 'all'.\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "Making all in man\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "make[2]: Nothing to be done for 'all'.\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "Making all in doc\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "make[2]: Nothing to be done for 'all'.\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "Making all in tests\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[2]: Nothing to be done for 'all'.\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "Making check in src\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "make[1]: Nothing to be done for 'check'.\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "Making check in man\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "make[1]: Nothing to be done for 'check'.\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "Making check in doc\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "make[1]: Nothing to be done for 'check'.\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "Making check in tests\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make  check-TESTS\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./unk.def ... 2\n",
      "emitting double-array: 100% |###########################################| \n",
      "./model.def is not found. skipped.\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./dic.csv ... 177\n",
      "emitting double-array: 100% |###########################################| \n",
      "reading ./matrix.def ... 178x178\n",
      "emitting matrix      : 100% |###########################################| \n",
      "\n",
      "done!\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./unk.def ... 2\n",
      "emitting double-array: 100% |###########################################| \n",
      "./model.def is not found. skipped.\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./dic.csv ... 83\n",
      "emitting double-array: 100% |###########################################| \n",
      "reading ./matrix.def ... 84x84\n",
      "emitting matrix      : 100% |###########################################| \n",
      "\n",
      "done!\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./unk.def ... 2\n",
      "emitting double-array: 100% |###########################################| \n",
      "./model.def is not found. skipped.\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./dic.csv ... 450\n",
      "emitting double-array: 100% |###########################################| \n",
      "reading ./matrix.def ... 1x1\n",
      "\n",
      "done!\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./unk.def ... 2\n",
      "emitting double-array: 100% |###########################################| \n",
      "./model.def is not found. skipped.\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./dic.csv ... 162\n",
      "emitting double-array: 100% |###########################################| \n",
      "reading ./matrix.def ... 3x3\n",
      "emitting matrix      : 100% |###########################################| \n",
      "\n",
      "done!\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./unk.def ... 2\n",
      "emitting double-array: 100% |###########################################| \n",
      "./model.def is not found. skipped.\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./dic.csv ... 4\n",
      "emitting double-array: 100% |###########################################| \n",
      "reading ./matrix.def ... 1x1\n",
      "\n",
      "done!\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./unk.def ... 11\n",
      "emitting double-array: 100% |###########################################| \n",
      "./model.def is not found. skipped.\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./dic.csv ... 1\n",
      "reading ./matrix.def ... 1x1\n",
      "\n",
      "done!\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./unk.def ... 2\n",
      "emitting double-array: 100% |###########################################| \n",
      "./model.def is not found. skipped.\n",
      "./pos-id.def is not found. minimum setting is used\n",
      "reading ./dic.csv ... 1\n",
      "reading ./matrix.def ... 1x1\n",
      "\n",
      "done!\n",
      "PASS: run-dics.sh\n",
      "PASS: run-eval.sh\n",
      "seed/pos-id.def is not found. minimum setting is used\n",
      "reading seed/unk.def ... 40\n",
      "emitting double-array: 100% |###########################################| \n",
      "seed/model.def is not found. skipped.\n",
      "seed/pos-id.def is not found. minimum setting is used\n",
      "reading seed/dic.csv ... 4335\n",
      "emitting double-array: 100% |###########################################| \n",
      "reading seed/matrix.def ... 1x1\n",
      "\n",
      "done!\n",
      "reading corpus ...\n",
      "Number of sentences: 34\n",
      "Number of features:  64108\n",
      "eta:                 0.00005\n",
      "freq:                1\n",
      "eval-size:           6\n",
      "unk-eval-size:       4\n",
      "threads:             1\n",
      "charset:             EUC-JP\n",
      "C(sigma^2):          1.00000\n",
      "\n",
      "iter=0 err=1.00000 F=0.35771 target=2406.28355 diff=1.00000\n",
      "iter=1 err=0.97059 F=0.65652 target=1484.25231 diff=0.38318\n",
      "iter=2 err=0.91176 F=0.79331 target=863.32765 diff=0.41834\n",
      "iter=3 err=0.85294 F=0.89213 target=596.72480 diff=0.30881\n",
      "iter=4 err=0.61765 F=0.95467 target=336.30744 diff=0.43641\n",
      "iter=5 err=0.50000 F=0.96702 target=246.53039 diff=0.26695\n",
      "iter=6 err=0.35294 F=0.95472 target=188.93963 diff=0.23361\n",
      "iter=7 err=0.20588 F=0.99106 target=168.62665 diff=0.10751\n",
      "iter=8 err=0.05882 F=0.99777 target=158.64865 diff=0.05917\n",
      "iter=9 err=0.08824 F=0.99665 target=154.14530 diff=0.02839\n",
      "iter=10 err=0.08824 F=0.99665 target=151.94257 diff=0.01429\n",
      "iter=11 err=0.02941 F=0.99888 target=147.20825 diff=0.03116\n",
      "iter=12 err=0.00000 F=1.00000 target=147.34956 diff=0.00096\n",
      "iter=13 err=0.02941 F=0.99888 target=146.32592 diff=0.00695\n",
      "iter=14 err=0.00000 F=1.00000 target=145.77299 diff=0.00378\n",
      "iter=15 err=0.02941 F=0.99888 target=145.24641 diff=0.00361\n",
      "iter=16 err=0.00000 F=1.00000 target=144.96490 diff=0.00194\n",
      "iter=17 err=0.02941 F=0.99888 target=144.90246 diff=0.00043\n",
      "iter=18 err=0.00000 F=1.00000 target=144.75959 diff=0.00099\n",
      "iter=19 err=0.00000 F=1.00000 target=144.71727 diff=0.00029\n",
      "iter=20 err=0.00000 F=1.00000 target=144.66337 diff=0.00037\n",
      "iter=21 err=0.00000 F=1.00000 target=144.61349 diff=0.00034\n",
      "iter=22 err=0.00000 F=1.00000 target=144.62987 diff=0.00011\n",
      "iter=23 err=0.00000 F=1.00000 target=144.60060 diff=0.00020\n",
      "iter=24 err=0.00000 F=1.00000 target=144.59125 diff=0.00006\n",
      "iter=25 err=0.00000 F=1.00000 target=144.58619 diff=0.00004\n",
      "iter=26 err=0.00000 F=1.00000 target=144.58219 diff=0.00003\n",
      "iter=27 err=0.00000 F=1.00000 target=144.58059 diff=0.00001\n",
      "\n",
      "Done! writing model file ... \n",
      "model-ipadic.c1.0.f1.model is not a binary model. reopen it as text mode...\n",
      "reading seed/unk.def ... 40\n",
      "reading seed/dic.csv ... 4335\n",
      "emitting model-ipadic.c1.0.f1.dic/left-id.def/ model-ipadic.c1.0.f1.dic/right-id.def\n",
      "emitting model-ipadic.c1.0.f1.dic/unk.def ... 40\n",
      "emitting model-ipadic.c1.0.f1.dic/dic.csv ... 4335\n",
      "emitting matrix      : 100% |###########################################| \n",
      "copying seed/char.def to model-ipadic.c1.0.f1.dic/char.def\n",
      "copying seed/rewrite.def to model-ipadic.c1.0.f1.dic/rewrite.def\n",
      "copying seed/dicrc to model-ipadic.c1.0.f1.dic/dicrc\n",
      "copying seed/feature.def to model-ipadic.c1.0.f1.dic/feature.def\n",
      "copying model-ipadic.c1.0.f1.model to model-ipadic.c1.0.f1.dic/model.def\n",
      "\n",
      "done!\n",
      "model-ipadic.c1.0.f1.dic/pos-id.def is not found. minimum setting is used\r\n",
      "reading model-ipadic.c1.0.f1.dic/unk.def ... 40\r\n",
      "emitting double-array: 100% |###########################################| \r\n",
      "model-ipadic.c1.0.f1.dic/pos-id.def is not found. minimum setting is used\r\n",
      "reading model-ipadic.c1.0.f1.dic/dic.csv ... 4335\r\n",
      "emitting double-array: 100% |###########################################| \r\n",
      "reading model-ipadic.c1.0.f1.dic/matrix.def ... 346x346\r\n",
      "emitting matrix      : 100% |###########################################| \n",
      "\n",
      "done!\n",
      "              precision          recall         F\n",
      "LEVEL 0:    12.8959(57/442) 11.8998(57/479) 12.3779\n",
      "LEVEL 1:    12.2172(54/442) 11.2735(54/479) 11.7264\n",
      "LEVEL 2:    11.7647(52/442) 10.8559(52/479) 11.2921\n",
      "LEVEL 4:    11.7647(52/442) 10.8559(52/479) 11.2921\n",
      "PASS: run-cost-train.sh\n",
      "==================\n",
      "All 3 tests passed\n",
      "==================\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "Making install in src\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "test -z \"/usr/local/lib\" || /usr/bin/mkdir -p \"/usr/local/lib\"\n",
      " /bin/bash ../libtool   --mode=install /usr/bin/install -c   libmecab.la '/usr/local/lib'\n",
      "libtool: install: /usr/bin/install -c .libs/libmecab.so.2.0.0 /usr/local/lib/libmecab.so.2.0.0\n",
      "libtool: install: (cd /usr/local/lib && { ln -s -f libmecab.so.2.0.0 libmecab.so.2 || { rm -f libmecab.so.2 && ln -s libmecab.so.2.0.0 libmecab.so.2; }; })\n",
      "libtool: install: (cd /usr/local/lib && { ln -s -f libmecab.so.2.0.0 libmecab.so || { rm -f libmecab.so && ln -s libmecab.so.2.0.0 libmecab.so; }; })\n",
      "libtool: install: /usr/bin/install -c .libs/libmecab.lai /usr/local/lib/libmecab.la\n",
      "libtool: install: /usr/bin/install -c .libs/libmecab.a /usr/local/lib/libmecab.a\n",
      "libtool: install: chmod 644 /usr/local/lib/libmecab.a\n",
      "libtool: install: ranlib /usr/local/lib/libmecab.a\n",
      "libtool: finish: PATH=\"/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/sbin\" ldconfig -n /usr/local/lib\n",
      "----------------------------------------------------------------------\n",
      "Libraries have been installed in:\n",
      "   /usr/local/lib\n",
      "\n",
      "If you ever happen to want to link against installed libraries\n",
      "in a given directory, LIBDIR, you must either use libtool, and\n",
      "specify the full pathname of the library, or use the `-LLIBDIR'\n",
      "flag during linking and do at least one of the following:\n",
      "   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable\n",
      "     during execution\n",
      "   - add LIBDIR to the `LD_RUN_PATH' environment variable\n",
      "     during linking\n",
      "   - use the `-Wl,-rpath -Wl,LIBDIR' linker flag\n",
      "   - have your system administrator add LIBDIR to `/etc/ld.so.conf'\n",
      "\n",
      "See any operating system documentation about shared libraries for\n",
      "more information, such as the ld(1) and ld.so(8) manual pages.\n",
      "----------------------------------------------------------------------\n",
      "test -z \"/usr/local/bin\" || /usr/bin/mkdir -p \"/usr/local/bin\"\n",
      "  /bin/bash ../libtool   --mode=install /usr/bin/install -c mecab '/usr/local/bin'\n",
      "libtool: install: /usr/bin/install -c .libs/mecab /usr/local/bin/mecab\n",
      "test -z \"/usr/local/libexec/mecab\" || /usr/bin/mkdir -p \"/usr/local/libexec/mecab\"\n",
      "  /bin/bash ../libtool   --mode=install /usr/bin/install -c mecab-dict-index mecab-dict-gen mecab-cost-train mecab-system-eval mecab-test-gen '/usr/local/libexec/mecab'\n",
      "libtool: install: /usr/bin/install -c .libs/mecab-dict-index /usr/local/libexec/mecab/mecab-dict-index\n",
      "libtool: install: /usr/bin/install -c .libs/mecab-dict-gen /usr/local/libexec/mecab/mecab-dict-gen\n",
      "libtool: install: /usr/bin/install -c .libs/mecab-cost-train /usr/local/libexec/mecab/mecab-cost-train\n",
      "libtool: install: /usr/bin/install -c .libs/mecab-system-eval /usr/local/libexec/mecab/mecab-system-eval\n",
      "libtool: install: /usr/bin/install -c .libs/mecab-test-gen /usr/local/libexec/mecab/mecab-test-gen\n",
      "test -z \"/usr/local/include\" || /usr/bin/mkdir -p \"/usr/local/include\"\n",
      " /usr/bin/install -c -m 644 mecab.h '/usr/local/include'\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
      "Making install in man\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "make[2]: Nothing to be done for 'install-exec-am'.\n",
      "test -z \"/usr/local/share/man/man1\" || /usr/bin/mkdir -p \"/usr/local/share/man/man1\"\n",
      " /usr/bin/install -c -m 644 mecab.1 '/usr/local/share/man/man1'\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
      "Making install in doc\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "make[2]: Nothing to be done for 'install-exec-am'.\n",
      "make[2]: Nothing to be done for 'install-data-am'.\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
      "Making install in tests\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[2]: Nothing to be done for 'install-exec-am'.\n",
      "make[2]: Nothing to be done for 'install-data-am'.\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
      "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "test -z \"/usr/local/bin\" || /usr/bin/mkdir -p \"/usr/local/bin\"\n",
      " /usr/bin/install -c mecab-config '/usr/local/bin'\n",
      "test -z \"/usr/local/etc\" || /usr/bin/mkdir -p \"/usr/local/etc\"\n",
      " /usr/bin/install -c -m 644 mecabrc '/usr/local/etc'\n",
      "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
      "--2022-11-19 16:12:31--  https://www.dropbox.com/s/i8girnk5p80076c/mecab-ko-dic-2.1.1-20180720.tar.gz?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/dl/i8girnk5p80076c/mecab-ko-dic-2.1.1-20180720.tar.gz [following]\n",
      "--2022-11-19 16:12:31--  https://www.dropbox.com/s/dl/i8girnk5p80076c/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucdb0a1fa7e531c5fcac8f604ef5.dl.dropboxusercontent.com/cd/0/get/BxA4SVFeOLldx2c8l9UT_YEXhpLct6ayxm_i5_Gd2RSAbjcFAtR0fpv1j8jAzYSbVN0UwjGAvqpygTWxEEmIYMypWXl74OPuUqDJ2apPMBBao6cljUoMc-LrrYg7Lz7fA8nGP0kipMTyEX6zP53k3PcVSyeCQXqoOfKpYt6D-Qd7A1DfFhu4Cio32eCFOnn0wDA/file?dl=1# [following]\n",
      "--2022-11-19 16:12:31--  https://ucdb0a1fa7e531c5fcac8f604ef5.dl.dropboxusercontent.com/cd/0/get/BxA4SVFeOLldx2c8l9UT_YEXhpLct6ayxm_i5_Gd2RSAbjcFAtR0fpv1j8jAzYSbVN0UwjGAvqpygTWxEEmIYMypWXl74OPuUqDJ2apPMBBao6cljUoMc-LrrYg7Lz7fA8nGP0kipMTyEX6zP53k3PcVSyeCQXqoOfKpYt6D-Qd7A1DfFhu4Cio32eCFOnn0wDA/file?dl=1\n",
      "Resolving ucdb0a1fa7e531c5fcac8f604ef5.dl.dropboxusercontent.com (ucdb0a1fa7e531c5fcac8f604ef5.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6016:15::a27d:10f\n",
      "Connecting to ucdb0a1fa7e531c5fcac8f604ef5.dl.dropboxusercontent.com (ucdb0a1fa7e531c5fcac8f604ef5.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 49775061 (47M) [application/binary]\n",
      "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz?dl=1.1’\n",
      "\n",
      "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  58.9MB/s    in 0.8s    \n",
      "\n",
      "2022-11-19 16:12:33 (58.9 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz?dl=1.1’ saved [49775061/49775061]\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "autoconf is already the newest version (2.69-11.1).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  accountsservice-ubuntu-schemas bc bluez-obexd cups cups-browsed cups-client\n",
      "  cups-common cups-core-drivers cups-daemon cups-filters\n",
      "  cups-filters-core-drivers cups-ipp-utils cups-ppdc cups-server-common\n",
      "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript\n",
      "  gir1.2-dbusmenu-glib-0.4 gnome-bluetooth gnome-power-manager\n",
      "  gnome-screensaver gsettings-ubuntu-schemas gvfs-backends indicator-applet\n",
      "  indicator-application indicator-appmenu indicator-bluetooth indicator-common\n",
      "  indicator-datetime indicator-keyboard indicator-messages indicator-power\n",
      "  indicator-printers indicator-session indicator-sound jayatana\n",
      "  libaccounts-glib0 libbamf3-2 libcdio-cdda2 libcdio-paranoia2 libcdio18\n",
      "  libcupsfilters1 libfcitx-config4 libfcitx-gclient1 libfcitx-utils0\n",
      "  libfontembed1 libgnome-panel0 libgs9 libgs9-common libido3-0.1-0 libijs-0.35\n",
      "  libindicator3-7 libjbig2dec0 liblightdm-gobject-1-0 liblouis-data liblouis20\n",
      "  liblouisutdml-bin liblouisutdml-data liblouisutdml9 libmessaging-menu0\n",
      "  libmtp-common libmtp-runtime libmtp9 libnfs13 libpaper-utils libpaper1\n",
      "  libpoppler-cpp0v5 libqpdf26 libunity-gtk2-parser0 libunity-gtk3-parser0\n",
      "  libunity-settings-daemon1 liburl-dispatcher1 lightdm nautilus-data\n",
      "  poppler-utils python3-psutil python3-xdg ssl-cert ubuntu-touch-sounds\n",
      "  unity-greeter unity-gtk-module-common unity-gtk2-module unity-gtk3-module\n",
      "  unity-settings-daemon unity-settings-daemon-schemas\n",
      "Use 'apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "mecab-ko-dic-2.1.1-20180720/\n",
      "mecab-ko-dic-2.1.1-20180720/configure\n",
      "mecab-ko-dic-2.1.1-20180720/COPYING\n",
      "mecab-ko-dic-2.1.1-20180720/autogen.sh\n",
      "mecab-ko-dic-2.1.1-20180720/Place-station.csv\n",
      "mecab-ko-dic-2.1.1-20180720/NNG.csv\n",
      "mecab-ko-dic-2.1.1-20180720/README\n",
      "mecab-ko-dic-2.1.1-20180720/EF.csv\n",
      "mecab-ko-dic-2.1.1-20180720/MAG.csv\n",
      "mecab-ko-dic-2.1.1-20180720/Preanalysis.csv\n",
      "mecab-ko-dic-2.1.1-20180720/NNB.csv\n",
      "mecab-ko-dic-2.1.1-20180720/Person-actor.csv\n",
      "mecab-ko-dic-2.1.1-20180720/VV.csv\n",
      "mecab-ko-dic-2.1.1-20180720/Makefile.in\n",
      "mecab-ko-dic-2.1.1-20180720/matrix.def\n",
      "mecab-ko-dic-2.1.1-20180720/EC.csv\n",
      "mecab-ko-dic-2.1.1-20180720/NNBC.csv\n",
      "mecab-ko-dic-2.1.1-20180720/clean\n",
      "mecab-ko-dic-2.1.1-20180720/ChangeLog\n",
      "mecab-ko-dic-2.1.1-20180720/J.csv\n",
      "mecab-ko-dic-2.1.1-20180720/.keep\n",
      "mecab-ko-dic-2.1.1-20180720/feature.def\n",
      "mecab-ko-dic-2.1.1-20180720/Foreign.csv\n",
      "mecab-ko-dic-2.1.1-20180720/XPN.csv\n",
      "mecab-ko-dic-2.1.1-20180720/EP.csv\n",
      "mecab-ko-dic-2.1.1-20180720/NR.csv\n",
      "mecab-ko-dic-2.1.1-20180720/left-id.def\n",
      "mecab-ko-dic-2.1.1-20180720/Place.csv\n",
      "mecab-ko-dic-2.1.1-20180720/Symbol.csv\n",
      "mecab-ko-dic-2.1.1-20180720/dicrc\n",
      "mecab-ko-dic-2.1.1-20180720/NP.csv\n",
      "mecab-ko-dic-2.1.1-20180720/ETM.csv\n",
      "mecab-ko-dic-2.1.1-20180720/IC.csv\n",
      "mecab-ko-dic-2.1.1-20180720/Place-address.csv\n",
      "mecab-ko-dic-2.1.1-20180720/Group.csv\n",
      "mecab-ko-dic-2.1.1-20180720/model.def\n",
      "mecab-ko-dic-2.1.1-20180720/XSN.csv\n",
      "mecab-ko-dic-2.1.1-20180720/INSTALL\n",
      "mecab-ko-dic-2.1.1-20180720/rewrite.def\n",
      "mecab-ko-dic-2.1.1-20180720/Inflect.csv\n",
      "mecab-ko-dic-2.1.1-20180720/configure.ac\n",
      "mecab-ko-dic-2.1.1-20180720/NNP.csv\n",
      "mecab-ko-dic-2.1.1-20180720/CoinedWord.csv\n",
      "mecab-ko-dic-2.1.1-20180720/XSV.csv\n",
      "mecab-ko-dic-2.1.1-20180720/pos-id.def\n",
      "mecab-ko-dic-2.1.1-20180720/Makefile.am\n",
      "mecab-ko-dic-2.1.1-20180720/unk.def\n",
      "mecab-ko-dic-2.1.1-20180720/missing\n",
      "mecab-ko-dic-2.1.1-20180720/VCP.csv\n",
      "mecab-ko-dic-2.1.1-20180720/install-sh\n",
      "mecab-ko-dic-2.1.1-20180720/Hanja.csv\n",
      "mecab-ko-dic-2.1.1-20180720/MAJ.csv\n",
      "mecab-ko-dic-2.1.1-20180720/XSA.csv\n",
      "mecab-ko-dic-2.1.1-20180720/Wikipedia.csv\n",
      "mecab-ko-dic-2.1.1-20180720/tools/\n",
      "mecab-ko-dic-2.1.1-20180720/tools/add-userdic.sh\n",
      "mecab-ko-dic-2.1.1-20180720/tools/mecab-bestn.sh\n",
      "mecab-ko-dic-2.1.1-20180720/tools/convert_for_using_store.sh\n",
      "mecab-ko-dic-2.1.1-20180720/user-dic/\n",
      "mecab-ko-dic-2.1.1-20180720/user-dic/nnp.csv\n",
      "mecab-ko-dic-2.1.1-20180720/user-dic/place.csv\n",
      "mecab-ko-dic-2.1.1-20180720/user-dic/person.csv\n",
      "mecab-ko-dic-2.1.1-20180720/user-dic/README.md\n",
      "mecab-ko-dic-2.1.1-20180720/NorthKorea.csv\n",
      "mecab-ko-dic-2.1.1-20180720/VX.csv\n",
      "mecab-ko-dic-2.1.1-20180720/right-id.def\n",
      "mecab-ko-dic-2.1.1-20180720/VA.csv\n",
      "mecab-ko-dic-2.1.1-20180720/char.def\n",
      "mecab-ko-dic-2.1.1-20180720/NEWS\n",
      "mecab-ko-dic-2.1.1-20180720/MM.csv\n",
      "mecab-ko-dic-2.1.1-20180720/ETN.csv\n",
      "mecab-ko-dic-2.1.1-20180720/AUTHORS\n",
      "mecab-ko-dic-2.1.1-20180720/Person.csv\n",
      "mecab-ko-dic-2.1.1-20180720/XR.csv\n",
      "mecab-ko-dic-2.1.1-20180720/VCN.csv\n",
      "Looking in current directory for macros.\n",
      "configure.ac:2: warning: AM_INIT_AUTOMAKE: two- and three-arguments forms are deprecated.  For more info, see:\n",
      "configure.ac:2: https://www.gnu.org/software/automake/manual/automake.html#Modernize-AM_005fINIT_005fAUTOMAKE-invocation\n",
      "checking for a BSD-compatible install... /usr/bin/install -c\n",
      "checking whether build environment is sane... yes\n",
      "/tmp/mecab-ko-dic-2.1.1-20180720/missing: Unknown `--is-lightweight' option\n",
      "Try `/tmp/mecab-ko-dic-2.1.1-20180720/missing --help' for more information\n",
      "configure: WARNING: 'missing' script is too old or missing\n",
      "checking for a thread-safe mkdir -p... /usr/bin/mkdir -p\n",
      "checking for gawk... no\n",
      "checking for mawk... mawk\n",
      "checking whether make sets $(MAKE)... yes\n",
      "checking whether make supports nested variables... yes\n",
      "checking for mecab-config... /usr/local/bin/mecab-config\n",
      "checking that generated files are newer than configure... done\n",
      "configure: creating ./config.status\n",
      "config.status: creating Makefile\n",
      "make: Nothing to be done for 'all'.\n",
      "make[1]: Entering directory '/tmp/mecab-ko-dic-2.1.1-20180720'\n",
      "make[1]: Nothing to be done for 'install-exec-am'.\n",
      " /usr/bin/mkdir -p '/usr/local/lib/mecab/dic/mecab-ko-dic'\n",
      " /usr/bin/install -c -m 644 model.bin matrix.bin char.bin sys.dic unk.dic left-id.def right-id.def rewrite.def pos-id.def dicrc '/usr/local/lib/mecab/dic/mecab-ko-dic'\n",
      "make[1]: Leaving directory '/tmp/mecab-ko-dic-2.1.1-20180720'\n",
      "fatal: destination path 'mecab-python-0.996' already exists and is not an empty directory.\n",
      "Requirement already satisfied: konlpy in /opt/conda/lib/python3.9/site-packages (0.5.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.9/site-packages (from konlpy) (4.6.3)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.9/site-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /opt/conda/lib/python3.9/site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from konlpy) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.9/site-packages (from konlpy) (1.21.4)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in /opt/conda/lib/python3.9/site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from tweepy>=3.7.0->konlpy) (1.16.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /opt/conda/lib/python3.9/site-packages (from tweepy>=3.7.0->konlpy) (2.26.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.26.12)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# mecab 설치\n",
    "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cdf37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    "    EarlyStoppingCallback\n",
    "\n",
    ")\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import EncoderDecoderModel\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45438db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = \"gogamza/kobart-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1733a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_textfile_path = \"data/train_text.csv\"\n",
    "val_textfile_path = \"data/val_text.csv\"\n",
    "\n",
    "with open(train_textfile_path, encoding=\"utf-8\") as f:\n",
    "            train_textlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]     \n",
    "\n",
    "with open(val_textfile_path, encoding=\"utf-8\") as f:\n",
    "            val_textlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d08f6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_textlines[0]\n",
    "del val_textlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3aa6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (..) 제거\n",
    "    sentence = re.sub(r'[#@]+[가-힣A-Za-z#]+', ' ', sentence)\n",
    "    sentence = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]+[/ㄱ-ㅎㅏ-ㅣ]', '', sentence) # 여러개 자음과 모음을 삭제한다.\n",
    "    sentence = re.sub(\"[^가-힣a-z0-9]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d94007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279992/279992 [00:08<00:00, 34766.41it/s]\n",
      "100%|██████████| 35004/35004 [00:01<00:00, 35002.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 (1)\n",
    "train_text = []\n",
    "val_text = []\n",
    "\n",
    "for tt in tqdm(train_textlines):\n",
    "    train_text.append(preprocess_sentence(tt))\n",
    "\n",
    "for vt in tqdm(val_textlines):\n",
    "      val_text.append(preprocess_sentence(vt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b2f5d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일이야 16일 아 이름 아 너 나한테 돈 보내주면 지금 할 수 잇옹 얼마야 최종 결제액이 잠시만 인당 952 900 합쳐서 1 905 800 근데 나중에 특가 뜰 수도 있으려나 좀 더 두고볼까 뜨기야 뜨겠지 웅웅 보니까 아시아나는 특가 이벤트 꽤 하는 것 같아서 일단 두고보장 그래 구럼 일단 자자'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f2e56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(zip(train_textlines), columns=['Text'])\n",
    "val_df = pd.DataFrame(zip(val_textlines), columns=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4238dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True, drop=True)\n",
    "val_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b93d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ 아 그 왕칫솔 또 사려...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>잠도안오네ㅐ얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일...\n",
       "1  kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...\n",
       "2  아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...\n",
       "3  칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ 아 그 왕칫솔 또 사려...\n",
       "4  잠도안오네ㅐ얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d11da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "\n",
    "def sentence_len_total(data):\n",
    "    text_split_text = []\n",
    "    # 반복문으로 Mecab 적용\n",
    "    for text_sen in tqdm(data['Text'].iloc[range(0, len(data))]):\n",
    "        text_split_text.append(mecab.morphs(text_sen))\n",
    "    \n",
    "    temp = pd.DataFrame(zip(text_split_text), columns=['Text'])\n",
    "    \n",
    "    # Mecab 적용 후 길이 출력\n",
    "    text_len = temp.Text.map(len)\n",
    "    \n",
    "    # text_len 사분위수 구하기    \n",
    "    text_Q1 = text_len.quantile(.25)\n",
    "    text_Q3 = text_len.quantile(.75)\n",
    "    text_IQR = text_Q3 - text_Q1\n",
    "    text_Q2 = text_len.quantile(.5)\n",
    "    text_Q4 = text_len.quantile(1)\n",
    "    text_threshold_len_left = text_Q1 - (1.5 * text_IQR)\n",
    "    text_threshold_len_right = text_Q3 + (1.5 * text_IQR)\n",
    "\n",
    "    \n",
    "    print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "    print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "    print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "    print('텍스트의 왼쪽 울타리 범위 : {}'. format(text_threshold_len_left),\n",
    "         '텍스트의 오른쪽 울타리 범위 : {}'. format(text_threshold_len_right))\n",
    "    print('text_Q1 = {}'.format(text_Q1), 'headlines_Q1 = {}'.format(text_Q1))\n",
    "    print('text_Q3 = {}'.format(text_Q3), 'headlines_Q3 = {}'.format(text_Q3))\n",
    "    print('text_IQR = {}'.format(text_IQR), 'headlines_IQR = {}'.format(text_IQR))\n",
    "    print('text_Q2 = {}'.format(text_Q2), 'headlines_Q2 = {}'.format(text_Q2))\n",
    "    print('text_Q4 = {}'.format(text_Q4), 'headlines_Q4 = {}'.format(text_Q4))\n",
    "\n",
    "    \n",
    "    plt.subplot(1,1,1)\n",
    "    plt.boxplot(text_len)\n",
    "    plt.title('text')\n",
    "    plt.show()\n",
    "\n",
    "    plt.title('text')\n",
    "    plt.hist(text_len, bins = 40)\n",
    "    plt.xlabel('length of samples')\n",
    "    plt.ylabel('number of samples')\n",
    "    plt.show()\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c411cafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279992/279992 [01:16<00:00, 3656.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 9\n",
      "텍스트의 최대 길이 : 937\n",
      "텍스트의 평균 길이 : 60.851017171919196\n",
      "텍스트의 왼쪽 울타리 범위 : -0.5 텍스트의 오른쪽 울타리 범위 : 115.5\n",
      "text_Q1 = 43.0 headlines_Q1 = 43.0\n",
      "text_Q3 = 72.0 headlines_Q3 = 72.0\n",
      "text_IQR = 29.0 headlines_IQR = 29.0\n",
      "text_Q2 = 55.0 headlines_Q2 = 55.0\n",
      "text_Q4 = 937.0 headlines_Q4 = 937.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ 아 그 왕칫솔 또 사려...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>잠도안오네ㅐ얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279987</th>\n",
       "      <td>도착하샸나염 자리잡고 알려주이소 아아 ㅋㅋㅋㅋ나 다이소좀 구경하느랔ㅋㅋㅋㅋㅋ 웅 이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279988</th>\n",
       "      <td>시간잘봐라 겁나 여러가지다 예약내역 올려바바 그러게 호텔은 언제해여 표사고 오늘밤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279989</th>\n",
       "      <td>언제 도착요정이십니까 15분뒤 도착이룝 옥희여 이제 나오묜 될듯 나나홨오 나와ㅉ오 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279990</th>\n",
       "      <td>근데 현인가요제 가면 최소20시간은 줄서서 기다려야하는거아님 ㅜ 그정도는아니고 한 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279991</th>\n",
       "      <td>아 그렇구나ㅋㅋㅋㅋㅋㅋㅋㅋ ㅋㅋㅋㅋㅋ어떄 대전 괜찮아 오 대전도 좋은거 같아요 ㅋㅋ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279992 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text\n",
       "0       그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일...\n",
       "1       kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...\n",
       "2       아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...\n",
       "3       칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ 아 그 왕칫솔 또 사려...\n",
       "4       잠도안오네ㅐ얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...\n",
       "...                                                   ...\n",
       "279987  도착하샸나염 자리잡고 알려주이소 아아 ㅋㅋㅋㅋ나 다이소좀 구경하느랔ㅋㅋㅋㅋㅋ 웅 이...\n",
       "279988      시간잘봐라 겁나 여러가지다 예약내역 올려바바 그러게 호텔은 언제해여 표사고 오늘밤\n",
       "279989  언제 도착요정이십니까 15분뒤 도착이룝 옥희여 이제 나오묜 될듯 나나홨오 나와ㅉ오 ...\n",
       "279990  근데 현인가요제 가면 최소20시간은 줄서서 기다려야하는거아님 ㅜ 그정도는아니고 한 ...\n",
       "279991  아 그렇구나ㅋㅋㅋㅋㅋㅋㅋㅋ ㅋㅋㅋㅋㅋ어떄 대전 괜찮아 오 대전도 좋은거 같아요 ㅋㅋ...\n",
       "\n",
       "[279992 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_len_total(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f28f4e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35004/35004 [00:09<00:00, 3595.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 10\n",
      "텍스트의 최대 길이 : 554\n",
      "텍스트의 평균 길이 : 61.192692263741286\n",
      "텍스트의 왼쪽 울타리 범위 : -2.0 텍스트의 오른쪽 울타리 범위 : 118.0\n",
      "text_Q1 = 43.0 headlines_Q1 = 43.0\n",
      "text_Q3 = 73.0 headlines_Q3 = 73.0\n",
      "text_IQR = 30.0 headlines_IQR = 30.0\n",
      "text_Q2 = 55.0 headlines_Q2 = 55.0\n",
      "text_Q4 = 554.0 headlines_Q4 = 554.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>웅 영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소로 엄청...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>너는 잘가라회사 선택 잘해 알겠어 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 많이 힘들구나 나도 이제 이력...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>느낌상 대통령까지는 아니고 오시면 여사님정도오시지않을까 ㅋㅋㅋ 그러면서 샘 여기있었...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>숨만수이ㅓ도 숨만쉬어도 100 이내 한달안에 일 무조건 해야대 아 딱한달 그냥 아무...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>목요일은 외근이구 금요일은 출장 금요일이 당진이양 아닝아닝 10일이 당진이야 그럼 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34999</th>\n",
       "      <td>방금 샐러드 정기배송 주문한 거 먹었눈데 시스템 사진 왜 먹어도 배고푸야 샐러드 정...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>닭갈비 맛있었다 ㅋㅋ 맛나맛나 그티웅 ㅋㅋㅋ 아깐 먹기 싫다더니 역시 너의 선택 ㅋ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35001</th>\n",
       "      <td>나 핫바 먹어야지 먹을래 아니 난 안 무거 그럼 나만 먹는다잉 핫바 찍어먹을 소스 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35002</th>\n",
       "      <td>아 본죽에 신메뉴 나온 거 알아 오 신메뉴 나온 걸 왜 못봤지 언니는 오늘 갔었는데...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35003</th>\n",
       "      <td>혹시 커피 시식하는거 남으면 그대 것을 아주 챙겨봅 아주감사합니다 엄마가 카누보다 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35004 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text\n",
       "0      웅 영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소로 엄청...\n",
       "1      너는 잘가라회사 선택 잘해 알겠어 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 많이 힘들구나 나도 이제 이력...\n",
       "2      느낌상 대통령까지는 아니고 오시면 여사님정도오시지않을까 ㅋㅋㅋ 그러면서 샘 여기있었...\n",
       "3      숨만수이ㅓ도 숨만쉬어도 100 이내 한달안에 일 무조건 해야대 아 딱한달 그냥 아무...\n",
       "4      목요일은 외근이구 금요일은 출장 금요일이 당진이양 아닝아닝 10일이 당진이야 그럼 ...\n",
       "...                                                  ...\n",
       "34999  방금 샐러드 정기배송 주문한 거 먹었눈데 시스템 사진 왜 먹어도 배고푸야 샐러드 정...\n",
       "35000  닭갈비 맛있었다 ㅋㅋ 맛나맛나 그티웅 ㅋㅋㅋ 아깐 먹기 싫다더니 역시 너의 선택 ㅋ...\n",
       "35001  나 핫바 먹어야지 먹을래 아니 난 안 무거 그럼 나만 먹는다잉 핫바 찍어먹을 소스 ...\n",
       "35002  아 본죽에 신메뉴 나온 거 알아 오 신메뉴 나온 걸 왜 못봤지 언니는 오늘 갔었는데...\n",
       "35003  혹시 커피 시식하는거 남으면 그대 것을 아주 챙겨봅 아주감사합니다 엄마가 카누보다 ...\n",
       "\n",
       "[35004 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_len_total(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea114d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text\n",
      "0  그럼 날짜는 가격 큰 변동 없으면 6 28 7 13로 확정할까 우리 비행포함 15일...\n",
      "1  kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...\n",
      "2  아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...\n",
      "3  칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남ㅋㅋㅋ 아 그 왕칫솔 또 사려...\n",
      "4  잠도안오네ㅐ얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...\n",
      "                                                Text\n",
      "0  웅 영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소로 엄청...\n",
      "1  너는 잘가라회사 선택 잘해 알겠어 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 많이 힘들구나 나도 이제 이력...\n",
      "2  느낌상 대통령까지는 아니고 오시면 여사님정도오시지않을까 ㅋㅋㅋ 그러면서 샘 여기있었...\n",
      "3  숨만수이ㅓ도 숨만쉬어도 100 이내 한달안에 일 무조건 해야대 아 딱한달 그냥 아무...\n",
      "4  목요일은 외근이구 금요일은 출장 금요일이 당진이양 아닝아닝 10일이 당진이야 그럼 ...\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())\n",
    "print(val_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b69c0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_pandas(train_df)\n",
    "val_data = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d0d7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 128\n",
    "max_target = 128\n",
    "batch_size = 4\n",
    "ignore_index = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c4dca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ignored_data(inputs, max_len, ignore_index):\n",
    "    if len(inputs) < max_len:\n",
    "        pad = [ignore_index] *(max_len - len(inputs)) # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)\n",
    "        inputs = np.concatenate([inputs, pad])\n",
    "    else:\n",
    "        inputs = inputs[:max_len]\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbc3b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "    label_id= []\n",
    "    label_ids = []\n",
    "\n",
    "    inputs = [dialogue for dialogue in data_to_process['Text']]\n",
    "    model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n",
    "\n",
    "    for i in range(len(data_to_process['Text'])):\n",
    "        label_id.append(tokenizer.encode(data_to_process['Text'][i]))  \n",
    "    for i in range(len(data_to_process['Text'])):\n",
    "        label_id[i].append(tokenizer.eos_token_id)\n",
    "        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))\n",
    "\n",
    "    model_inputs['labels'] = label_ids\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8b980c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd33d14f56d54bae9f4c2e92f887ac37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/280 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fee4d5eb834f3da5ac3b5499c87030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e8e78d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 128\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 2\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b82e59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     print(\"labels_ids\",labels_ids)\n",
    "#     print(\"labels_ids[labels_ids == -100]\",labels_ids[labels_ids == -100])\n",
    "#     print(\"tokenizer.pad_token_id\",tokenizer.pad_token_id)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "    rouge_output2 = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    rouge_outputL = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rougeL\"])[\"rougeL\"].mid\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"rouge1_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge1_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge1_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "        \n",
    "        \"rouge2_precision\": round(rouge_output2.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output2.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output2.fmeasure, 4), \n",
    "        \n",
    "        \"rougeL_precision\": round(rouge_outputL.precision, 4),\n",
    "        \"rougeL_recall\": round(rouge_outputL.recall, 4),\n",
    "        \"rougeL_fmeasure\": round(rouge_outputL.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c42b22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"MLM_pretrain_6ep_221120\",\n",
    "    num_train_epochs=5,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    logging_steps=5000,\n",
    "    save_total_limit=3,\n",
    "  #  evaluation_strategy = \"epoch\",# step별로 2버 loss가 오르는거 아니면 계속 반복하는듯\n",
    " #   load_best_model_at_end = True,\n",
    "#    save_strategy='epoch'\n",
    "\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eefc7a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3aa02564",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "132c319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 279992\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 87500\n",
      "  Number of trainable parameters = 123859968\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87500' max='87500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87500/87500 13:20:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.869700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.833400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.759600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>3.723800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>3.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>3.668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>3.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>3.649500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>3.622700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>3.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>3.613800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-1000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-1000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-1500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-1500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-2000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-2000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-2500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-2500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-3000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-3000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-3500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-3500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-4000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-4000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-4500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-4500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-5000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-5000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-5500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-5500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-6000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-6000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-6500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-6500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-7000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-7000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-7500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-7500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-8000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-8000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-8500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-8500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-9000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-9000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-9500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-9500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-10000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-10000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-10500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-10500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-11000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-11000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-11500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-11500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-12000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-12000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-12500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-12500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-13000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-13000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-13500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-13500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-14000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-14000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-14500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-14500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-15000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-15000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-15500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-15500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-16000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-16000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-16500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-16500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-17000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-17000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-17500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-17500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-18000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-18000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-18500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-18500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-19000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-19000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-19500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-19500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-20000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-20000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-20500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-20500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-20500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-21000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-21000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-21500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-21500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-21500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-22000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-22000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-22500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-22500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-22500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-23000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-23000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-23500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-23500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-23500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-24000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-24000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-24000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-24500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-24500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-24500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-25000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-25000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-25000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-25500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-25500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-25500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-26000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-26000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-26000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-26500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-26500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-26500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-27000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-27000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-27000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-27500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-27500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-27500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-28000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-28000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-28000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-28500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-28500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-28500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-29000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-29000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-29000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-29500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-29500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-29500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-30000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-30000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-30000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-30500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-30500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-30500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-31000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-31000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-31000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-31500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-31500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-31500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-32000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-32000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-32000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-32500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-32500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-32500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-33000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-33000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-33000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-33500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-33500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-33500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-34000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-34000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-34000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-34500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-34500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-34500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-35000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-35000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-35000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-35500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-35500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-35500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-36000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-36000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-36000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-36500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-36500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-36500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-35000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-37000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-37000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-37000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-37500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-37500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-37500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-38000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-38000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-38000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-38500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-38500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-38500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-37000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-39000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-39000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-39000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-37500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-39500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-39500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-39500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-38000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-40000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-40000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-40000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-38500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-40500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-40500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-40500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-40500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-40500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-39000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-41000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-41000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-41000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-39500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-41500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-41500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-41500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-41500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-41500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-40000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-42000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-42000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-42000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-40500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-42500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-42500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-42500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-42500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-42500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-41000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-43000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-43000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-43000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-41500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-43500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-43500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-43500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-43500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-43500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-42000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-44000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-44000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-44000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-44000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-42500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-44500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-44500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-44500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-44500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-44500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-43000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-45000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-45000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-45000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-45000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-45000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-43500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-45500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-45500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-45500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-45500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-45500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-44000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-46000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-46000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-46000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-46000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-46000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-44500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-46500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-46500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-46500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-46500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-46500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-45000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-47000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-47000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-47000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-47000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-47000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-45500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-47500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-47500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-47500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-47500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-47500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-46000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-48000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-48000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-48000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-46500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-48500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-48500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-48500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-48500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-48500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-47000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-49000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-49000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-49000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-47500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-49500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-49500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-49500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-49500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-49500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-48000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-50000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-50000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-50000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-48500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-50500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-50500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-50500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-50500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-50500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-49000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-51000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-51000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-51000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-49500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-51500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-51500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-51500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-51500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-51500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-52000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-52000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-52000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-52000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-52000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-50500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-52500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-52500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-52500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-52500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-52500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-51000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-53000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-53000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-53000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-53000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-53000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-51500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-53500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-53500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-53500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-53500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-53500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-52000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-54000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-54000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-54000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-54000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-54000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-52500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-54500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-54500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-54500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-54500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-54500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-53000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-55000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-55000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-55000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-55000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-55000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-53500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-55500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-55500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-55500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-55500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-55500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-54000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-56000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-56000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-56000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-56000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-56000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-54500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-56500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-56500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-56500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-56500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-56500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-55000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-57000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-57000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-57000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-57000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-57000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-55500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-57500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-57500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-57500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-57500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-57500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-56000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-58000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-58000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-58000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-58000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-58000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-56500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-58500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-58500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-58500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-58500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-58500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-57000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-59000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-59000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-59000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-59000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-59000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-57500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-59500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-59500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-59500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-59500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-59500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-58000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-60000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-60000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-60000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-60000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-60000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-58500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-60500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-60500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-60500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-60500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-60500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-59000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-61000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-61000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-61000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-61000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-61000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-59500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-61500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-61500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-61500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-61500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-61500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-60000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-62000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-62000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-62000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-62000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-62000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-60500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-62500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-62500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-62500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-62500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-62500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-61000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-63000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-63000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-63000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-63000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-63000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-61500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-63500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-63500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-63500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-63500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-63500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-62000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-64000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-64000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-64000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-62500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-64500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-64500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-64500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-64500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-64500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-63000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-65000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-65000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-65000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-65000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-65000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-63500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-65500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-65500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-65500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-65500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-65500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-64000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-66000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-66000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-66000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-66000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-66000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-64500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-66500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-66500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-66500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-66500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-66500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-65000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-67000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-67000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-67000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-67000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-67000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-65500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-67500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-67500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-67500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-67500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-67500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-66000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-68000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-68000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-68000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-68000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-68000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-66500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-68500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-68500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-68500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-68500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-68500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-67000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-69000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-69000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-69000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-69000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-69000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-67500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-69500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-69500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-69500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-69500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-69500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-68000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-70000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-70000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-70000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-70000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-70000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-68500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-70500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-70500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-70500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-70500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-70500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-69000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-71000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-71000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-71000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-71000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-71000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-69500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-71500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-71500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-71500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-71500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-71500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-70000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-72000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-72000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-72000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-72000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-72000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-70500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-72500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-72500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-72500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-72500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-72500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-71000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-73000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-73000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-73000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-73000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-73000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-71500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-73500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-73500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-73500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-73500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-73500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-72000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-74000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-74000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-74000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-74000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-74000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-72500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-74500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-74500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-74500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-74500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-74500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-73000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-75000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-75000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-75000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-75000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-75000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-73500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-75500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-75500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-75500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-75500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-75500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-74000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-76000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-76000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-76000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-76000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-76000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-74500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-76500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-76500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-76500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-76500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-76500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-75000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-77000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-77000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-77000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-77000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-77000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-75500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-77500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-77500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-77500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-77500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-77500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-76000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-78000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-78000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-78000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-78000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-78000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-76500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-78500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-78500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-78500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-78500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-78500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-77000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-79000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-79000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-79000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-79000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-79000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-77500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-79500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-79500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-79500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-79500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-79500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-78000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-80000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-80000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-80000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-80000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-80000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-78500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-80500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-80500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-80500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-80500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-80500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-79000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-81000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-81000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-81000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-81000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-81000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-79500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-81500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-81500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-81500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-81500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-81500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-80000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-82000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-82000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-82000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-82000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-82000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-80500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-82500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-82500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-82500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-82500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-82500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-81000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-83000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-83000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-83000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-83000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-83000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-81500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-83500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-83500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-83500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-83500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-83500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-82000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-84000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-84000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-84000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-84000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-84000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-82500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-84500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-84500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-84500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-84500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-84500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-83000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-85000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-85000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-85000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-85000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-85000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-83500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-85500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-85500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-85500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-85500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-85500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-84000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-86000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-86000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-86000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-86000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-86000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-84500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-86500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-86500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-86500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-86500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-86500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-85000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-87000\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-87000/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-87000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-87000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-87000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-85500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_6ep_221120/checkpoint-87500\n",
      "Configuration saved in MLM_pretrain_6ep_221120/checkpoint-87500/config.json\n",
      "Model weights saved in MLM_pretrain_6ep_221120/checkpoint-87500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_6ep_221120/checkpoint-87500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_6ep_221120/checkpoint-87500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_6ep_221120/checkpoint-86000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=87500, training_loss=3.8001913839285715, metrics={'train_runtime': 48030.9623, 'train_samples_per_second': 29.147, 'train_steps_per_second': 1.822, 'total_flos': 1.067008315097088e+17, 'train_loss': 3.8001913839285715, 'epoch': 5.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6329818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 35004\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2188' max='2188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2188/2188 1:24:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 8.68995189666748,\n",
       " 'eval_rouge1_precision': 0.0118,\n",
       " 'eval_rouge1_recall': 0.0342,\n",
       " 'eval_rouge1_fmeasure': 0.0157,\n",
       " 'eval_rouge2_precision': 0.0006,\n",
       " 'eval_rouge2_recall': 0.0023,\n",
       " 'eval_rouge2_fmeasure': 0.0008,\n",
       " 'eval_rougeL_precision': 0.0117,\n",
       " 'eval_rougeL_recall': 0.0339,\n",
       " 'eval_rougeL_fmeasure': 0.0156,\n",
       " 'eval_runtime': 5099.0122,\n",
       " 'eval_samples_per_second': 6.865,\n",
       " 'eval_steps_per_second': 0.429,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d361550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /aiffel/.cache/huggingface/hub/models--gogamza--kobart-base-v2/snapshots/f9f2ec35d3c32a1ecc7a3281f9626b7ec1913fed/config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /aiffel/.cache/huggingface/hub/models--gogamza--kobart-base-v2/snapshots/f9f2ec35d3c32a1ecc7a3281f9626b7ec1913fed/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at gogamza/kobart-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4503/1767568221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mck_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mtest_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mck_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# 0, len(test_data), 200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, num_beams=2,no_repeat_ngram_size=2, max_length=128,\n",
    "                            suppress_tokens= [234,23782,14338,240,199,198,161,116, 14338, 239], \n",
    "                             attention_mask=attention_mask, top_p=0.92)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)# 여기에 기본 kobart가져오기?\n",
    "import random\n",
    "from random import randrange\n",
    "ck_num = len(test_data)\n",
    "test_samples = test_data.select(range(0, ck_num, 500))# 0, len(test_data), 200\n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1] # 여기에 체크포인트 가져오기 \n",
    "# 연구해봐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(summaries_after_tuning)):\n",
    "    print('idx_{} '.format(i))\n",
    "    print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f5f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
