{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab523a6a",
   "metadata": {},
   "source": [
    "## 1.Import 및 라이브러리 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa872793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aaa3cab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.21.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: datasets==1.0.2 in /opt/conda/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.0.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.3.3)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.21.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (4.62.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (0.3.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers==4.24.0 in /opt/conda/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (1.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2021.11.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.10.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2021.10.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformer-utils in /opt/conda/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.62.3)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (1.9.1+cu111)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.24.0)\n",
      "Requirement already satisfied: colorcet in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (3.0.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (0.11.2)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /opt/conda/lib/python3.9/site-packages (from colorcet->transformer-utils) (0.4.8)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.7.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.21.4)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.3.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->transformer-utils) (4.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (3.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2021.11.10)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2.26.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (3.0.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (8.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->seaborn->transformer-utils) (2021.3)\n",
      "Requirement already satisfied: param>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from pyct>=0.4.4->colorcet->transformer-utils) (1.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn->transformer-utils) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (0.13.5)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.0.10)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.10.1)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "!pip install datasets==1.0.2\n",
    "!pip install transformers==4.24.0\n",
    "!pip install transformer-utils\n",
    "!pip install packaging\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117e702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments\n",
    "\n",
    ")\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import EncoderDecoderModel\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64bcf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "# 20per data\n",
    "train_20per = pd.read_csv('data/train_20per.csv')\n",
    "val_20per = pd.read_csv('data/val_20per.csv')\n",
    "\n",
    "# # 2~3sent\n",
    "# train_2-3sent = pd.read_csv('data/train_2-3sent.csv')\n",
    "# val_2-3sent = pd.read_csv('data/val_2-3sent.csv')\n",
    "\n",
    "# # sum1\n",
    "# train_Sum1 = pd.read_csv('data/train_Sum1.csv')\n",
    "# val_Sum1 = pd.read_csv('data/val_Sum1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50c3bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text        73431\n",
      "Summary     73431\n",
      "Category    73431\n",
      "dtype: int64\n",
      "Text        9150\n",
      "Summary     9150\n",
      "Category    9150\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_20per.count())\n",
    "print(val_20per.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981e4cb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speech        16000\n",
       "minute        13600\n",
       "news_r        10800\n",
       "briefing       8000\n",
       "literature     4800\n",
       "narration      4231\n",
       "his_cul        4000\n",
       "paper          4000\n",
       "edit           4000\n",
       "public         4000\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_20per['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c617bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지어내버린 대목부터는 흥분이 버썩 줄어지었다 ──. \"선생님! 또 기침이 나고 토...</td>\n",
       "      <td>그래서 나는 곧 H에게 간단하고 힘 있는 답장을 썼다 ──. \"선생님! 또 기침이 ...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이 송아지가 젖을 떼우고 집으로 끌고 오던 날은 첨지는 개선장군이 성안 에 들어올...</td>\n",
       "      <td>이 송아지가 젖을 떼우고 집으로 끌고 오던 날은 첨지는 개선장군이 성안 에 들어올 ...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>어떤 여름날 밤 손주딸에게 관한 불길한 꿈을 꾼 이 한머니는 이튿날 조반 후에 생...</td>\n",
       "      <td>어떤 여름날 밤 손주딸에게 관한 불길한 꿈을 꾼 이 한머니는 이튿날 조반 후에 생각...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이러한 가운데서 왕후는 자기의 입장을 위태롭게 여기고 겸하여 장래 자기 의 몸으로...</td>\n",
       "      <td>자기는 태자의 위 따위는 부럽지 않다. 이 전 낙랑공주가 아직 살아 있고 그 낙랑공...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“가겠소.” “언니가 나오시면 일러드 리겠으니 그때까지는 찾아오지 않으시는 것이 ...</td>\n",
       "      <td>별안간 골목쟁이에서 쑥 내달아 붙잡지나 않을까를 염려하여 빠른 걸음으 로 골목 골목...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0   지어내버린 대목부터는 흥분이 버썩 줄어지었다 ──. \"선생님! 또 기침이 나고 토...   \n",
       "1   이 송아지가 젖을 떼우고 집으로 끌고 오던 날은 첨지는 개선장군이 성안 에 들어올...   \n",
       "2   어떤 여름날 밤 손주딸에게 관한 불길한 꿈을 꾼 이 한머니는 이튿날 조반 후에 생...   \n",
       "3   이러한 가운데서 왕후는 자기의 입장을 위태롭게 여기고 겸하여 장래 자기 의 몸으로...   \n",
       "4   “가겠소.” “언니가 나오시면 일러드 리겠으니 그때까지는 찾아오지 않으시는 것이 ...   \n",
       "\n",
       "                                             Summary    Category  \n",
       "0  그래서 나는 곧 H에게 간단하고 힘 있는 답장을 썼다 ──. \"선생님! 또 기침이 ...  literature  \n",
       "1  이 송아지가 젖을 떼우고 집으로 끌고 오던 날은 첨지는 개선장군이 성안 에 들어올 ...  literature  \n",
       "2  어떤 여름날 밤 손주딸에게 관한 불길한 꿈을 꾼 이 한머니는 이튿날 조반 후에 생각...  literature  \n",
       "3  자기는 태자의 위 따위는 부럽지 않다. 이 전 낙랑공주가 아직 살아 있고 그 낙랑공...  literature  \n",
       "4  별안간 골목쟁이에서 쑥 내달아 붙잡지나 않을까를 염려하여 빠른 걸음으 로 골목 골목...  literature  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_20per.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aacdaaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_20per = train_20per.loc[(train_20per.Category != 'literature')& \n",
    "               (train_20per.Category != 'minute')&\n",
    "               (train_20per.Category != 'edit')&\n",
    "               (train_20per.Category != 'speech')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae20f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_20per = val_20per.loc[(val_20per.Category != 'literature')& \n",
    "               (val_20per.Category != 'minute')&\n",
    "               (val_20per.Category != 'edit')&\n",
    "               (val_20per.Category != 'speech')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5811a887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>방문객이 상상으로 연주하는 곳.\\n쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거...</td>\n",
       "      <td>쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거리에는 공연과 전시를 홍보하는 포스...</td>\n",
       "      <td>narration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...</td>\n",
       "      <td>저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...</td>\n",
       "      <td>narration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>이제 뭐 훈련된 익숙한 인력이 투입이 돼야 되는데 이게 쉽지가 않은 거죠. 그래서 ...</td>\n",
       "      <td>게다가 지금의 발생양상이 지속되거나 더 나빠진다면 한 1~2주 정도 지나면 중환자 ...</td>\n",
       "      <td>narration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>하지만 지금은 연어가 아닌 산천어가 이 곳의 주인이 되었다.\\n그 옛날 바다로 나가...</td>\n",
       "      <td>금강 최상류에 위치한 용담댐. 2001년 건설된 댐으로 우리나라에서 다섯번째로 큰 ...</td>\n",
       "      <td>narration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>티 없이 맑은 어린아이.\\n금발의 젊은 여성.\\n그리고 말을 타고 해변을 산책하는 ...</td>\n",
       "      <td>피우라 시에서 직접 운영하는 박물관을 찾았다. 비쿠스 왕조의 금 장식물들이 유명한 ...</td>\n",
       "      <td>narration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "4800  방문객이 상상으로 연주하는 곳.\\n쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거...   \n",
       "4801  저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...   \n",
       "4802  이제 뭐 훈련된 익숙한 인력이 투입이 돼야 되는데 이게 쉽지가 않은 거죠. 그래서 ...   \n",
       "4803  하지만 지금은 연어가 아닌 산천어가 이 곳의 주인이 되었다.\\n그 옛날 바다로 나가...   \n",
       "4804  티 없이 맑은 어린아이.\\n금발의 젊은 여성.\\n그리고 말을 타고 해변을 산책하는 ...   \n",
       "\n",
       "                                                Summary   Category  \n",
       "4800  쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거리에는 공연과 전시를 홍보하는 포스...  narration  \n",
       "4801  저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...  narration  \n",
       "4802  게다가 지금의 발생양상이 지속되거나 더 나빠진다면 한 1~2주 정도 지나면 중환자 ...  narration  \n",
       "4803  금강 최상류에 위치한 용담댐. 2001년 건설된 댐으로 우리나라에서 다섯번째로 큰 ...  narration  \n",
       "4804  피우라 시에서 직접 운영하는 박물관을 찾았다. 비쿠스 왕조의 금 장식물들이 유명한 ...  narration  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_20per.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e40a8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_20per= train_20per.drop(columns='Category')\n",
    "val_20per = val_20per.drop(columns='Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27d592d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_20per.reset_index(inplace=True, drop=True)\n",
    "val_20per.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac1eebb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>방문객이 상상으로 연주하는 곳.\\n쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거...</td>\n",
       "      <td>쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거리에는 공연과 전시를 홍보하는 포스...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...</td>\n",
       "      <td>저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이제 뭐 훈련된 익숙한 인력이 투입이 돼야 되는데 이게 쉽지가 않은 거죠. 그래서 ...</td>\n",
       "      <td>게다가 지금의 발생양상이 지속되거나 더 나빠진다면 한 1~2주 정도 지나면 중환자 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>하지만 지금은 연어가 아닌 산천어가 이 곳의 주인이 되었다.\\n그 옛날 바다로 나가...</td>\n",
       "      <td>금강 최상류에 위치한 용담댐. 2001년 건설된 댐으로 우리나라에서 다섯번째로 큰 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>티 없이 맑은 어린아이.\\n금발의 젊은 여성.\\n그리고 말을 타고 해변을 산책하는 ...</td>\n",
       "      <td>피우라 시에서 직접 운영하는 박물관을 찾았다. 비쿠스 왕조의 금 장식물들이 유명한 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  방문객이 상상으로 연주하는 곳.\\n쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거...   \n",
       "1  저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...   \n",
       "2  이제 뭐 훈련된 익숙한 인력이 투입이 돼야 되는데 이게 쉽지가 않은 거죠. 그래서 ...   \n",
       "3  하지만 지금은 연어가 아닌 산천어가 이 곳의 주인이 되었다.\\n그 옛날 바다로 나가...   \n",
       "4  티 없이 맑은 어린아이.\\n금발의 젊은 여성.\\n그리고 말을 타고 해변을 산책하는 ...   \n",
       "\n",
       "                                             Summary  \n",
       "0  쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거리에는 공연과 전시를 홍보하는 포스...  \n",
       "1  저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...  \n",
       "2  게다가 지금의 발생양상이 지속되거나 더 나빠진다면 한 1~2주 정도 지나면 중환자 ...  \n",
       "3  금강 최상류에 위치한 용담댐. 2001년 건설된 댐으로 우리나라에서 다섯번째로 큰 ...  \n",
       "4  피우라 시에서 직접 운영하는 박물관을 찾았다. 비쿠스 왕조의 금 장식물들이 유명한 ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_20per.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "565a471f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       35031\n",
      "Summary    35031\n",
      "dtype: int64\n",
      "Text       4350\n",
      "Summary    4350\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_20per.count())\n",
    "print(val_20per.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78afafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 괄호까지 없앨수 있도록 구현해야 겠음, 결과물에서 괄호가 너무 방해됨\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거\n",
    "    sentence = re.sub(\"[()]\",\"\", sentence)\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 제거\n",
    "    sentence = re.sub(\"'\",'', sentence) # 따옴표 제거\n",
    "    sentence = re.sub('\\n','', sentence) # \\n \" 제거\n",
    "    sentence = re.sub('.{2,3}\\W{0,1}기자','', sentence) # 기자 이름 제거\n",
    "    sentence = re.sub(r'[?.!,][/?.!,]', '', sentence) # 여러개 문장 부호를 하나의 문장부호로 바꿉니다\n",
    "    sentence = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-z0-9]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ec7ab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35031/35031 [00:07<00:00, 4459.76it/s]\n",
      "100%|██████████| 4350/4350 [00:00<00:00, 20409.03it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_text = []\n",
    "clean_headlines = []\n",
    "\n",
    "for i in tqdm(train_20per['Text']):\n",
    "    clean_text.append(preprocess_sentence(i))\n",
    "    \n",
    "for i in tqdm(val_20per['Summary']):\n",
    "    clean_headlines.append(preprocess_sentence(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9505d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['방문객이 상상으로 연주하는 곳 쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거리에는 공연과 전시를 홍보하는 포스터들이 옹기종기 붙어 있다 이곳은 단순한 예술가 거리가 아니다 독자적인 헌법이 있는 엄연한 자칭 공화국이다 41개의 헌법 조항에 차마 포함되지 못한 예민한 사항들은 이렇게 거리에서 논의되기도 한다 여의도 면적의 5분의 1밖에 안되는 작은 공화국에는 우체국을 겸하는 입국심사대도 있다 이곳에서 무료로 도착 비자가 발급된다 이 사람은 우주피스의 대통령 로마스 릴레이키스다 영화감독인 그는 1997년 이 지역에 살던 예술가들과 함께 아무도 인정하지 않지만 누구나 오고 싶은 나라를 만들었다 빌뉴스에서 북서쪽으로 002시간 반을 달려 도착한 샤 울랴이시 근교의 시골 마을 집도 한 채 안 보이는 허허발판에', '저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니다 그동안 소위 대통령 탄핵이라고 하는 그런 아주 비정상적 상황 속에서 출발한 정권이거든요 나름의 그 어려움을 뚫고 지금까지 와 있는데 물론 부족하고 모자란 측면도 많이 있을 겁니다 또 나머지 임기 2년을 무사히 또 현재 이런 코로나 경제위기와 같은 이런 어려운 상황을 뚫고 나가려면 국민들이 이번 선거에서 보다 더 연료를 채워주시는 그래서 저희는 중간 급유를 해주는 그런 선거다 이렇게 좀 생각을 하는데 그런 면에서 투표률이 저희도 좀 높아지길 원하고 있습니다 근데 엊그제 다행스러운 건 여론조사 결과를 보니까 지난 총선 대비 투표를 하겠다고 하는 의향률이 한 10 이상 올라갔습니다 우리 걱정과는 달리 그래서 저희는 이번 과정을 보면서 뭔가 이런 경제적인 위기나 어려움 속에서 국가의 존재가 무엇이어야 되는가 뭐 이런 거와 관련해서 국민들의 관심이 이제 높아진 것 같습니다 재난기본지원금 재난 그 지원금 이런 것들 때문에', '이제 뭐 훈련된 익숙한 인력이 투입이 돼야 되는데 이게 쉽지가 않은 거죠 그래서 단기간에 확충이 좀 어려운 면이 있는데 사실은 지금 발생한 환자 숫자만 하더라도 이 환자 분들이 당장 중증이 아니지만 대개 한 1주일 열흘 정도 지나면서 일부가 중증으로 진행할 가능성이 높고 그렇다면 일정 비율이 그렇게 간다고 생각하면 현재 남아 있는 중환자 병상이 이미 발생한 환자들 만으로도 충분히 찰 가능성이 높다는 거죠 게다가 지금의 발생양상이 지속되거나 더 나빠진다면 한 1 2주 정도 지나면 중환자 병상이 부족해지는 사태가 발생할 거라는 위기감을 갖고 있습니다', '하지만 지금은 연어가 아닌 산천어가 이 곳의 주인이 되었다 그 옛날 바다로 나가지 못한 연어가 그대로 민물에서 자라 정착한 종 즉 육봉화한 녀석들이다 육봉종은 강과 바다의 길이 막히면서 생기기 시작했다 금강 최상류에 위치한 용담댐 2001년 건설된 댐으로 우리나라에서 다섯번째로 큰 댐이다 댐의 상류 바다와는 수백킬로미터 떨어진 내륙 깊숙한 산골짜기에 은어가 살고 있다 은어는 해마다 봄이 오면 바다에서 올라오는 대표적인 회귀어종 하지만 이곳의 은어는 바다에서 올라온 것이 아니다 3년 전 진안군에서 어족자원 보호를 위해 양식은어를 방류했는데 댐에 갇혀 육봉화한 은어다 댐이 들어서면서 볼 수 없었던 은어 오랜만에 다시 만난 은어에게서 사람들은 그 옛날 먹던 추억의 맛을 느낀다 은어에서는 수박냄새가 난다는데 이건 큰 거여서 수박냄새가 안 나는 것 같아', '티 없이 맑은 어린아이 금발의 젊은 여성 그리고 말을 타고 해변을 산책하는 광경은 놓칠 수 없는 만코라의 볼거리들이다 해변에서의 모래찜질은 어느 곳 에서나 즐길 수 있는 단골 놀이다 참 행복한 일상의 순간들이다 해변에 인접한 레스토랑에서 허기진 배를 채우고 있으니 하루가 짧다는 생각이 든다 피우라 시에서 직접 운영하는 박물관을 찾았다 비쿠스 왕조의 금 장식물들이 유명한 곳이다 황금색은 우리 눈을 끄는 무언의 힘이 있다 원주민 복장을 한 밀랍인형은 관광객들에게 인기다 비록 제한된 공간이지만 시간 여행을 온 듯한 체험을 할 수 있기 때문이다 다양한 금 장식물은 비쿠스 왕조가 화려했음을 보여준다 금목걸이와 금관 금으로 된 갑옷까지 페루의 잉카에는 유독 황금 얘기가 많다 후일 탐욕스러운 침략자들에게 큰 화를 입은 이유이기도 하다', '한참을 달리다보니 하얀 지평선 속으로 내가 빨려 들어가고 있었다 땅과 하늘의 경계가 사라진다 약 40분을 달려 우유니 사막 한가운데에 있는 물고기섬에 도착했다 물고기처럼 떠 있는 섬 같다고 해서 붙여진 이름이다 잉카와시 섬이라고도 하는데 잉카의 정령들이 머물렀다는 전설이 있다 물고기섬은 콜차니에서 약 70km 정도 떨어져있다 섬을 바라보니 정말 물고기처럼 보인다 아마도 옛 잉카인들은 풀 한 포기 없는 사막 한가운데에 선인장 같은 생명이 자라고 있어 이곳을 신성시했을 것이다 이 선인장은 1년에 1cm씩만 자라는데 가장 오래된 선인장은 900년이나 된 것도 있다고 한다 물고기섬 정상을 향해 올라갔다 위에는 기암괴석들이 있는데 오랜 시간 소금이 바람에 날려 바위에 붙은 결정체들이다', '대자연을 온전히 느껴보는 여유 이것이 지중해 여행이 주는 선물이 아닐까 절벽 위에 위치한 네르하는 유럽의 발코니라 불린다 자세히 살펴보면 절벽이 실제 발코니처럼 돌출돼 있다 내가 지금껏 본 발코니 중 가장 예쁘고 낭만적이다 작은 마을에 불과했던 네르하가 세상에 알려지게 된 건 스페인의 왕이었던 알폰소 12세 덕분 그는 아름다운 경치에 감동받아 네르하에유럽의 발코니라는 이름을 붙여줬다 발코니에서는 지중해의 아름다운 풍광은 물론 네르하의 작고 아담한 해변들 그리고 스페인의 이국적인 풍경을 만들어주는하얀 집들을 모두 조망할 수 있다 유럽의 발코니에서 10분 거리에 부리아나 해변이 있다 아름다운 지중해를 더 가까이 직접 느껴보고 싶은 마음에 전문가를 찾아 나선 길 요즘 유행이라는 카약을 타보기로 했다 길이 약 7m 너비는 50cm', '졸업식을 맞이해 어린 시절의 꿈과 희망을 간직하자는 의미에서 시작된 붉은 돛 축제 이날 화려한 불꽃은 밤하늘을 붉게 물들였다 다음 날 아침 성 이삭 성당으로 향했다 세계에서 세 번째로 큰 규모를 자랑하는 이 성당은 1818 년 공사를 시작해 40년 만에 완공됐다 100m가 넘는 거대한 크기와 황금빛으로 빛나는 지붕이 돋보인다 성당 안으로 들어가봤다 화려한 내부의 모습과 꼭대기의 돛 모양이 눈길을 끈다 천국을 상징하는 지붕에는 12사도의 모습이 그려져 있다 주변에는 22명의 화가들이 그린 벽화와 캔버스 그림이 전시돼 있다 천장과 바닥까지 모두 섬세하게 꾸며져 있다 관광객들과 순례자들의 발길이 끊이질 않는다 약 1000년 전 다양한 종교와 토속신앙이 난무했던 러시아는 통일된 종교가 필요했다', 'kbs의 빅데이터 분석을 통해 선정한 지난 한 주간 네이버에서 가장 많이 본 뉴스입니다 정치 분야입니다 울산시장 민주당 후보 공천 과정에서 대통령과 청와대의 개입 의혹이 담긴 송병기 울산부시장은 업무일지 메모가 나와 검찰이 조사 중이라는 조선일보 기사입니다 이에 대해 청와대는 전형적인 허위 보도라고 반박했습니다 경제 분야입니다 마크롱 프랑스 대통령이 연금개혁을 위해 월 2 500만원의 대통령 연금을 포기하겠다고 밝히면서 한국의 대통령 연금에 대해 알려주는 기사입니다 한국 대통령은 퇴임하면 재직 연봉의 95 를 연금으로 받게 됩니다 사회 분야에선 초등학교 5학년 여학생이 친구에게 흉기를 휘둘러 숨지게 했다는 소식입니다 가족에 대한 험담 때문에 범행을 저지른 것으로 알려졌는데 가해 학생은 14세 미만이어서 형사처벌 대신 보호 처분을 받게 됩니다 생활 분야에선 손석희 jtbc 사장이 6년 4개월 만에 간판 뉴스 뉴스룸 앵커에서 하차한다는 소식입니다 손 사장의 하차를 두고 뉴스룸 시청률 하락 등과 무관하지 않다는 분석이 나옵니다 세계 분야에선 성탄절 전날 스페인의 한 리조트에서 영국인 가족 2명이 수영장에서 익사했다는 기사입니다 수영장에 빠진 9살 딸을 구하기 위해 아버지와 오빠가 뛰어들었다가 변을 당한 것으로 전해졌습니다 빅데이터 이슈였습니다', '그래도 내일을 기약할 수 있어 가는 길이 가볍습니다 마지막으로 어제 만난 부부의 나물 가게를 찾았는데요 지금 보니 재미있는 간판이 달려있네요 아이고 이렇게 재미있는 장인데 끝나서 아쉽네요 다음 봄에 꼭 다시 오겠습니다 나날이 아침 해가 빨라지는 요즘입니다 오늘도 어르신은 어김없이 밭으로 향합니다 겨우내 얼었던 땅이 풀리면서 어르신의 손길도 하루가 다르게 분주해집니다 바이러스는 여전히 기세를 세우고 있지만 어르신은 그에 아랑곳없이 생명을 일구고 씨앗을 뿌립니다 농부의 손길이 있는 한 새싹은 결국 자라날 겁니다 봄날이라고 매일 좋은 건 아니겠죠 오늘은 버섯 농가가 신통치 않아 보입니다 농사를 짓는다는 건 어쩌면 부침 많은 날들에 익숙해지는 건지도 모르겠습니다 나름 호기롭고 뜨거운 청춘을 보냈다는 사장님']\n"
     ]
    }
   ],
   "source": [
    "print(clean_text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92264bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_20per['Text'] = clean_text\n",
    "val_20per['Summary'] = clean_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db0e9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca418b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55e9ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_data = Dataset.from_pandas(train_20per) \n",
    "val_len = len(val_20per) // 2\n",
    "val_data = Dataset.from_pandas(val_20per[:val_len])\n",
    "test_data=Dataset.from_pandas(val_20per[val_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e20c5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_len = len(train_20sent)//4\n",
    "# train_data = Dataset.from_pandas(train_20sent[:train_len]) \n",
    "# val_len = len(val_20sent) // 8\n",
    "# val_data = Dataset.from_pandas(val_20sent[:val_len])\n",
    "# test_data=Dataset.from_pandas(val_20sent[val_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36a6a3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 35031)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 2175)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 2175)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ae98312",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 512\n",
    "max_target = 128\n",
    "batch_size = 4\n",
    "model_checkpoints = \"/aiffel/aiffel/Aiffelthon_koBART/results221111/checkpoint-21500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25d5967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3f579ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "  #get all the dialogues\n",
    "  inputs = [dialogue for dialogue in data_to_process['Text']]\n",
    "  #tokenize the dialogues\n",
    "  model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n",
    "  #tokenize the summaries\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(data_to_process['Summary'], max_length=max_target, padding='max_length', truncation=True)\n",
    "    \n",
    "  #set labels\n",
    "  model_inputs['labels'] = targets['input_ids']\n",
    "  #return the tokenized data\n",
    "  #input_ids, attention_mask and labels\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f082619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730d477d5fc742db901a107f5e0093db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c1f6bb2e3846d48fd199bd8d82f560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6c88e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e191c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set special tokens\n",
    "# #from transformers import EncoderDecoderConfig\n",
    "# model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "# model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 128\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 2\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f452a6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"/aiffel/aiffel/Aiffelthon_koBART/results221111/checkpoint-21500\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"length_penalty\": 2.0,\n",
       "  \"max_length\": 128,\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 2,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 2,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cc2868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f38cce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results221112\",\n",
    "    num_train_epochs=5,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=2000,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    logging_steps=2000,\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46adef31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \\n또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\\ntokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
    "\"\"\"\n",
    "DataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \n",
    "또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\n",
    "tokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63a9ad45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10c68171",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 35031\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10950\n",
      "  Number of trainable parameters = 123859968\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/Aiffelthon_koBART/wandb/run-20221112_002558-65o6xoc3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/65o6xoc3\" target=\"_blank\">results221112</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 3:51:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.445800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.433900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.423600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.416300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results221112/checkpoint-500\n",
      "Configuration saved in results221112/checkpoint-500/config.json\n",
      "Model weights saved in results221112/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to results221112/checkpoint-1000\n",
      "Configuration saved in results221112/checkpoint-1000/config.json\n",
      "Model weights saved in results221112/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to results221112/checkpoint-1500\n",
      "Configuration saved in results221112/checkpoint-1500/config.json\n",
      "Model weights saved in results221112/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to results221112/checkpoint-2000\n",
      "Configuration saved in results221112/checkpoint-2000/config.json\n",
      "Model weights saved in results221112/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-2500\n",
      "Configuration saved in results221112/checkpoint-2500/config.json\n",
      "Model weights saved in results221112/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-3000\n",
      "Configuration saved in results221112/checkpoint-3000/config.json\n",
      "Model weights saved in results221112/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-3500\n",
      "Configuration saved in results221112/checkpoint-3500/config.json\n",
      "Model weights saved in results221112/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-4000\n",
      "Configuration saved in results221112/checkpoint-4000/config.json\n",
      "Model weights saved in results221112/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-4500\n",
      "Configuration saved in results221112/checkpoint-4500/config.json\n",
      "Model weights saved in results221112/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-5000\n",
      "Configuration saved in results221112/checkpoint-5000/config.json\n",
      "Model weights saved in results221112/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-5500\n",
      "Configuration saved in results221112/checkpoint-5500/config.json\n",
      "Model weights saved in results221112/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-6000\n",
      "Configuration saved in results221112/checkpoint-6000/config.json\n",
      "Model weights saved in results221112/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-6500\n",
      "Configuration saved in results221112/checkpoint-6500/config.json\n",
      "Model weights saved in results221112/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-7000\n",
      "Configuration saved in results221112/checkpoint-7000/config.json\n",
      "Model weights saved in results221112/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-7500\n",
      "Configuration saved in results221112/checkpoint-7500/config.json\n",
      "Model weights saved in results221112/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-8000\n",
      "Configuration saved in results221112/checkpoint-8000/config.json\n",
      "Model weights saved in results221112/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-8500\n",
      "Configuration saved in results221112/checkpoint-8500/config.json\n",
      "Model weights saved in results221112/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-9000\n",
      "Configuration saved in results221112/checkpoint-9000/config.json\n",
      "Model weights saved in results221112/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-9500\n",
      "Configuration saved in results221112/checkpoint-9500/config.json\n",
      "Model weights saved in results221112/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-10000\n",
      "Configuration saved in results221112/checkpoint-10000/config.json\n",
      "Model weights saved in results221112/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112/checkpoint-10500\n",
      "Configuration saved in results221112/checkpoint-10500/config.json\n",
      "Model weights saved in results221112/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in results221112/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112/checkpoint-9000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10950, training_loss=1.4319670488192067, metrics={'train_runtime': 13864.5228, 'train_samples_per_second': 12.633, 'train_steps_per_second': 0.79, 'total_flos': 5.33991946715136e+16, 'train_loss': 1.4319670488192067, 'epoch': 5.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22f32663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2175\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.1757662296295166,\n",
       " 'eval_rouge2_precision': 0.1236,\n",
       " 'eval_rouge2_recall': 0.2408,\n",
       " 'eval_rouge2_fmeasure': 0.149,\n",
       " 'eval_runtime': 408.8078,\n",
       " 'eval_samples_per_second': 5.32,\n",
       " 'eval_steps_per_second': 0.333,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)# 여기에 기본 kobart가져오기?\n",
    "\n",
    "test_samples = val_data.select(range(16))\n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1] # 여기에 체크포인트 가져오기 \n",
    "# 연구해봐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df6a5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(summaries_after_tuning)):\n",
    "    print('idx_{} '.format(i))\n",
    "    print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     tabulate(\n",
    "#         zip(\n",
    "#             range(len(summaries_after_tuning)),\n",
    "#             summaries_after_tuning,\n",
    "#             summaries_before_tuning,\n",
    "#         ),\n",
    "#         headers=[\"Id\", \"Summary after\", \"Summary before\"]\n",
    "#     )\n",
    "# )\n",
    "# print(\"\\nTarget summaries:\\n\")\n",
    "# print(\n",
    "#     tabulate(list(enumerate(test_samples[\"Summary\"])), headers=[\"Id\", \"Target summary\"])\n",
    "# )\n",
    "# print(\"\\nSource documents:\\n\")\n",
    "# print(tabulate(list(enumerate(test_samples[\"Text\"])), headers=[\"Id\", \"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번재 시도 : max_len128로 학습, 10 epoch, lr 3e-05 ,target_len 32, results_ml128 폴더 생성\n",
    "# 두번째 시도 : max_len128로 학습, 10 epoch, lr 4e-05 ,target_len 32, results_ml128_2 폴더 생성\n",
    "# 세번째 시도 : max_len512로 학습, 2 epoch, lr 5e-05 ,target_len 128\n",
    "# 네번째 시도 : max_len512로 학습, 3 epoch, lr 5e-05 ,target_len 128\n",
    "# 모델 바꿈 : noahkim/KoT5_news_summarization, https://huggingface.co/digit82/kobart-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e6f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4406bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96839e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
