{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef22de11",
   "metadata": {},
   "source": [
    "## 1.Import 및 라이브러리 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4093684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9327b42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.21.4)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: datasets==1.0.2 in /opt/conda/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.21.4)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (6.0.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (0.3.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.3.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (4.62.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.26.12)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers==4.24.0 in /opt/conda/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (1.21.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (3.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.0.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformer-utils in /opt/conda/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.62.3)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.24.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (0.11.2)\n",
      "Requirement already satisfied: colorcet in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (3.0.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (1.9.1+cu111)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /opt/conda/lib/python3.9/site-packages (from colorcet->transformer-utils) (0.4.8)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.21.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (3.4.3)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.3.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->transformer-utils) (4.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2021.11.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (3.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.13.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (8.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->seaborn->transformer-utils) (2021.3)\n",
      "Requirement already satisfied: param>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from pyct>=0.4.4->colorcet->transformer-utils) (1.12.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn->transformer-utils) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (0.13.5)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.10.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.0.10)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "!pip install datasets==1.0.2\n",
    "!pip install transformers==4.24.0\n",
    "!pip install transformer-utils\n",
    "!pip install packaging\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42e1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments\n",
    "\n",
    ")\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import EncoderDecoderModel\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a0c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "# 20per data\n",
    "train_20per = pd.read_csv('data/train_20per.csv')\n",
    "val_20per = pd.read_csv('data/val_20per.csv')\n",
    "\n",
    "# # 2~3sent\n",
    "# train_2-3sent = pd.read_csv('data/train_2-3sent.csv')\n",
    "# val_2-3sent = pd.read_csv('data/val_2-3sent.csv')\n",
    "\n",
    "# # sum1\n",
    "# train_Sum1 = pd.read_csv('data/train_Sum1.csv')\n",
    "# val_Sum1 = pd.read_csv('data/val_Sum1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae6f2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text        73431\n",
      "Summary     73431\n",
      "Category    73431\n",
      "dtype: int64\n",
      "Text        9150\n",
      "Summary     9150\n",
      "Category    9150\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_20per.count())\n",
    "print(val_20per.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573d2c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speech        16000\n",
       "minute        13600\n",
       "news_r        10800\n",
       "briefing       8000\n",
       "literature     4800\n",
       "narration      4231\n",
       "his_cul        4000\n",
       "paper          4000\n",
       "edit           4000\n",
       "public         4000\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_20per['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f2ff9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지어내버린 대목부터는 흥분이 버썩 줄어지었다 ──. \"선생님! 또 기침이 나고 토...</td>\n",
       "      <td>그래서 나는 곧 H에게 간단하고 힘 있는 답장을 썼다 ──. \"선생님! 또 기침이 ...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이 송아지가 젖을 떼우고 집으로 끌고 오던 날은 첨지는 개선장군이 성안 에 들어올...</td>\n",
       "      <td>이 송아지가 젖을 떼우고 집으로 끌고 오던 날은 첨지는 개선장군이 성안 에 들어올 ...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>어떤 여름날 밤 손주딸에게 관한 불길한 꿈을 꾼 이 한머니는 이튿날 조반 후에 생...</td>\n",
       "      <td>어떤 여름날 밤 손주딸에게 관한 불길한 꿈을 꾼 이 한머니는 이튿날 조반 후에 생각...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이러한 가운데서 왕후는 자기의 입장을 위태롭게 여기고 겸하여 장래 자기 의 몸으로...</td>\n",
       "      <td>자기는 태자의 위 따위는 부럽지 않다. 이 전 낙랑공주가 아직 살아 있고 그 낙랑공...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“가겠소.” “언니가 나오시면 일러드 리겠으니 그때까지는 찾아오지 않으시는 것이 ...</td>\n",
       "      <td>별안간 골목쟁이에서 쑥 내달아 붙잡지나 않을까를 염려하여 빠른 걸음으 로 골목 골목...</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0   지어내버린 대목부터는 흥분이 버썩 줄어지었다 ──. \"선생님! 또 기침이 나고 토...   \n",
       "1   이 송아지가 젖을 떼우고 집으로 끌고 오던 날은 첨지는 개선장군이 성안 에 들어올...   \n",
       "2   어떤 여름날 밤 손주딸에게 관한 불길한 꿈을 꾼 이 한머니는 이튿날 조반 후에 생...   \n",
       "3   이러한 가운데서 왕후는 자기의 입장을 위태롭게 여기고 겸하여 장래 자기 의 몸으로...   \n",
       "4   “가겠소.” “언니가 나오시면 일러드 리겠으니 그때까지는 찾아오지 않으시는 것이 ...   \n",
       "\n",
       "                                             Summary    Category  \n",
       "0  그래서 나는 곧 H에게 간단하고 힘 있는 답장을 썼다 ──. \"선생님! 또 기침이 ...  literature  \n",
       "1  이 송아지가 젖을 떼우고 집으로 끌고 오던 날은 첨지는 개선장군이 성안 에 들어올 ...  literature  \n",
       "2  어떤 여름날 밤 손주딸에게 관한 불길한 꿈을 꾼 이 한머니는 이튿날 조반 후에 생각...  literature  \n",
       "3  자기는 태자의 위 따위는 부럽지 않다. 이 전 낙랑공주가 아직 살아 있고 그 낙랑공...  literature  \n",
       "4  별안간 골목쟁이에서 쑥 내달아 붙잡지나 않을까를 염려하여 빠른 걸음으 로 골목 골목...  literature  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_20per.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b068d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_20per = train_20per.loc[(train_20per.Category != 'literature')& \n",
    "               (train_20per.Category != 'minute')&\n",
    "               (train_20per.Category != 'edit')&\n",
    "               (train_20per.Category != 'speech')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edfee1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_20per = val_20per.loc[(val_20per.Category != 'literature')& \n",
    "               (val_20per.Category != 'minute')&\n",
    "               (val_20per.Category != 'edit')&\n",
    "               (val_20per.Category != 'speech')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9898ec24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>방문객이 상상으로 연주하는 곳.\\n쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거...</td>\n",
       "      <td>쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거리에는 공연과 전시를 홍보하는 포스...</td>\n",
       "      <td>narration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801</th>\n",
       "      <td>저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...</td>\n",
       "      <td>저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...</td>\n",
       "      <td>narration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4802</th>\n",
       "      <td>이제 뭐 훈련된 익숙한 인력이 투입이 돼야 되는데 이게 쉽지가 않은 거죠. 그래서 ...</td>\n",
       "      <td>게다가 지금의 발생양상이 지속되거나 더 나빠진다면 한 1~2주 정도 지나면 중환자 ...</td>\n",
       "      <td>narration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>하지만 지금은 연어가 아닌 산천어가 이 곳의 주인이 되었다.\\n그 옛날 바다로 나가...</td>\n",
       "      <td>금강 최상류에 위치한 용담댐. 2001년 건설된 댐으로 우리나라에서 다섯번째로 큰 ...</td>\n",
       "      <td>narration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804</th>\n",
       "      <td>티 없이 맑은 어린아이.\\n금발의 젊은 여성.\\n그리고 말을 타고 해변을 산책하는 ...</td>\n",
       "      <td>피우라 시에서 직접 운영하는 박물관을 찾았다. 비쿠스 왕조의 금 장식물들이 유명한 ...</td>\n",
       "      <td>narration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "4800  방문객이 상상으로 연주하는 곳.\\n쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거...   \n",
       "4801  저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...   \n",
       "4802  이제 뭐 훈련된 익숙한 인력이 투입이 돼야 되는데 이게 쉽지가 않은 거죠. 그래서 ...   \n",
       "4803  하지만 지금은 연어가 아닌 산천어가 이 곳의 주인이 되었다.\\n그 옛날 바다로 나가...   \n",
       "4804  티 없이 맑은 어린아이.\\n금발의 젊은 여성.\\n그리고 말을 타고 해변을 산책하는 ...   \n",
       "\n",
       "                                                Summary   Category  \n",
       "4800  쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거리에는 공연과 전시를 홍보하는 포스...  narration  \n",
       "4801  저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...  narration  \n",
       "4802  게다가 지금의 발생양상이 지속되거나 더 나빠진다면 한 1~2주 정도 지나면 중환자 ...  narration  \n",
       "4803  금강 최상류에 위치한 용담댐. 2001년 건설된 댐으로 우리나라에서 다섯번째로 큰 ...  narration  \n",
       "4804  피우라 시에서 직접 운영하는 박물관을 찾았다. 비쿠스 왕조의 금 장식물들이 유명한 ...  narration  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_20per.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f70c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_20per= train_20per.drop(columns='Category')\n",
    "val_20per = val_20per.drop(columns='Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc009611",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_20per.reset_index(inplace=True, drop=True)\n",
    "val_20per.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6516ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>방문객이 상상으로 연주하는 곳.\\n쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거...</td>\n",
       "      <td>쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거리에는 공연과 전시를 홍보하는 포스...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...</td>\n",
       "      <td>저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이제 뭐 훈련된 익숙한 인력이 투입이 돼야 되는데 이게 쉽지가 않은 거죠. 그래서 ...</td>\n",
       "      <td>게다가 지금의 발생양상이 지속되거나 더 나빠진다면 한 1~2주 정도 지나면 중환자 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>하지만 지금은 연어가 아닌 산천어가 이 곳의 주인이 되었다.\\n그 옛날 바다로 나가...</td>\n",
       "      <td>금강 최상류에 위치한 용담댐. 2001년 건설된 댐으로 우리나라에서 다섯번째로 큰 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>티 없이 맑은 어린아이.\\n금발의 젊은 여성.\\n그리고 말을 타고 해변을 산책하는 ...</td>\n",
       "      <td>피우라 시에서 직접 운영하는 박물관을 찾았다. 비쿠스 왕조의 금 장식물들이 유명한 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  방문객이 상상으로 연주하는 곳.\\n쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거...   \n",
       "1  저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...   \n",
       "2  이제 뭐 훈련된 익숙한 인력이 투입이 돼야 되는데 이게 쉽지가 않은 거죠. 그래서 ...   \n",
       "3  하지만 지금은 연어가 아닌 산천어가 이 곳의 주인이 되었다.\\n그 옛날 바다로 나가...   \n",
       "4  티 없이 맑은 어린아이.\\n금발의 젊은 여성.\\n그리고 말을 타고 해변을 산책하는 ...   \n",
       "\n",
       "                                             Summary  \n",
       "0  쇼핑몰과 기념품 가게가 없는 우주피스 공화국의 거리에는 공연과 전시를 홍보하는 포스...  \n",
       "1  저희 입장에서도 사실 이번 선거를 저희는 문재인 정부가 이제 한 2년 남짓 남았습니...  \n",
       "2  게다가 지금의 발생양상이 지속되거나 더 나빠진다면 한 1~2주 정도 지나면 중환자 ...  \n",
       "3  금강 최상류에 위치한 용담댐. 2001년 건설된 댐으로 우리나라에서 다섯번째로 큰 ...  \n",
       "4  피우라 시에서 직접 운영하는 박물관을 찾았다. 비쿠스 왕조의 금 장식물들이 유명한 ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_20per.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eccdcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       35031\n",
      "Summary    35031\n",
      "dtype: int64\n",
      "Text       4350\n",
      "Summary    4350\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_20per.count())\n",
    "print(val_20per.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aae271a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 괄호까지 없앨수 있도록 구현해야 겠음, 결과물에서 괄호가 너무 방해됨\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거\n",
    "    sentence = re.sub(\"[()]\",\"\", sentence)\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 제거\n",
    "    sentence = re.sub(\"'\",'', sentence) # 따옴표 제거\n",
    "    sentence = re.sub('\\n','', sentence) # \\n \" 제거\n",
    "    sentence = re.sub('.{2,3}\\W{0,1}기자','', sentence) # 기자 이름 제거\n",
    "    sentence = re.sub(r'[?.!,][/?.!,]', '', sentence) # 여러개 문장 부호를 하나의 문장부호로 바꿉니다\n",
    "    sentence = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-z0-9]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개 공백을 하나의 공백으로 바꿉니다.\n",
    "    sentence = sentence.strip() # 문장 양쪽 공백 제거\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7b6bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35031/35031 [00:07<00:00, 4381.71it/s]\n",
      "100%|██████████| 35031/35031 [00:01<00:00, 19452.13it/s]\n"
     ]
    }
   ],
   "source": [
    "train_clean_text = []\n",
    "train_clean_headlines = []\n",
    "\n",
    "for i in tqdm(train_20per['Text']):\n",
    "    train_clean_text.append(preprocess_sentence(i))\n",
    "\n",
    "    \n",
    "for i in tqdm(train_20per['Summary']):\n",
    "    train_clean_headlines.append(preprocess_sentence(i))\n",
    "    \n",
    "train_20per['Text'] = train_clean_text\n",
    "train_20per['Summary'] = train_clean_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd0edcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4350/4350 [00:01<00:00, 4290.24it/s]\n",
      "100%|██████████| 4350/4350 [00:00<00:00, 19575.64it/s]\n"
     ]
    }
   ],
   "source": [
    "val_clean_text = []\n",
    "val_clean_headlines = []\n",
    "\n",
    "for i in tqdm(val_20per['Text']):\n",
    "    val_clean_text.append(preprocess_sentence(i))\n",
    "\n",
    "    \n",
    "for i in tqdm(val_20per['Summary']):\n",
    "    val_clean_headlines.append(preprocess_sentence(i))\n",
    "    \n",
    "val_20per['Text'] = val_clean_text\n",
    "val_20per['Summary'] = val_clean_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61d4cb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 최대길이 :  358\n",
      "Summary 최소길이 :  39\n",
      "Summary 평균길이 :  157.51768433673033\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for i in train_20per['Summary']:\n",
    "    temp.append(len(i))\n",
    "    \n",
    "print(\"Summary 최대길이 : \", max(temp))\n",
    "print(\"Summary 최소길이 : \", min(temp))\n",
    "\n",
    "mean = sum(temp) / len(temp)\n",
    "print(\"Summary 평균길이 : \", mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff750cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 최대길이 :  1408\n",
      "Text 최소길이 :  169\n",
      "Text 평균길이 :  794.7913276811967\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for i in train_20per['Text']:\n",
    "    temp.append(len(i))\n",
    "    \n",
    "print(\"Text 최대길이 : \", max(temp))\n",
    "print(\"Text 최소길이 : \", min(temp))\n",
    "\n",
    "mean = sum(temp) / len(temp)\n",
    "print(\"Text 평균길이 : \", mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f26a7",
   "metadata": {},
   "source": [
    "## 2022.11.13(오전)\n",
    "- max_input : 1024 ??\n",
    "- max_target : 180 ??\n",
    "- results221112_2로 학습된 checkpoint로 위와 같이 길이를 지정해서 1학습후 테스트 진행해뵈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed212071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_data = Dataset.from_pandas(train_20per) \n",
    "val_len = len(val_20per) // 2\n",
    "val_data = Dataset.from_pandas(val_20per[:val_len])\n",
    "test_data=Dataset.from_pandas(val_20per[val_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72a8c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_len = len(train_20sent)//4\n",
    "# train_data = Dataset.from_pandas(train_20sent[:train_len]) \n",
    "# val_len = len(val_20sent) // 8\n",
    "# val_data = Dataset.from_pandas(val_20sent[:val_len])\n",
    "# test_data=Dataset.from_pandas(val_20sent[val_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f974b9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 35031)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 2175)\n",
      "Dataset(features: {'Text': Value(dtype='string', id=None), 'Summary': Value(dtype='string', id=None)}, num_rows: 2175)\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1115afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 512\n",
    "max_target = 180\n",
    "batch_size = 4\n",
    "model_checkpoints = \"digit82/kobart-summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05e4a226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb0fb1888a04944bf185a5b0bec8d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/295 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79332ab31835416bb40880b2b5a11285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/682k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4974b0560e64130a65db8303b459706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "726ae665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "  #get all the dialogues\n",
    "  inputs = [dialogue for dialogue in data_to_process['Text']]\n",
    "  #tokenize the dialogues\n",
    "  model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n",
    "  #tokenize the summaries\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(data_to_process['Summary'], max_length=max_target, padding='max_length', truncation=True)\n",
    "    \n",
    "  #set labels\n",
    "  model_inputs['labels'] = targets['input_ids']\n",
    "  #return the tokenized data\n",
    "  #input_ids, attention_mask and labels\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53fbddb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ac54f22fdb46f79384eacefe164f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a13bbfa921c492e8ad331090af99eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bef13dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38fccd9802c4d1e9c5fd523fe8147ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4731c96542194eac929e2bf351c62012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7103f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set special tokens\n",
    "# #from transformers import EncoderDecoderConfig\n",
    "# model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "# model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 180 # 256은 쿠다 메모리 오류 생김\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 2\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cec20199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"digit82/kobart-summarization\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"length_penalty\": 2.0,\n",
       "  \"max_length\": 180,\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 2,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 2,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a0ccc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b91371b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results221112_2\",\n",
    "    num_train_epochs=5,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=2000,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    logging_steps=2000,\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58a4accf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \\n또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\\ntokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
    "\"\"\"\n",
    "DataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \n",
    "또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\n",
    "tokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42eeae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0e1d7d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 35031\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10950\n",
      "  Number of trainable parameters = 123859968\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 4:22:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.394000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results221112_2/checkpoint-500\n",
      "Configuration saved in results221112_2/checkpoint-500/config.json\n",
      "Model weights saved in results221112_2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-1000\n",
      "Configuration saved in results221112_2/checkpoint-1000/config.json\n",
      "Model weights saved in results221112_2/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-1500\n",
      "Configuration saved in results221112_2/checkpoint-1500/config.json\n",
      "Model weights saved in results221112_2/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-2000\n",
      "Configuration saved in results221112_2/checkpoint-2000/config.json\n",
      "Model weights saved in results221112_2/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-2500\n",
      "Configuration saved in results221112_2/checkpoint-2500/config.json\n",
      "Model weights saved in results221112_2/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-3000\n",
      "Configuration saved in results221112_2/checkpoint-3000/config.json\n",
      "Model weights saved in results221112_2/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-3500\n",
      "Configuration saved in results221112_2/checkpoint-3500/config.json\n",
      "Model weights saved in results221112_2/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-4000\n",
      "Configuration saved in results221112_2/checkpoint-4000/config.json\n",
      "Model weights saved in results221112_2/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-4500\n",
      "Configuration saved in results221112_2/checkpoint-4500/config.json\n",
      "Model weights saved in results221112_2/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-5000\n",
      "Configuration saved in results221112_2/checkpoint-5000/config.json\n",
      "Model weights saved in results221112_2/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-5500\n",
      "Configuration saved in results221112_2/checkpoint-5500/config.json\n",
      "Model weights saved in results221112_2/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-6000\n",
      "Configuration saved in results221112_2/checkpoint-6000/config.json\n",
      "Model weights saved in results221112_2/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-6500\n",
      "Configuration saved in results221112_2/checkpoint-6500/config.json\n",
      "Model weights saved in results221112_2/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-7000\n",
      "Configuration saved in results221112_2/checkpoint-7000/config.json\n",
      "Model weights saved in results221112_2/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-7500\n",
      "Configuration saved in results221112_2/checkpoint-7500/config.json\n",
      "Model weights saved in results221112_2/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-8000\n",
      "Configuration saved in results221112_2/checkpoint-8000/config.json\n",
      "Model weights saved in results221112_2/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-8500\n",
      "Configuration saved in results221112_2/checkpoint-8500/config.json\n",
      "Model weights saved in results221112_2/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-9000\n",
      "Configuration saved in results221112_2/checkpoint-9000/config.json\n",
      "Model weights saved in results221112_2/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-9500\n",
      "Configuration saved in results221112_2/checkpoint-9500/config.json\n",
      "Model weights saved in results221112_2/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-10000\n",
      "Configuration saved in results221112_2/checkpoint-10000/config.json\n",
      "Model weights saved in results221112_2/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to results221112_2/checkpoint-10500\n",
      "Configuration saved in results221112_2/checkpoint-10500/config.json\n",
      "Model weights saved in results221112_2/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in results221112_2/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in results221112_2/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [results221112_2/checkpoint-9000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10950, training_loss=1.5208620393657248, metrics={'train_runtime': 15767.6282, 'train_samples_per_second': 11.109, 'train_steps_per_second': 0.694, 'total_flos': 5.33991946715136e+16, 'train_loss': 1.5208620393657248, 'epoch': 5.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf2732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2175\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 09:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.4202300310134888,\n",
       " 'eval_rouge2_precision': 0.1742,\n",
       " 'eval_rouge2_recall': 0.2566,\n",
       " 'eval_rouge2_fmeasure': 0.1914,\n",
       " 'eval_runtime': 582.7862,\n",
       " 'eval_samples_per_second': 3.732,\n",
       " 'eval_steps_per_second': 0.233,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed6607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /aiffel/.cache/huggingface/hub/models--digit82--kobart-summarization/snapshots/0887f7ac6e66df93248890f3460299d28bae1ddd/config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"digit82/kobart-summarization\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /aiffel/.cache/huggingface/hub/models--digit82--kobart-summarization/snapshots/0887f7ac6e66df93248890f3460299d28bae1ddd/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at digit82/kobart-summarization.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 180 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)# 여기에 기본 kobart가져오기?\n",
    "\n",
    "test_samples = val_data.select(range(16))\n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1] # 여기에 체크포인트 가져오기 \n",
    "# 연구해봐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c93684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d74821",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_0 \n",
      "Summary before \n",
      " 오늘 아침 우리 임시공관에서 오전 내내 쉬며 지냈다 오늘 아침 우리 임시공관의\n",
      "\n",
      "Summary after \n",
      " 성스러운 모스크바 시내에 우리나라 국기가 빛을 발한 것은 러시아 역사상 처음이다 그리고 나는 니콜라이 황제 폐하와 황후 입성을 보기 위해 모스크바 대공작의 궁전에 갔다 오늘 아침 우리 임시공관의 발코니 위에 태극기를 게양하다덜다 그리고 그 한 이 묘올 이어 그다음에 이후 마지막으로 다음은 마지막 수 하지만 이번 올해는 미국 현재는 코인치 두들까지했다크라어리 이들은클라도르 또한 박인 이번에도크 이제 민주당 등 앞으로도자크는보수베키스탄 최근에 것이었쭉 공화공화국 전반적으로까지도 양국 차례로과도 모두징 채로걸브리공제 끝까지\n",
      "\n",
      "Target summary \n",
      " 러시아 황실이 제공한 임시공관에서 오전 내내 쉬며 지냈다 오늘 아침 우리 임시공관의 발코니 위에 태극기를 게양하다 성스러운 모스크바 시내에 우리나라 국기가 빛을 발한 것은 러시아 역사상 처음이다\n",
      "\n",
      "Text 러시아 황실이 제공한 임시공관에서 오전 내내 쉬며 지냈다 오늘 아침 우리 임시공관의 발코니 위에 태극기를 게양하다 성스러운 모스크바 시내에 우리나라 국기가 빛을 발한 것은 러시아 역사상 처음이다 오후 2시 민영환 공과 물고기 김득련 그리고 나는 니콜라이 황제 폐하와 황후 입성을 보기 위해 모스크바 대공작의 궁전에 갔다 행진은 두 줄로 늘어선 병정들의 행렬 한가운데를 지나갔다 그 광경은 내가 전에 본 어떤 의식과도 비교할 수 없이 대단한 것이었다 병정 관리 시동 말들과 마차들을 모두 금과 은으로 입혀 놓은 것 같았다 황제는 홀로 말 위에 똑바로 앉은 자세로 가장 소박한 차림을 하고 들어섰다 황태후는 온통 은빛 나는 의상을 걸치고 황금마차에 혼자 올라앉아 양편에서 만세소리가 진동하자 그들에게 내내 고개를 숙여 인사하며 지나갔다 행차는 정해진 장소까지 한 시간 넘게 진행되었다 특별사절들 가운데 중국사절들은 화려한 비단에 수를 놓은 옷을 걸쳤음에도 불구하고 누런 이를 드러내고 길게 땋아 늘인 머리를 짧게 잘라 볼썽사나운 모습을 하고 있었다 일본 사절단들은 유럽식 복장에 가장 세련되고 부러운 동방의 나라로 군림하려고 기를 쓰고 있는 듯했다 페르시아 사절단은 화려한 정장에 잘생긴 친구가 등장했다 하지만 그런 유의 친구도 그 친구의 왕이 최근에 살해되고 그의 정부는 영국파와 러시아파로 갈라진 것으로 알고 있다 비참한 처지에 있는 우리나라의 상황을 생각할 때 불쌍한 우리 측 대표들은 다른 행복한 국가 대표들로부터 경멸과 조롱의 대상이 될 수밖에 없겠다는 생각이 든다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1 \n",
      "Summary before \n",
      " 배우자 공제건수를 종합소득과 근로소득의 경우로 나누어 추정하고 그 추정\n",
      "\n",
      "Summary after \n",
      " 배우자가 있는 여성의 부녀자 추가공제는 납세자가 여성근로자이면공제대상이므로 공제대상은므로므로공제 규모 증가분이고 여기에 평균 실효세율을 적용하면 최종적으로 추정하고자 하는 세목별 세수 감소 규모를 얻을 수 있음 감염음으로음과임 그는 학습 하지만음이함침다 한 음성 이 한편 그음을됨 국제 달성음은 양성 징음의 통합음에 임신 환경 하기 규정 가정 설정 통계 관련 산정 일과 침 수학 공부 가기 사건으로 하면서 치는 금연 청소ing로로 모금 의학냄 입시 자살 흡연 코르게  그다 자으로도 소요\n",
      "\n",
      "Target summary \n",
      " 기본공제액 확대시 공제 규모에서 기존의 공제 규모를 차감하고 남은 금액이 배우자 기본공제액 확대에 따른 공제 규모 증가분이고 여기에 평균 실효세율을 적용하면 최종적으로 추정하고자 하는 세목별 세수 감소 규모를 얻을 수 있음\n",
      "\n",
      "Text 위에서 살펴본 방법으로 배우자 공제건수를 종합소득과 근로소득의 경우로 나누어 추정하고 그 추정 결과에 현재 배우자 기본공제액인 100만원을 곱하여 현행 배우자 기본공제 규모를 추정할 수 있으며 1 200만원으로 확대될 경우의 기본공제 규모 역시 같은 방법으로 구할 수 있음 기본공제액 확대시 공제 규모에서 기존의 공제 규모를 차감하고 남은 금액이 배우자 기본공제액 확대에 따른 공제 규모 증가분이고 여기에 평균 실효세율을 적용하면 최종적으로 추정하고자 하는 세목별 세수 감소 규모를 얻을 수 있음 부녀자 추가공제 확대가 초래할 세수 감소 규모를 추정하기 위해서는 부녀자 추가공제 건수를 추정해야 하는데 이는 배우자가 있는 여자 납세자와 배우자가 없는 여자 납세자로 구분하여야 함 배우자가 있는 여성의 부녀자 추가공제는 납세자가 여성근로자이면 공제대상이므로 기혼 여자납세자 수와 동일할 것임 배우자가 있는 여성 중 부녀자 추가공제 건수 기혼 여자납세자 수 배우자가 없는 여성 중에서 부녀자 추가공제를 받기 위해서는 부양가족이 있는 세대주이어야 하는데 경제활동인구조사나 여타 가용한 통계 중에서 부양가족이 있는 여성 세대주의 비율을 추정할 수 있는 방안을 찾을 수 없음 다소 과대평가할 수 있으나 경제활동인구조사에서 찾을 수 있는 배우 자가 없는 여성 세대주의 비율을 무배우자 여성 납세자수에 적용하여 구하였음\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_2 \n",
      "Summary before \n",
      " 국회예산정책처는 공공기관의 현황 분석을 시작으로 공공기관을 체계적으로 평가하기 시작하였으며 공공기관\n",
      "\n",
      "Summary after \n",
      " 공공기관 정부지원 예산안 평가 에서는 일부 공공기관들이 자체수입을 적게 계상함으로써 정부의 예산지원을 더 많이 받으려 한다는 점과 여유자금을 과다하게 보유하고 있다는 점이 지적되었다 이러한 분석 결과는 의원실에서 활용되어 대정부질문으로 이어졌고 많은 언론에서 기사로 다루어져 공공기관의 잘못된 예산 및 재무관행이 시정될 수 있는 계기가 마련되었습니다다 있다 코 하지만했다 그러나 그 현금 이 한도되었 다시보다는 중앙 국회 현재는 또한 민주당 앞으로도 이러한 올해는 격려 특히되었던이었다고 신체들도까지도 통하여 사람들도되어 결론 되돌아남도 안정 형편공단정도 본회의 67솜 관절 헤어 결국은 광주시 광양력도북도 전남도\n",
      "\n",
      "Target summary \n",
      " 공공기관 정부지원 예산안 평가 에서는 일부 공공기관들이 자체수입을 적게 계상함으로써 정부의 예산지원을 더 많이 받으려 한다는 점과 여유자금을 과다하게 보유하고 있다는 점이 지적되었다\n",
      "\n",
      "Text 우리나라 공공기관의 현황 분석을 시작으로 공공기관을 체계적으로 평가하기 시작하였다 공공기관 정부지원 예산안 평가 에서는 일부 공공기관들이 자체수입을 적게 계상함으로써 정부의 예산지원을 더 많이 받으려 한다는 점과 여유자금을 과다하게 보유하고 있다는 점이 지적되었다 이러한 분석 결과는 의원실에서 활용되어 대정부질문으로 이어졌고 많은 언론에서 기사로 다루어져 공공기관의 잘못된 예산 및 재무관행이 시정될 수 있는 계기가 마련되었다 이후에도 국회예산정책처는 공공기관 결산평 가 공공기관 중장기 재무관리계획평가 공공기관 지정제도의 문제점과 개선과제 등 공공기관 예 결산 재무현황 관리정책 등에 대한 평가를 수행하였다 공공기관에 대한 국회예산정책처의 지속적인 분석과 평가는 정부의 공공기관 관리정책을 개선하는 데에 활용되고 있다 국민과 함께하는 나라살림 대토론회국회예산정책처는 국회가 예산안 편성 단계에서부터 국민들의 다양한 의견을 폭넓게 취합하여 차년도의 재정정책 방향을 제시하고 조화로운 예산을 마련할 수 있도록 지원하기 위하여 2011년 3월 중 나흘간 국민과 함께하는 나라살림 대토론회 를 개최하였다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_3 \n",
      "Summary before \n",
      " 프로게이머 특성상 선수 생명이 짧은 만큼 20대 후반에 진로 고민 을 했을 테\n",
      "\n",
      "Summary after \n",
      " 프로게이머 특성상 선수 생명이 짧은 만큼 20대 후반에 진로 고민 을 했을 테지만 당장 벌 수 있는 수입을 버리고 출구가 좁은 공무원 시험 준비 를 한다는 게 쉽지는 않았을 것이었다 그런 점에서 볼 때 박영민씨 경우는 이례적인 케이스로 꼽힌다요드레 밴드습니다밴밴드재판토론 하지만도 밴했다 옵랜드인도봅드래곤가야뵀 요소를플루뮤지궤숏팅 그 흑 봅 피아도어 영장팬 코 달린 보컬 룰팝 요율토 득표 하셨압율이민프라주도 얼음촌 75율을듀 강세 뮤지컬 숫 퀄구조학회로도다\n",
      "\n",
      "Target summary \n",
      " 프로게이머 특성상 선수 생명이 짧은 만큼 20대 후반에 진로 고민 을 했을 테지만 당장 벌 수 있는 수입을 버리고 출구가 좁은 공무원 시험 준비 를 한다는 게 쉽지는 않았을 것이었다 그런 점에서 볼 때 박영민씨 경우는 이례적인 케이스로 꼽힌다 공시의 당락을 좌우하는 영어의 경우 기초 지식이 부족해 문답을 통째로 외우다시피 했다\n",
      "\n",
      "Text 그런 점에서 볼 때 박영민씨 경우는 이례적인 케이스로 꼽힌다 프로게이머 특성상 선수 생명이 짧은 만큼 20대 후반에 진로 고민 을 했을 테지만 당장 벌 수 있는 수입을 버리고 출구가 좁은 공무원 시험 준비 를 한다는 게 쉽지는 않았을 것이었다 프로게이머들이 20대 중반 넘어가면 생각이 많아져요 군대 갔다 오면 다시 선수 생활하기도 힘들고 해봤자 1 2년밖에 못 할 텐데 그때가 인생에서는 중요한 시기잖아요 게임 말고 할 줄 아는 게 없었지만 코치 감독은 티오 도 잘 안 나던 때였어요 마침 아버지께서 공무원이셨고 저도 생각해보니까 경력이나 베이스가 없어도 할 수 있는 게 공시더라고요 시험 성적만 보니까요 물론 제가 처음 공부한다고 하니까 합격할 거라고 믿는 지인들이 10명 중 1명도 안 됐어요 다들 저보고 1인 인터넷 방송하자 고 자기가 매니저를 해줄 테니까 너는 게임만 하면 된다 고 많이 제안해왔는데 모두 거절했어요 말을 잘하는 편도 아니고 준비하는 시간이 아까웠어요 그렇게 상경한 박씨는 스마트폰을 구식 폴더폰으로 바꾸고 지인들과 연락을 끊었다 공시의 당락을 좌우하는 영어의 경우 기초 지식이 부족해 문답을 통째로 외우다시피 했다 손과 팔에 문신처럼 단어와 문장을 적고 아침밥 먹을 때부터 잠자리에 들기까지 달달 외웠다 1년 반 동안 노력해 첫 번째 시험을 봤지만 고배를 마셨고 이후 6개월은 군산으로 다시 내려가 집중했다 의자에 오래 앉아 연습하던 프로게이머 때 습관이 공부에도 도움이 됐다 두 번째 시험에서는 3관왕은 물론 성적도 높게 받아 합격했다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_4 \n",
      "Summary before \n",
      " 지번 주소를 기준으로 한 주소를 사용한 국민들은 새주소 시행 초기에 혼란\n",
      "\n",
      "Summary after \n",
      " 도로명 주소는 지번 주소에 비해 위치 파악 시 더 많은 범위를 탐색해야 하는 단점을 가지고 있다 즉 지 번 주소를 기준으로 할 경우에는 국회를 찾기 위해서 여의도동이라는 한정된 지역이 탐색을범위가 된다다 있다 하지만 코 이 한 그 예를했다 이름 서쪽로다운요 신종톡 인사 이런 이번 한편 발주 우대 발탁 동기 취지돛 등 간판 여부 국회 무려보기  입단동원 다소 재임 복무솜냇멘트봄 배출 배정타수탁 몸 개막빙팽 노사모로 명 위치 본회의운용 수 리듬 주소 선수로도볼 옮긴 유세\n",
      "\n",
      "Target summary \n",
      " 도로명 주소는 지번 주소에 비해 위치 파악 시 더 많은 범위를 탐색해야 하는 단점을 가지고 있다 오랜 세월 지번체계를 기준으로 한 주소를 사용한 국민들은 도로명을 제 로 찾지 못하여 오히려 새주소 시행 초기에 커다란 혼란을 겪을 가능성이 크다\n",
      "\n",
      "Text 도로명 주소는 지번 주소에 비해 위치 파악 시 더 많은 범위를 탐색해야 하는 단점을 가지고 있다 즉 지번 주소를 기준으로 할 경우에는 국회를 찾기 위해서 여의도동이라는 한정된 지역이 탐색범위가 된다 반면 도로명 주소를 기준으로 할 경우 국회를 찾기 위해서 영등포구 전체가 탐색범위가 된다 오랜 세월 지번체계를 기준으로 한 주소를 사용한 국민들은 도로명을 제 로 찾지 못하여 오히려 새주소 시행 초기에 커다란 혼란을 겪을 가능성이 크다 이 경우 위치파악 등을 수월하게 하려는 동 사업의 기본 취지 자체가 왜곡될 우려가 존재하는 것이 사실이다 따라서 지번 주소에서 도로명 주소로 전환하는 초기 단계에서 행정동 명칭을 한시적으로 표기하는 방안을 검토할 필요성이 있다 예컨 국회의 한시적인 도로명 주소는 서울특별시 영등포구 여의도동 의사당로 48가 되는 것이다 이 경우 의사당로 48이라는 위치 탐색의 기본 범위가 여의도동으로 한정되어 지번 주소에 익숙한 국민들이 사업시행 초기에 겪을 수 있는 어려움을 다소 감소시킬 수 있을 것으로 예상된다 이에 해 동 사업을 시작하게 된 계기 중 하나가 행정동과 법정동의 불일치 등 현재 동 단위의 문제점이고 동 명칭을 병기하는 것이 도로명 주소의 기본 성격에도 적합하지 않다는 지적이 있기는 하다 그러나 광복 이후 계속하여 지번 주소를 사용하여 왔고 이를 단기간에 급격히 도로명 주소로 체할 경우 상당한 혼란이 있을 것으로 예상된다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_5 \n",
      "Summary before \n",
      " 백두용이 편찬한 동상기찬 에는 김안국 이야기가 실려있고\n",
      "\n",
      "Summary after \n",
      " 백두용이 편찬한 동상기찬 에는 김안국 이야기가 실려 있다 이 김 안국전 은 허구가 가미된 픽션이다 실존인물김안국의 아버지는 참봉 김연이며 그의 동생도 김정국이란 것을 안다면 이 이야기가 모두 지어냈음을 알 수 있다다다 이 코 코 하지만 그 한했다 이런이다요도 영장 국가 중 보 그는 수 묘 쓰 써야 모 이곳 판 전이면자 크궈 갔 땅을 국회 랏 정킬 두 민주당드에 판사 하느 앓고공화체에 배상 나간 조회 쓰고 가면 봤 대성촌 명쳤 자 달린\n",
      "\n",
      "Target summary \n",
      " 이 김안국전 은 허구가 가미된 픽션이다 실존인물 김안국의 아버지는 참봉 김연이며 그의 동생도 김정국이란 것을 안다면 이 이야기가 모두 지어냈음을 알 수 있다 모재 김안국은 성종 중종 때의 인물이다\n",
      "\n",
      "Text 백두용이 편찬한 동상기찬 에는 김안국 이야기가 실려 있다 모재 김안국은 성종 중종 때의 인물이다 이 김안국전 은 허구가 가미된 픽션이다 실존인물 김안국의 아버지는 참봉 김연이며 그의 동생도 김정국이란 것을 안다면 이 이야기가 모두 지어냈음을 알 수 있다 김안국의 아버지는 판서와 대제학을 역임한 김숙이었다 판서는 정치적 상황에 따라 할 수 있는 자리지만 대제학은 당대 최고의 문인이 아니면 앉을 수 없는 청직이다 김숙만 그런 것이 아니라 위로 3대 모두가 대제학을 지냈으니 이 가문의 명망은 하늘을 찌를 정도였다 이런 명문가의 주인인 김숙에게 아들이 태어났다 이 아들의 용모가 빼어나게 아름답고 듬직해 김숙은 너무나 기뻤다 용이 용을 낳은 격이었다 진정 우리 집안의 자식이다 그런데 문제가 생겼다 어린 아들이 맹탕인 거였다 하늘 천 따 지 에서 조금도 나아가지 못하는 거였다 무슨 대단하고 어려운 것에서 왔다 갔다 하는 것이 아니라 그냥 시작부터 콱 막히니 아버지 김숙은 뜨악했다 아니 이렇게 총명하게 생겼는데 어찌 이러는 건가 너무 어려 그런가 김숙은 몇 년을 기다렸지만 웬걸 여전히 하늘 천 따 지를 벗어나지 못했다 밤낮으로 가르치고 틈만 나면 꾸짖고 깨치게 하려 했지만 요지부동이었다 그렇게 1년 2년이 지나 김안국의 나이가 열네 살이 되었다 아버지는 막막했다 이젠 아들의 문제가 아니라 가문의 문제였다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_6 \n",
      "Summary before \n",
      " 지난 9월 30일 스위스 세계경제포럼은 140개국을 상대로 한 금융시장 성숙도 조사에서\n",
      "\n",
      "Summary after \n",
      " 지난 9월 30일 스위스 세계경제포럼은 140개국을 상대로 한 금융시장 성숙도 조사에서 한국이 87위로 우간다보다 뒤처진다고 발표했다 이에 한국 금융위원회는 조사 방식이 설문조사 위주이기 때문에 설득력이 떨어진다 며 만족도 조사의 성격이 높지 국가 간 경쟁력 비교 잣대로 보기엔 무리가 있다 며 불만을 토로했다 코 하지만다 그친 민주당 선거 당시 당시 이에 이에 선거 선거 선거에 그는 그에게 국회 이 이번에도 한 이번 신종 이런 탓에학회 이후 이어 있다 등 의원끽자 자의 봤 따라 그렇다고 정치권 문학했다다다했다했다 민주당 관련해서도 정치인 2018년 선거에서 때도 의원에엘 자민깎 경험이냈봤왈 그랬\n",
      "\n",
      "Target summary \n",
      " 지난 9월 30일 스위스 세계경제포럼은 140개국을 상대로 한 금융시장 성숙도 조사에서 한국이 87위로 우간다보다 뒤처진다고 발표했다 이에 한국 금융위원회는 조사 방식이 설문조사 위주이기 때문에 설득력이 떨어진다 며 만족도 조사의 성격이 높지 국가 간 경쟁력 비교 잣대로 보기엔 무리가 있다 며 불만을 토로했다\n",
      "\n",
      "Text 지난 9월 30일 스위스 세계경제포럼은 140개국을 상대로 한 금융시장 성숙도 조사에서 한국이 87위로 우간다보다 뒤처진다고 발표했다 이번 설문조사에 한국에서는 100명이 응했는데 주로 외국계 기업의 최고경영자나 금융회사의 간부들이 대상이었다 이에 한국 금융위원회는 조사 방식이 설문조사 위주이기 때문에 설득력이 떨어진다 며 만족도 조사의 성격이 높지 국가 간 경쟁력 비교 잣대로 보기엔 무리가 있다 며 불만을 토로했다 그러던 차에 10월 10일 페루 리마 컨벤션센터에서 열린 세계은행 개발위원회에 참석한 최경환 부총리 겸 기획재정부 장관이 한국 금융계 인사들을 모아 놓고 만찬 건배사로 우간다를 하고 외쳤다 하여 뒷말들이 많았다 세계경제포럼의 이 같은 발표가 유독 이번만이 아니란다 이전부터도 한국의 순위가 그랬다고 한다 그럼에도 한국 금융인들은 그때마다 매번 무시해 왔다 왜 그렇게 나왔는지 깊이 성찰해 보지 않았으니 대책 또한 있을 리가 없겠다 아무렴 세계경제포럼이 심심풀이로 그런 조사를 했을까 만족도 라 하든 성숙도 라 하든 그건 곧 신뢰이고 신뢰가 곧 금융업의 출발점이자 경쟁력이라는 기본조차 망각하고 있으니 우리가 그토록 오매불망하던 고부가가치 서비스 산업의 글로벌화는 요원한 일이겠다 한국 금융인들의 글로벌 비즈니스 소통 매너가 그만큼 부족하다는 방증이기도 하다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_7 \n",
      "Summary before \n",
      " 대상포진까지 앓으면서 1999년에 내놓은 책이 정치인 캐릭터 분석집 그들 속의 그들 속의\n",
      "\n",
      "Summary after \n",
      " 대상포진까지 앓으면서 1999년에 내놓은 책이 정치인 캐릭터 분석집 그들 속의 이다 이 책을 계기로 방송 mc로까지 활동영역을 넓히게 됐는데 food 채널의 거인들의 저녁식사 라는 토크쇼 진행을 맡게 됐다 이 사이에 모교인 이대 정책과학대학원의 언론학 석사 과정에 입학해서 공부를 시작했다도 그리고 이 하지만 그다 한국뿐로로였고 한 이후이었을자 대상였던 이번 신종 등럴학회쿤였을나마 국회이었는데 이런이던 당시 맞은들까지이었고았던 이후로 했을괘까지남도 적용할 맞이  봤을미 환영 국가반도 금메달을 했기까지도보상로도했다\n",
      "\n",
      "Target summary \n",
      " 세 번째 5개년 계획은 정치생활 20년이 넘기 전에 책을 하나 쓰자는 생각이었다 이 책을 계기로 방송 mc로까지 활동영역을 넓히게 됐는데 food 채널의 거인들의 저녁식사 라는 토크쇼 진행을 맡게 됐다 이 자리에 노무현 전 대통령 김종필 전 총리를 비롯한 많은 정치인과 각계 유명 인사들이 출연해 준 덕분에 프로그램을 2년간 성공적으로 진행할 수 있었다\n",
      "\n",
      "Text 세 번째 5개년 계획은 정치생활 20년이 넘기 전에 책을 하나 쓰자는 생각이었다 당시 경향신문 차장 시절이었는데 가장 바쁜 시기에 책을 쓰는 일은 휴식이 없는 일상을 각오해야 했다 대상포진까지 앓으면서 1999년에 내놓은 책이 정치인 캐릭터 분석집 그들 속의 이다 이 책을 계기로 방송 mc로까지 활동영역을 넓히게 됐는데 food 채널의 거인들의 저녁식사 라는 토크쇼 진행을 맡게 됐다 이 자리에 노무현 전 대통령 김종필 전 총리를 비롯한 많은 정치인과 각계 유명 인사들이 출연해 준 덕분에 프로그램을 2년간 성공적으로 진행할 수 있었다 이 사이에 모교인 이대 정책과학대학원의 언론학 석사 과정에 입학해서 공부를 시작했다 석사 학위를 시작한 건 쳇바퀴 도는 일상에서 벗어나 한 번쯤 내 일을 학문적으로 재정비해 보고 싶었기 때문이다 그리고 2002년 중앙일보 로 자리를 옮겨 뉴스위크 한국판 편집장으로서 일하면서 한국 언론에서 주는 모든 상을 다 받는 영광도 누려보았다 석사를 마친 뒤 나의 네 번째 5개년 생활을 마무리하는 의미로 그동안의 경험을 학문의 얼개 속에서 재정리하며 일종의 자격증 같은 걸 내게 주는 일이었다 그 목표를 이루기 위해 2006년에 경희대 박사 과정에 입학해 2008년에 학위를 마쳤다 박사 학위 논문으로 쓴 들의 정치인에 대한 인식과 뉴스생산에 관한 연구 는 20년간 뛰었던 로서의 활동을 종합정리하는 의미로 6개월 만에 완성한 논문이다 박사 학위를 마친 뒤 바로 언론사 일을 마무리하고 경희대에서 후배를 가르치는 일을 해왔다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_8 \n",
      "Summary before \n",
      " 다섯 번째 근육은 지렛대를 강화해야 한다 지렛대가 없으면 잠재능력을 최대한 발휘\n",
      "\n",
      "Summary after \n",
      " 지금까지 이룬 성공을 뛰어넘기 위해서는 지금 우리가 이룬 성공에 도취되어 자기성찰과 연구를 게을리하는 마치 종착역에 도착한 느낌을 갖는 종 착역 질환 을 경계해야 한다이다 코이션 이다했다 하지만페이스 그 결론실천 묘벌지능랩마토즘부채 한오롱 성취 했기 벌졌기 증거 성과 형편 중독 이번 순위 달성 하느냐 민주당 하 실적 실현 장수 있다em 국가 동기 한다는 윗맷 교통을 이득미를 애플 과학기술 환경 곳미의 학습 이란소의처분 화 골든 기온 운 흥행 2018년 온라인 선정된 통합졌다는심리\n",
      "\n",
      "Target summary \n",
      " 고객의 법칙에 101010이라는 전략이 있다 고객 1명 데려오는 데 보통 10달러 비용이 드는데 고객을 잃어버리는 데는 10초밖에 안 걸리고 잃어버린 고객을 다시 데려오는 데는 10년 걸린다는 것이다\n",
      "\n",
      "Text 지금까지 이룬 성공을 뛰어넘기 위해서는 지금 우리가 이룬 성공에 도취되어 자기성찰과 연구를 게을리하는 마치 종착역에 도착한 느낌을 갖는 종착역 질환 을 경계해야 한다 자기타협과 적당주의를 불식해야 한다 nabo의 열망수준은 최고 눈높이인 맥시멈 열망수준 이어야 한다 달성 가능한 목표보다 약간 더 어려운 목표를 정하고 추진해야 한다 nabo 비전의 핵심은 바로 탁월한 성과를 달성하는 것이기 때문이다 다섯 번째 근육은 leverage 지렛대를 강화해야 한다 지렛대는 동기나 희망 이상의 힘을 발휘하여 원하는 것과 행동하는 것 꿈과 현실사이의 필수적인 연결고리이다 지렛대가 없으면 잠재능력을 최대한 발휘 하지 못한다 지렛대의 법칙은 인생의 산을 올라갈 때 로프를 던지면 그 로프를 단단히 붙잡아 정상을 향해가는 자신을 이끌어주는 사람이 거기에 있다는 사실을 아는 것과 마찬가지이다 우리 nabo와 부서를 앞장서서 홍보해 주고 전 파해 줄 수 있는 조력자를 많이 만들어야 한다 고객의 법칙에 101010이라는 전략이 있다 고객 1명 데려오는 데 보통 10달러 비용이 드는데 고객을 잃어버리는 데는 10초밖에 안 걸리고 잃어버린 고객을 다시 데려오는 데는 10년 걸린다는 것이다 nabo에 실망해서 뒤돌아서버린 고객을 다시 등대고객으로 만들기 위해서는 10년이라는 세월이 필요하다는 것이다 빌 게이츠는 점수를 내서 이기는 것이 아니라 실점을 줄여서 승리하는 전략으로 성공했다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_9 \n",
      "Summary before \n",
      " 기획예산처는 자체 기금운용계획 변경을 통하여 신규사업을 추진하는 것은 국회의 기금에 대한\n",
      "\n",
      "Summary after \n",
      " 이처럼 자체 기금운용계획 변경범위를 축소한 것은 기금변경에 대한 국회의 심의대상을 확대하여 기금에 대한 재정통제가 커져 바람직하다고 하겠으나 기금에서 대한 실질적인 통제를 강화하기 위해서는 지출항목비율보다 항목금액으로 국회심의 대상으로 하거나 항간의 통합 또는 분할을 통해 금액편차를 축소하는 방안을 강구해야 할 것이다 있다했다다 하지만 한 그 코 코 이에 대한 이 그만큼 그는겠다이다 한편 1명뭇 신종 금리를 등 110하겠다팡 상주 의원도 최우수방울가로뜰주도가를 배롱 189배당 치열 일도 떠났 한마디로 이름 입시 새삼그라직을털잉 주가가 그야말로비리미를웃 것이다\n",
      "\n",
      "Target summary \n",
      " 이처럼 자체 기금운용계획 변경을 통하여 신규사업을 추진하는 것은 국회의 기금에 대한 통제를 사실상 유명무실하게 하는 것이므로 기획예산처는 이에 대한 관리를 강화해야 할 것이다 즉 기금운용계획 변경을 통한 신규사업의 추진은 기금운용계획 수립 당시에는 예측할 수 없었으나 시급하게 추진할 필요가 있는 사업에 한하여 극히 제한적으로 추진할 수 있도록 해야 할 것이다\n",
      "\n",
      "Text 이처럼 자체 기금운용계획 변경범위를 축소한 것은 기금변경에 대한 국회의 심의대상을 확대하여 기금에 대한 재정통제가 커져 바람직하다고 하겠으나 기금에 대한 실질적인 통제를 강화하기 위해서는 지출항목비율보다 항목금액으로 국회심의 대상으로 하거나 항간의 통합 또는 분할을 통해 금액편차를 축소하는 방안을 강구해야 할 것이다 한편 자체 기금운용계획 변경을 통하여 신규사업을 추진함으로써 국회의 심의권을 침해하는 사례가 있는데 감사원 감사결과에 따르면 기획예산처는 2003년부터 2005년까지 문화관광부가 관광진흥개발기금의 계획변경을 통하여 31개의 신규사업 사업비 계 295억 4 000만원을 추진할 수 있도록 협의해 준 것으로 나타났다 이처럼 자체 기금운용계획 변경을 통하여 신규사업을 추진하는 것은 국회의 기금에 대한 통제를 사실상 유명무실하게 하는 것이므로 기획예산처는 이에 대한 관리를 강화해야 할 것이다 즉 기금운용계획 변경을 통한 신규사업의 추진은 기금운용계획 수립 당시에는 예측할 수 없었으나 시급하게 추진할 필요가 있는 사업에 한하여 극히 제한적으로 추진할 수 있도록 해야 할 것이다 2004년도 예비비결산 심의 시 지적된 사항은 일반예비비의 선집행 및 신대통령의 승인을 얻은 후 국회에 제출하여야 한다 다만 주요항목지출금액이 다음 각 호의 어느 하나에 해당하는 경우에는 기금운용계획변경안을 국회에 제출하지 아니하고 대통령령이 정하는 바에 따라 변경할 수 있다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_10 \n",
      "Summary before \n",
      " 센터링크는 자립할 수 있도록 보조할 뿐 아니라 노동연령의 인구에게 취업 기회를\n",
      "\n",
      "Summary after \n",
      " 센터링크는 자립할 수 있도록 보조할 뿐 만 아니라 노동연령의 인구에게 취업 기회를 제공하고 장애 발생의 위기를 겪거나 퇴직을 앞두고 있는 등 생애과정 중 특별 보조가 필요한 사람을 지원하는 일을 담당하고 있다 코 그 k 다 a 국회 복구 등 이 국제 중앙 귀국 신종 25 미국 50 택시 69 55 67 자 201 56 800 63 39 47 79 40 42 49 52 120 31 57 66 72 82 86 44 62 195 61 두 59 196 65 84형과 600 64 53 88 68 77 95 54 71 잃고끔 51 끄도르 60\n",
      "\n",
      "Target summary \n",
      " 센터링크는 자립할 수 있도록 보조할 뿐 만 아니라 노동연령의 인구에게 취업 기회를 제공하고 장애 발생의 위기를 겪거나 퇴직을 앞두고 있는 등 생애과정 중 특별 보조가 필요한 사람을 지원하는 일을 담당한다\n",
      "\n",
      "Text 호주 센터링크는 호주 휴먼서비스포트폴리오산하의 정부조직으로서 연방정부 예산의 30 를 지원받아 27 000명 직원이 1 000개의 서비스 거점센터와 360개 office를 통해 120가지의 현급급여와 서비스를 제공하되 지방정부에서 중복급여가 지원되지 않도록 관리하고 있다 센터링크가 운영하는 통합전산망은 세계 8위로 전국 어디서나 이용이 가능하여 47만 명이 웹사이트를 검색하고 8천6백만의 방문객이 편지를 보내온다 16개 콜센터에는 150250명의 직원이 200개의 언어로 전화상담을 받으며 매달 65만 명을 상담하는 것 외에 연간 4백만 명을 대상으로 수요 설문조사를 실시한다 센터링크는 자립할 수 있도록 보조할 뿐 만 아니라 노동연령의 인구에게 취업 기회를 제공하고 장애 발생의 위기를 겪거나 퇴직을 앞두고 있는 등 생애과정 중 특별 보조가 필요한 사람을 지원하는 일을 담당한다 또한 적절한 현금급여와 서비스 옵션에 관해 상담을 제공하며 고용서비스제공자에게 연결해 주기도 한다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_11 \n",
      "Summary before \n",
      " 슐릭만은 1982년 5월 매사추세츠주 법원에 백혈병 환자\n",
      "\n",
      "Summary after \n",
      " 슐릭만은 1982년 5월 매사추세츠주 법원에 백혈병 환자 가족을 대리해 그레이스와 비트리스를 상대로 소장을 접수했다 당시만 해도 그는 피고들이 그 우물을 오염시킨 기업이고 오염물질이 백 혈병을 유발했음을 입증하는 데 큰 어려움이 없을 것으로 판단했다  그 이 한 신종 그는 코 하지만 당시 두 이는 현재는다일러뿌됐으나였던 식민 이번텀도르몽껑소서했다 이 이력을왕은령은규는됐으며자 출신의 팬뉘인은 민족로 국회이었다고럿 자도 중기셈인으로령을족 이씨는멘혈압였다고타수엿 모팬\n",
      "\n",
      "Target summary \n",
      " 슐릭만은 1982년 5월 매사추세츠주 법원에 백혈병 환자 가족을 대리해 그레이스와 비트리스를 상대로 소장을 접수했다 당시만 해도 그는 피고들이 그 우물을 오염시킨 기업이고 오염물질이 백혈병을 유발했음을 입증하는 데 큰 어려움이 없을 것으로 판단했다\n",
      "\n",
      "Text 슐릭만은 1982년 5월 매사추세츠주 법원에 백혈병 환자 가족을 대리해 그레이스와 비트리스를 상대로 소장을 접수했다 당시만 해도 그는 피고들이 그 우물을 오염시킨 기업이고 오염물질이 백혈병을 유발했음을 입증하는 데 큰 어려움이 없을 것으로 판단했다 그러나 보스턴의 대형 로펌 두 군데가 이 거대 기업의 변호를 맡으면서 상황은 완전 달라졌다 주 법원에 제기된 이 사건은 피고 측의 신청에 의해 대기업에 유리한 연방법원으로 이송되었으며 매사추세츠주 연방법원의 담당 판사 스키너는 피고 비트리스를 대리하는 로펌 대표와 30년 친구였다 배심원도 판사의 교묘한 개입으로 기업에 유리한 사람들로 구성되었다 판사는 재판 절차 역시 유해기업과 대형 로펌에 유리하게 진행시켰다 피고 측 변호사들의 방해공작으로 과학적 증거는 법정에서 여지없이 왜곡되었다 피고 측 변호사들은 피고 기업이 tce 등 유해물질을 내다버린 사실도 부인했고 그 유해물질이 우물을 오염시킨 것도 부인했다 심지어 tce와 퍼크 등이 백혈병을 일으키는 의학적 증거도 없다고 주장했다 피고 측 변호사들은 백혈병의 원인으로 베이컨 테프론 팬 플라스틱 샤워 커튼 등 무수한 일반식품과 가정용품을 거론하면서 어떤 물질이 백혈병의 원인이 되었는지 알 수 없다며 배심원들을 현혹시켰다 피고 측 공장에서 근무했던 유력한 증인 후보들은 대부분 암으로 죽었으며 남아 있는 증인 중 일부가 tce 등을 땅과 도랑에 버렸다고 증언했음에도 사주와 공장장은 법정에서 선서하고도 이 사실을 부인했다 이 사건에 참가한 원고 측 전문가들은 유해환경의 해독을 알리는 사명감으로 임했고 원고 측 변호사들은 환경오염에 대한 기업의 책임추궁과 소송을 통해 사회를 변화시킬 수 있다는 믿음으로 이 소송을 진행시켰다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_12 \n",
      "Summary before \n",
      " 보이어가 이 물체들 근처에서 비행한 시간은 약 15분이었으며 이\n",
      "\n",
      "Summary after \n",
      " 보이어 기장은 물체들의 실체를 파악하기 위해 더 가까이 가고 싶었지만 승객들을 위험에 빠뜨릴 수 있다는 생각에 목적지에 착륙하는 방안을 택했다고 했다 그동안 비행기의 기계장치나 라디오 통신장치는 정상 작동했다 이 한했다했다 이 이기는 하지만다 그 신종 등 코 이름 그는도 섬 1명 2명체 했기 있다 이번 묘훼뿌 이씨는과도했으나 할지넥 들었학회쏠 대입 1명이긍 국가 배 크기 봤 번 연관 알았 막내 받았 종종 캐스팅 인기 해도 크 체제 받았다고촌 등도직에 비중이엿이었다고 숫 입시 대학에 출연 배치 모\n",
      "\n",
      "Target summary \n",
      " 보이어 기장은 물체들의 실체를 파악하기 위해 더 가까이 가고 싶었지만 승객들을 위험에 빠뜨릴 수 있다는 생각에 목적지에 착륙하는 방안을 택했다고 했다 보이어는 착륙한 뒤 승객들에게 무언가 특별한 것을 보지 못했냐 고 물었다고 한다 조종석에서 세 칸 뒤에 있었던 케이트와 존 러셀 부부가 실명으로 이를 봤다고 했다 보이어가 이 물체들 근처에서 비행한 시간은 약 15분이었다\n",
      "\n",
      "Text 보이어 기장은 물체들의 실체를 파악하기 위해 더 가까이 가고 싶었지만 승객들을 위험에 빠뜨릴 수 있다는 생각에 목적지에 착륙하는 방안을 택했다고 했다 보이어가 이 물체들 근처에서 비행한 시간은 약 15분이었다 그동안 비행기의 기계장치나 라디오 통신장치는 정상 작동했다 보이어는 착륙한 뒤 승객들에게 무언가 특별한 것을 보지 못했냐 고 물었다고 한다 선입견을 줄 수 있었기 때문에 자신이 본 것은 설명하지 않았다 공항 체크인 카운터에 하고 싶은 말을 메모로 남겨달라고 했다 조종석에서 세 칸 뒤에 있었던 케이트와 존 러셀 부부가 실명으로 이를 봤다고 했다 또 다른 승객 중 최소 4명이 이를 봤다고 했고 조종석 바로 뒤에 앉아 있던 남성은 보이어로부터 망원경을 빌려 직접 보기도 했다 한편 영국 항공청은 이 사건에 대한 설명을 즉각 내놓지 않았다 언론의 압박이 거세지자 영국 국방부가 한 주 뒤 성명을 발표했다 이 물체를 목격했을 때 비행기는 프랑스 영공에 있었기 때문에 영국 정부가 공식 발표할 사안이 아니라고 했다 1976년 9월 18일 오후 11시 이란의 테헤란 인근에서 저고도로 비행하고 있는 미확인 물체로 인해 주민들이 겁을 집어먹었다 별같이 보이기도 했는데 더 크고 밝았다 일부 사람들이 메흐레파드 공항 관제탑에 전화를 걸었다 후세인 피로우지가 당직을 서고 있었다 피로우지는 네 통의 전화를 받은 뒤 밖으로 나가 망원경을 들고 사람들이 말한 곳을 바라봤다 그 역시도 6000피트 상공에서 움직이고 있는 밝게 빛나는 물체를 볼 수 있었다 이 물체는 모양이 수시로 바뀌고 있는 것처럼 보였다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_13 \n",
      "Summary before \n",
      " 엄장이 엄장에게 친구 광덕이 득도하면 서로 알려주기로 약속하고 친구\n",
      "\n",
      "Summary after \n",
      " 둘이 각각 도를 닦았는데 먼저 득도하면 서로 알려주기로 약속했다 어느 날 저녁 하늘에서 엄장에게 소리가 들려왔다 나는 먼저 서방정토로 가네 자네도 서둘러 오게 약속대로 친구 광덕이 득 도하여 서방으로 가면서 알려준 것이다 혼자다 a 그는 이 한 남편이했다 함께 그 둘이 오십더니 신종와의스와의 중와엿 홀로혼 배도셔널 안드로 불가 아들의 코 싶어 괄 십족향 연계 그녀에게쭉 국회 집단 수준의옴느자 엘리쑥 관련하여 자액이 연동지능 와 김윤 뱃 하느 옥 본회의와는 알레\n",
      "\n",
      "Target summary \n",
      " 그러려면 뭐 하러 혼인을 했는지 모르겠으나 아무튼 이 말을 들은 엄장은 부끄러워 그 길로 그곳을 나와 원효를 찾아갔다 바르게 앉아 한결같은 목소리로 아미타불을 불렀습니다 정성이 이랬으니 어떻게 득도하지 않겠습니까 광덕 처의 말인즉 광덕이 10년 넘게 같이 살면서 동침은커녕 밤마다 염불을 외며 도를 닦았다는 거였다\n",
      "\n",
      "Text 광덕과 엄장은 친구였다 둘이 각각 도를 닦았는데 먼저 득도하면 서로 알려주기로 약속했다 어느 날 저녁 하늘에서 엄장에게 소리가 들려왔다 나는 먼저 서방정토로 가네 자네도 서둘러 오게 약속대로 친구 광덕이 득도하여 서방으로 가면서 알려준 것이다 이에 엄장이 광덕의 집으로 가보니 정말 광덕이 죽어 있었다 혼자 살던 엄장과 달리 광덕은 처가 있었는데 엄장은 광덕의 처와 함께 시신을 거둬 무덤을 만들어 주고는 그녀에게 말했다 남편이 죽었으니 나와 같이 지내는 것이 어떻소 광덕의 처가 좋다며 그러자고 했다 엄장이 밤이 되자 자연스럽게 광덕의 처를 가까이하려 했다 그러자 광덕의 처가 벌떡 일어나 꾸짖었다 당신은 절대로 득도하지 못하겠군요 그러고는 정말이지 황당한 소리를 늘어놓았다 남편은 저와 십여 년을 함께 살았지만 단 한 번도 같은 침상에 눕지 않았고 몸을 더럽히지도 않았습니다 밤이면 밤마다 단정하고 바르게 앉아 한결같은 목소리로 아미타불을 불렀습니다 정성이 이랬으니 어떻게 득도하지 않겠습니까 광덕 처의 말인즉 광덕이 10년 넘게 같이 살면서 동침은커녕 밤마다 염불을 외며 도를 닦았다는 거였다 그러려면 뭐 하러 혼인을 했는지 모르겠으나 아무튼 이 말을 들은 엄장은 부끄러워 그 길로 그곳을 나와 원효를 찾아갔다 원효에게서 깨끗하게 바라보는 법 을 배웠고 결국은 득도를 했다고 한다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_14 \n",
      "Summary before \n",
      " 의원발의법안 비용추계의 주체가 국회예산정책처로 일원화되어 의원\n",
      "\n",
      "Summary after \n",
      " 의원발의법안 비용추계의 주체가 국회예산정책처로 일원화되었다 그 동안 의원실에서 상당 부분을 추계하던 것을 전문기관이 담당하게 됨에 따라 비용 추계의 일관성을 유지할 수 있게 되고 신뢰성도 높아질 수있게 되었다능능능력능력능력을로서의으로서의됨됨 상실 그나마되었던였던 이로서되던스턴베이스 있다 신종던 한함으로써중에중의 코 중 배 후보에게 했다자 그녀에게되어기능돗 수 있는배에리의 인하여간의중인라고감의라인 간의배당 김준단의릿젤운용중학교 그녀를면에서 중기 집단 됐다고결과를상에서공단다\n",
      "\n",
      "Target summary \n",
      " 그 동안 의원실에서 상당 부분을 추계하던 것을 전문기관이 담당하게 됨에 따라 비용추계의 일관성을 유지할 수 있게 되고 신뢰성도 높아질 수 있게 되었다 의원발의법안 비용추계의 주체가 국회예산정책처로 일원화되었다\n",
      "\n",
      "Text 의원발의법안 비용추계의 주체가 국회예산정책처로 일원화되었다 그 동안 의원실에서 상당 부분을 추계하던 것을 전문기관이 담당하게 됨에 따라 비용추계의 일관성을 유지할 수 있게 되고 신뢰성도 높아질 수 있게 되었다 재정을 수반하는 위원회 제안법안에 대한 비용추계서 첨부의무가 강화되었다 위원회로 하여금 국회예산정책처의 비용추계서를 제출하도록 하되 긴급한 사유가 있는 경우에는 위원회의 의결로 이를 생략할 수 있도록 하였다 재정수반 법안이 위원회에서 수정된 경우에는 심사보고서에 국회예산정책처가 작성한 추계서를 첨부하도록 하되 긴급한 사유가 있는 경우에 위원회의 의결로 이를 생략할 수 있도록 하였다 이러한 국회법 의 개정으로 개선된 비용추계 관련 규정이 시행되는 2015년 3월 말경부터는 앞에서 지적한 현행 비용추계의 문제점들이 상당 부분 제322회 제10차 본회의에서 의결되었고 3월 18일 공포되었음 비용추계관련 규정은 공포 후 1년이 경과한 날부터 시행하는 것으로 되어 있어서 2015년 3월 19일부터 시행될 예정임\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_15 \n",
      "Summary before \n",
      " 민간화기금에 대한 재정통제가 어떻게 이루어질지에 대한 구체적인 방안이 필요하며 민간\n",
      "\n",
      "Summary after \n",
      " 즉 민간화기금에 대한 재정통제가 어떻게 이루어질지에 대한 구체적인 방안이 필요하며 폐지 기금의 융자사업의 이차보전 전환에 대해 지침을 만들 필요가 있다 코 코 창 신종했다 다톰 다시제와도재의 블라 블라 진주 혜 혜면의에도 강릉 용인 이 아이스재와 기숙 세탁 택시 택시 교통 교통 모습에 도 해운 해운 운전 운전 관련된주도 등판 산은 출연 출연 운영에 건전한 거리를 운전자 거리에 역할이 운잠 얼음롯 스웨 화성 과천 용에게도 양도재에 여행 대해서도 화 트럼프휴담당 휴대폰 마라톤능 마운드 목욕 디자뮤지 회사채원도\n",
      "\n",
      "Target summary \n",
      " 또한 문화산업기반기금은 소관부처 변경에 따른 긴밀한 업무협조가 필요하며 폐지 기금의 적립금의 처리와 활용문제도 추후 논의될 필요가 있다\n",
      "\n",
      "Text 즉 민간화기금에 대한 재정통제가 어떻게 이루어질지에 대한 구체적인 방안이 필요하며 폐지 기금의 융자사업의 이차보전 전환에 대한 지침을 만들 필요가 있다 또한 문화산업기반기금은 소관부처 변경에 따른 긴밀한 업무협조가 필요하며 폐지 기금의 적립금의 처리와 활용문제도 추후 논의될 필요가 있다 기금의 정비방안에서 폐지가 유보되었던 근로자복지진흥기금 과학기술진흥기금 축산발전기금은 기금 존치평가에서 지적되었던 사항들이 2006년도 기금운용계획안에서 반영된 사례가 거의 없었는데 이 기금들은 차기 존치평가 시에도 존폐 문제가 제기될 것이므로 기금 정비방안 원칙에 따른 예산과 기금의 사업편성에 대한 검토가 필요하다 이번 정비방안에서는 직접 다루어지지 않았지만 향후 기금정비방안을 마련함에 있어 검토되어야 할 과제를 정리하면 다음과 같다 신용보증기금과 기술신용보증기금은 통폐합 문제가 유보되었지만 신용보증기금의 통폐합의 찬반양론을 객관적으로 분석평가하면서 논의를 지속할 필요가 있으며 향후 통폐합 논의 시 다시 판단해야 할 것이다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(summaries_after_tuning)):\n",
    "    print('idx_{} '.format(i))\n",
    "    print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     tabulate(\n",
    "#         zip(\n",
    "#             range(len(summaries_after_tuning)),\n",
    "#             summaries_after_tuning,\n",
    "#             summaries_before_tuning,\n",
    "#         ),\n",
    "#         headers=[\"Id\", \"Summary after\", \"Summary before\"]\n",
    "#     )\n",
    "# )\n",
    "# print(\"\\nTarget summaries:\\n\")\n",
    "# print(\n",
    "#     tabulate(list(enumerate(test_samples[\"Summary\"])), headers=[\"Id\", \"Target summary\"])\n",
    "# )\n",
    "# print(\"\\nSource documents:\\n\")\n",
    "# print(tabulate(list(enumerate(test_samples[\"Text\"])), headers=[\"Id\", \"Text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c9c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번재 시도 : max_len128로 학습, 10 epoch, lr 3e-05 ,target_len 32, results_ml128 폴더 생성\n",
    "# 두번째 시도 : max_len128로 학습, 10 epoch, lr 4e-05 ,target_len 32, results_ml128_2 폴더 생성\n",
    "# 세번째 시도 : max_len512로 학습, 2 epoch, lr 5e-05 ,target_len 128\n",
    "# 네번째 시도 : max_len512로 학습, 3 epoch, lr 5e-05 ,target_len 128\n",
    "# 모델 바꿈 : noahkim/KoT5_news_summarization, https://huggingface.co/digit82/kobart-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5ae8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c8dc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc3887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
