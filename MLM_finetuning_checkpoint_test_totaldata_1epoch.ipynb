{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d037907",
   "metadata": {},
   "source": [
    "# MLM_finetuning_checkpoint_test\n",
    "- 다른 ipynb 파일에서 전처리를 진행후 생성된 csv 파일로 본 학습이 진행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4a339",
   "metadata": {},
   "source": [
    "## 1. Import 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3278e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.10.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.10.0)\n",
      "Requirement already satisfied: rouge_score in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: datasets in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (2.7.1)\n",
      "Requirement already satisfied: transformers==4.24.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (4.24.0)\n",
      "Requirement already satisfied: transformer-utils in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (0.1.1)\n",
      "Requirement already satisfied: packaging in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (21.3)\n",
      "Requirement already satisfied: wandb in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.13.5)\n",
      "Requirement already satisfied: pandas in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (1.3.5)\n",
      "Requirement already satisfied: numpy in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (1.21.6)\n",
      "Requirement already satisfied: matplotlib in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (3.5.3)\n",
      "Requirement already satisfied: dataclasses in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (0.6)\n",
      "Requirement already satisfied: tqdm in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (4.64.1)\n",
      "Requirement already satisfied: rouge in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from -r requirements.txt (line 14)) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from torch==1.10.0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (0.13.2)\n",
      "Requirement already satisfied: filelock in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (3.8.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (5.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (2022.10.31)\n",
      "Requirement already satisfied: requests in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformers==4.24.0->-r requirements.txt (line 4)) (2.28.1)\n",
      "Requirement already satisfied: absl-py in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: nltk in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: multiprocess in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (10.0.1)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (2022.11.0)\n",
      "Requirement already satisfied: aiohttp in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (3.8.3)\n",
      "Requirement already satisfied: colorcet in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformer-utils->-r requirements.txt (line 5)) (3.0.1)\n",
      "Requirement already satisfied: seaborn in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from transformer-utils->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from packaging->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (65.5.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (3.1.29)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.11.1)\n",
      "Requirement already satisfied: pathtools in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (0.4.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (5.9.4)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (2.3)\n",
      "Requirement already satisfied: setproctitle in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (1.0.11)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (8.1.3)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from wandb->-r requirements.txt (line 7)) (4.21.9)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 8)) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (9.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 11)) (4.38.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.8.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb->-r requirements.txt (line 7)) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from requests->transformers==4.24.0->-r requirements.txt (line 4)) (1.26.13)\n",
      "Requirement already satisfied: pyct>=0.4.4 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from colorcet->transformer-utils->-r requirements.txt (line 5)) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from importlib-metadata->transformers==4.24.0->-r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: joblib in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from nltk->rouge_score->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->-r requirements.txt (line 7)) (5.0.0)\n",
      "Requirement already satisfied: param>=1.7.0 in /home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages (from pyct>=0.4.4->colorcet->transformer-utils->-r requirements.txt (line 5)) (1.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8568795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d40fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    "    EarlyStoppingCallback\n",
    "\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75481418",
   "metadata": {},
   "source": [
    "## 2. 모델 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa23dd3",
   "metadata": {},
   "source": [
    "### 1) 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc889662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = \"checkpoint/kobart-base-v2_ft_test/checkpoint-17500\"#\"gogamza/kobart-base-v2\"#\"chckpoint/MLM_pretrain_basev2_freezing/checkpoint-626500\"\n",
    "\n",
    "#\"/aiffel/aiffel/Korean_Conversation_Summary/MLM_pretrain_basev2_freezing/checkpoint-175000\"#\"/aiffel/aiffel/Korean_Conversation_Summary/MLM_ft_freez/checkpoint-21500\"#\"/aiffel/aiffel/Korean_Conversation_Summary/MLM_pretrain_basev2_freezing/checkpoint-175000\"#\"/aiffel/aiffel/Korean_Conversation_Summary/MLM_pretrain_basev2_total10ep_3/checkpoint-133500\" #MLM_ft2/checkpoint-4000\" #MLM_pretrain_basev2_total10ep/checkpoint-14000\" #/MLM_ft/checkpoint-4000\"#\"/aiffel/aiffel/Korean_Conversation_Summary/MLM_pretrain_5ep_221120_2/checkpoint-87500\"#\"gogamza/kobart-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e379b576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train params in Summarization Model : 259\n"
     ]
    }
   ],
   "source": [
    "# for i in model.parameters():\n",
    "#     print(i.requires_grad)\n",
    "train_p = [p for p in model.parameters() if p.requires_grad] \n",
    "print(f'Length of train params in Summarization Model : {len(train_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b131c",
   "metadata": {},
   "source": [
    "### 2) 데이터 불러오기 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4841706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_category_path = \"data/train_category.csv\"\n",
    "val_category_path = \"data/val_category.csv\"\n",
    "\n",
    "with open(train_category_path, encoding=\"utf-8\") as f:\n",
    "            train_category = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "with open(val_category_path, encoding=\"utf-8\") as f:\n",
    "            val_category = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265fa230",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_textfile_path = \"data/train_text.csv\"\n",
    "train_summaryfile_path = \"data/train_summary.csv\"\n",
    "\n",
    "with open(train_textfile_path, encoding=\"utf-8\") as f:\n",
    "            train_textlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "with open(train_summaryfile_path, encoding=\"utf-8\") as f:\n",
    "            train_sumlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d11ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_textfile_path = \"data/val_text.csv\"\n",
    "val_summaryfile_path = \"data/val_summary.csv\"\n",
    "\n",
    "with open(val_textfile_path, encoding=\"utf-8\") as f:\n",
    "            val_textlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "with open(val_summaryfile_path, encoding=\"utf-8\") as f:\n",
    "            val_sumlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23616d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_textlines[0]\n",
    "del val_textlines[0]\n",
    "del train_category[0]\n",
    "del val_category[0]\n",
    "del val_sumlines[0]\n",
    "del train_sumlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fd3afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= pd.read_csv(\"data/train_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7081bd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우리 비행포함 15일이야 16일 아 너 나한테 돈 보내주면 지금 할 수 잇옹 얼마야 최종 결제액이 잠시만 인당 952 900 합쳐서 1 905 800 근데 나중에 특가 뜰 수도 있으려나 좀 더 두고볼까 뜨기야 뜨겠지 웅웅 보니까 아시아나는 특가 이벤트 꽤 하는 것 같아서 일단 두고보장 그래 구럼 일단 자자'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4606cbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우리 비행포함 15일이야 16일 아 너 나한테 돈 보내주면 지금 할 수 잇옹 얼마야 최종 결제액이 잠시만 인당 952 900 합쳐서 1 905 800 근데 나중에 특가 뜰 수도 있으려나 좀 더 두고볼까 뜨기야 뜨겠지 웅웅 보니까 아시아나는 특가 이벤트 꽤 하는 것 같아서 일단 두고보장 그래 구럼 일단 자자'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_textlines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8e6fd",
   "metadata": {},
   "source": [
    "### 3) 메타 데이터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4540f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_textlines)):\n",
    "    temp_cat = \"#\"+val_category[i]+\"# \"\n",
    "    val_textlines[i] = temp_cat+val_textlines[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f2ac372",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_textlines)):\n",
    "    temp_cat = \"#\"+train_category[i]+\"# \"\n",
    "    train_textlines[i] = temp_cat+train_textlines[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbfea9f",
   "metadata": {},
   "source": [
    "### 4) DataFrame로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6912ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(zip(train_textlines, train_sumlines), columns=['Text', 'Summary'])\n",
    "val_df = pd.DataFrame(zip(val_textlines, val_sumlines), columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0abc6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True, drop=True)\n",
    "val_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afd42003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#상거래(쇼핑)# 그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#상거래(쇼핑)# kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#상거래(쇼핑)# 아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 ...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#상거래(쇼핑)# 칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#상거래(쇼핑)# 잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  #상거래(쇼핑)# 그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우...   \n",
       "1  #상거래(쇼핑)# kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마...   \n",
       "2  #상거래(쇼핑)# 아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 ...   \n",
       "3  #상거래(쇼핑)# 칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕...   \n",
       "4  #상거래(쇼핑)# 잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보...   \n",
       "\n",
       "                                             Summary  \n",
       "0                 비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다  \n",
       "1                 비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  \n",
       "3                칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다  \n",
       "4                   잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7925f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#상거래(쇼핑)# 그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우...</td>\n",
       "      <td>비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#상거래(쇼핑)# kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마...</td>\n",
       "      <td>비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#상거래(쇼핑)# 아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 ...</td>\n",
       "      <td>케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#상거래(쇼핑)# 칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕...</td>\n",
       "      <td>칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#상거래(쇼핑)# 잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보...</td>\n",
       "      <td>잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  #상거래(쇼핑)# 그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우...   \n",
       "1  #상거래(쇼핑)# kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마...   \n",
       "2  #상거래(쇼핑)# 아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 ...   \n",
       "3  #상거래(쇼핑)# 칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕...   \n",
       "4  #상거래(쇼핑)# 잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보...   \n",
       "\n",
       "                                             Summary  \n",
       "0                 비행기 표 가격에 대해 이야기하며 특가 이벤트를 기다리고 있다  \n",
       "1                 비염이 있어서 싸게 나온 일회용 부직포 마스크를 사두려고 한다  \n",
       "2  케이크 업체 중 중앙동 쪽은 맛만 있고 디자인은 별로고 고잔동 케이크 업체는 배달도...  \n",
       "3                칫솔을 3개월에 하나씩 바꿔서 왕 칫솔 사러 신세계 가자고 했다  \n",
       "4                   잠도 안 와서 고구마 말랭이를 양심상 하나만 먹으려고 한다  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53502956",
   "metadata": {},
   "source": [
    "## 4. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4690787f",
   "metadata": {},
   "source": [
    "### 1) dataset으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "513a0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF > data Set으로 전환\n",
    "train_data = Dataset.from_pandas(train_df) \n",
    "val_len = len(val_df) // 2\n",
    "val_data = Dataset.from_pandas(val_df[:val_len])\n",
    "test_data=Dataset.from_pandas(val_df[val_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7892fc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Text', 'Summary'],\n",
      "    num_rows: 279992\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Text', 'Summary'],\n",
      "    num_rows: 17502\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69934d6e",
   "metadata": {},
   "source": [
    "### 2) EDA 바탕으로 길이 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0648f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 128\n",
    "max_target = 32\n",
    "batch_size = 4\n",
    "ignore_index = -100# tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ee533",
   "metadata": {},
   "source": [
    "### 3) 토큰화 함수 구현 및 토큰화\n",
    "- input_ids, attention_mask, lables토큰만 구현\n",
    "- 추가로 decoder_input_ids, decoder_attention_mask 토큰화도 시도해봤으나 성능에는 차이가 없고 허깅페이스에 그에 관한 설명이 나왔있음\n",
    "    - https://huggingface.co/docs/transformers/glossary#decoder-input-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac5549bc",
   "metadata": {},
   "outputs": [],
   "source": [
    " def add_ignored_data(inputs, max_len, ignore_index):\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [ignore_index] *(max_len - len(inputs)) # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00d00dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding_data(inputs, max_len):\n",
    "        pad_index = tokenizer.pad_token_id\n",
    "        if len(inputs) < max_len:\n",
    "            pad = [pad_index] *(max_len - len(inputs))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:max_len]\n",
    "\n",
    "        return inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3052528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "    label_id= []\n",
    "    label_ids = []\n",
    "    dec_input_ids = []\n",
    "    inputs = [dialogue for dialogue in data_to_process['Text']]\n",
    "    model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n",
    "\n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_id.append(tokenizer.encode(data_to_process['Summary'][i]))  \n",
    "    for i in range(len(data_to_process['Summary'])):\n",
    "        label_id[i].append(tokenizer.eos_token_id)\n",
    "        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))\n",
    "    for i in range(len(data_to_process['Summary'])):  \n",
    "        dec_input_id = [tokenizer.eos_token_id]\n",
    "        dec_input_id += label_ids[i][:-1]\n",
    "        dec_input_ids.append(add_padding_data(dec_input_id, max_target))  \n",
    "    \n",
    "    model_inputs['labels'] = label_ids\n",
    "    model_inputs['decoder_input_ids'] = dec_input_ids\n",
    "    model_inputs['decoder_attention_mask'] = (np.array(dec_input_ids) != tokenizer.pad_token_id).astype(int) \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5e03cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2affebe3792047c98acb565725db7c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/280 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b25ef3d049c42468e72a059aa6f9022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f87ff",
   "metadata": {},
   "source": [
    "## 5. 학습을 진행하기 위한 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b9df01",
   "metadata": {},
   "source": [
    "### 1) config 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dde282fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set special tokens\n",
    "#from transformers import EncoderDecoderConfig\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 32 # 256은 쿠다 메모리 오류 생김\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 5\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 5\n",
    "#model.config.suppress_tokens = [23782, 14338, 22554, 234]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d879b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"kobart_version\": 2.0,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"length_penalty\": 2.0,\n",
       "  \"max_length\": 32,\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 5,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 5,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b17b01",
   "metadata": {},
   "source": [
    "### 2) rounge 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fa4030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return rouge.get_scores(pred_str, label_str, avg=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860ae15",
   "metadata": {},
   "source": [
    "### 3) arguments 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d96ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoint/kobart-base-v2_ft_test\",\n",
    "    num_train_epochs=1,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    logging_steps=500,\n",
    "    save_total_limit=3,\n",
    "    #evaluation_strategy = \"steps\",# step별로 2버 loss가 오르는거 아니면 계속 반복하는듯\n",
    "    #load_best_model_at_end = True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a4724",
   "metadata": {},
   "source": [
    "### 4) data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c61ba86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \\n또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\\ntokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) # 데이터 일괄 처리?\n",
    "\"\"\"\n",
    "DataCollatorForSeq2Seq 를 사용하여 예제 배치를 생성 하십시오 . \n",
    "또한 일괄 처리에서 가장 긴 요소의 길이로 텍스트와 레이블을 동적으로 채워서 균일한 길이가 되도록 합니다.\n",
    "tokenizer를 설정하여 함수 에서 텍스트를 채울 수 있지만 padding=True동적 패딩이 더 효율적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf0b9f",
   "metadata": {},
   "source": [
    "### 5) train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b77d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "   # callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0774a107",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/jx7789/anaconda3/envs/dohwan/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 279992\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 17500\n",
      "  Number of trainable parameters = 123859968\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjx7789\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jx7789/Download/koBART/wandb/run-20221129_021410-3mbrum70</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/3mbrum70\" target=\"_blank\">checkpoint/kobart-base-v2_ft_test</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17500' max='17500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17500/17500 39:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.692100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.457600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.391800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.380500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.355500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.302200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.226200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.223700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.208700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.095100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.107400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-1000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-1000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-1500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-1500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-2000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-2000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-2500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-2500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-3000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-3000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-3500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-3500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-4000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-4000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-4500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-4500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-5000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-5000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-5500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-5500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-6000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-6000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-6500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-6500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-7000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-7000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-7500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-7500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-8000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-8000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-8500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-8500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-9000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-9000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-9500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-9500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-10000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-10000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-10500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-10500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-11000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-11000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-11500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-11500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-12000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-12000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-12500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-12500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-13000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-13000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-13500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-13500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-14000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-14000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-14500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-14500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-15000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-15000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-15500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-15500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-16000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-16000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-16500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-16500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-17000\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-17000/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkpoint/kobart-base-v2_ft_test/checkpoint-17500\n",
      "Configuration saved in checkpoint/kobart-base-v2_ft_test/checkpoint-17500/config.json\n",
      "Model weights saved in checkpoint/kobart-base-v2_ft_test/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in checkpoint/kobart-base-v2_ft_test/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint/kobart-base-v2_ft_test/checkpoint-16000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17500, training_loss=3.2914645647321428, metrics={'train_runtime': 2353.221, 'train_samples_per_second': 118.982, 'train_steps_per_second': 7.437, 'total_flos': 2.134016630194176e+16, 'train_loss': 3.2914645647321428, 'epoch': 1.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eca5803a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17502\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='274' max='274' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [274/274 09:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.0616579055786133,\n",
       " 'eval_rouge-1': {'r': 0.2651331065183319,\n",
       "  'p': 0.2614331737547393,\n",
       "  'f': 0.2547780108948525},\n",
       " 'eval_rouge-2': {'r': 0.10896695624828533,\n",
       "  'p': 0.10809187663587141,\n",
       "  'f': 0.10438215873852225},\n",
       " 'eval_rouge-l': {'r': 0.2515454058906064,\n",
       "  'p': 0.24792145761848206,\n",
       "  'f': 0.2416519214839856},\n",
       " 'eval_runtime': 600.087,\n",
       " 'eval_samples_per_second': 29.166,\n",
       " 'eval_steps_per_second': 0.457,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1cd53e",
   "metadata": {},
   "source": [
    "## 6. 학습 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "835d22bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, num_beams=5,no_repeat_ngram_size=5, max_length=40,\n",
    "                            suppress_tokens= [234,23782,14338,240,199,198,161,116, 14338, 239], \n",
    "                            attention_mask=attention_mask, top_p=0.92,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "#model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)# 여기에 기본 kobart가져오기?ㅇ\n",
    "import random\n",
    "from random import randrange\n",
    "ck_num = len(val_data)\n",
    "test_samples = val_data.select(range(0, ck_num, 500))# 0, len(test_data), 200\n",
    "\n",
    "#summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1] # 여기에 체크포인트 가져오기 \n",
    "# 연구해봐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb004e64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_0 \n",
      "\n",
      "Summary after \n",
      " 영업팀 과장님이 보내줬는데 팀장님이 해줄지 모르겠고 저번에 부산 갈 때도 숙소로 엄청 싸웠다\n",
      "\n",
      "Target summary \n",
      " 팀장님이 출장 가서 머물 숙소를 계속해서 더 싼 데로 하게 한다고 이야기하고 있다\n",
      "\n",
      "Text #일과 직업# 웅 영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소로 엄청 싸워서 웅 흥 4개월가는거도아니고 3일가는데 좀해주지 거 얼마한다구 내말이 아니 해봤자 3일 다 합해도 몇만원 차이인데 너무해 그때는 2일이었는데도 만원 더 싼데에서 자꾸 자라고 넘 시러 일단\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_1 \n",
      "\n",
      "Summary after \n",
      " 헤어져도 안 할 것 같고 대리님 대기팀으로 넘어간다고 한다\n",
      "\n",
      "Target summary \n",
      " 금요일에 대리님 이사님이랑 면담했는데 이사님이 과장 달아야 하지 않겠냐고 해서 대기팀으로 넘어갈 것 같다\n",
      "\n",
      "Text #일과 직업# 헤어져도 안할듯 그치 수원에 일자리 구한다고 고 안구할거같긴하다 근데 대리님 대기팀으로 넘어갈뜻 이사랑 금욜 면담했대 이사님이 과장 달아야하지 않겠냐면서 와 근데 진심 개호구다 대기팀이랑 수질팀 유지보수 같이 들어가는 사업장 예를들어 대기두명 수질두명 들어갈일 있으면 세명만 넣는거지 대리님이 수질대기 다봐주고 그렇게 돌리고싶나봐 통합인이고\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_2 \n",
      "\n",
      "Summary after \n",
      " 내일 아르바이트 면접을 봐야 하는데 집에서 가면 너무 멀다\n",
      "\n",
      "Target summary \n",
      " 내일 채점 보조 아르바이트 면접이 4시 30분에 있다\n",
      "\n",
      "Text #일과 직업# 내일 못 먹겠다 나 알바 면접 네시반이얌 그래서 거기 가야되는데 집에서 가면 넘 멀엉 혼자 밥먹고 알바 면접보고 오겠움여 어디 면접 알바면접 뭐 채점보조 알써 잘갔다와\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_3 \n",
      "\n",
      "Summary after \n",
      " 너희 회사 잘되면 커지면 나 고용 좀 해달라고 애들 다 그 말했다고 하니 그렇게 됐으면 좋겠다고 한다\n",
      "\n",
      "Target summary \n",
      " 회사가 커지면 채용해달라고 하니까 애들이 청탁을 한다면서 제발 그렇게 잘 됐으면 좋겠다고 말한다\n",
      "\n",
      "Text #일과 직업# 너희회사 잘되면 커지면 나고용좀해주라 애들 다 그말함 제발 그렇게되게 잘 됏음 좋겠다 청탁을 나만하는게아니엇군 개웃겨 사람생각하느거 역시 똑가태 개웃기네\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_4 \n",
      "\n",
      "Summary after \n",
      " 인턴인데 발표를 왜 안 하는지 궁금해하고 있다\n",
      "\n",
      "Target summary \n",
      " 인턴 발표가 나질 않는데 포스팅은 올라와 의문을 갖는다\n",
      "\n",
      "Text #일과 직업# 근데아직도 발표안함 다들똑같은입장 강 안했나봄 발표를 왜 안하지 인턴인데 그니까 근데 포스팅은 계속 올라놈 그뭐지 담당자가다를듯 포스팅올리눈사람따로 에휴 언제까지 기다려\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_5 \n",
      "\n",
      "Summary after \n",
      " 잡코리아에서 합격 자기소개서를 볼 수 있다\n",
      "\n",
      "Target summary \n",
      " 합격 자기소개서 보다가 너네 학교 사람이 쓴 거 봤다고 하니 어디서 봤냐고 하고 잡코리아에서 봤다고 한다\n",
      "\n",
      "Text #일과 직업# 나합격자소서보다가 너네학교사람이 쓴거봄 오 어디서 잡코리아에서 합격자소서볼수있는데 너네학교도 지방4년으로나오는구낭 지방에있으니까 그렇군\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_6 \n",
      "\n",
      "Summary after \n",
      " 다이아 바로 옆 양쪽 부분이 평평한 건 없고 일자를 좋아하면 리본처럼 접히는 거 말고 접히는 거 말고 일자를 좋아해야 한다\n",
      "\n",
      "Target summary \n",
      " 다이아 옆을 리본처럼 접히는 것 말고 일자로 해 달라고 했다\n",
      "\n",
      "Text #주거와 생활# 다이아바로옆 양쪽부분 평평한건없어여 일자를 좋아하면 저렇게 리본처럼 접히는거말구 일자로 해준데 그얘기야 일자가조아여 알았어 리본처럼접히는게머지 넌 몰라\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_7 \n",
      "\n",
      "Summary after \n",
      " 카드에 한도가 있는지 체크카드에 제한이 있는지 물어본다\n",
      "\n",
      "Target summary \n",
      " 자기 경험 상 체크카드 한도가 백만 원 이상은 될 거라고 말하자 한 번 노는 것에 십만 원도 긁어본 적이 없다고 말한다\n",
      "\n",
      "Text #주거와 생활# 한도가잇나카드에근데 쪕 체카도 제한 이쓸걸 그르냐 체크는그렇구먼 한도 백이상은도 왜냐면 내가그이상써본거가터 뱅기값이구먼 난 한번에 노는거에 십마넌도 못 긁어봤어 그러치 누가그렇게긁어 타패살면인정 꺄륵 숙소나 호텔잡을때 긁자너\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_8 \n",
      "\n",
      "Summary after \n",
      " 몇 년 만에 가서 설렌다고 하니 심각한 애슐리덕구라고 한다\n",
      "\n",
      "Target summary \n",
      " 고등학교 이후로 빕스를 간 적이 없어서 오랜만에 갈 생각하니 설렌다고 얘기했다\n",
      "\n",
      "Text #주거와 생활# 빖스를몇년만에가서 설렌다 심각한애슐리덕구여꾸만 난니가부페덕구인줄알앗는데 취향이확고했었어 빕스는 고딩때이후로안간거같아 애슐리카드를보여주던 내카드에 천얼마밖에없어써 그거밖에안모여써 하 실망이다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_9 \n",
      "\n",
      "Summary after \n",
      " 전포동 1번이 좋은데 1번은 전포동 1번이다\n",
      "\n",
      "Target summary \n",
      " 두 개가 연락이 돼서 전포동과 부전동에 가보려는데 위치도 괜찮고 해서 같이 가달라고 했다\n",
      "\n",
      "Text #주거와 생활# 내일 일단 가보려고 여기 두개 연락 되서리 오 옹키옹키 올 1번 좋은데 1번은 전포동 2번은 부전동 닥 1번 위치도 괜춘 오키오키 내일 같이 가주실 아님 언니아랑 형부 쉬어서 같이 가준다는데\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_10 \n",
      "\n",
      "Summary after \n",
      " 엄마 아빠가 교회 앞에 조그맣게 해놨는데 농약을 안 쳐서 배추 밭도 있다\n",
      "\n",
      "Target summary \n",
      " 엄마와 아빠가 심은 배추에 대해 이야기한다\n",
      "\n",
      "Text #주거와 생활# 엄빠가 심은 배추 대박 밭두 잇냐 교회앞에 조그맣게 해놧는데 농약을 안쳐서 벌레가 엄청먹어서 막걸리뿌렷어 민간요법 저걸로 김장하면 되겠다 야 요즘 배추값 금값이여 두포기에 3마넌인가\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_11 \n",
      "\n",
      "Summary after \n",
      " 언니에게 지갑을 받으러 가는 곳이 어딘지 물어보고 있다\n",
      "\n",
      "Target summary \n",
      " 지갑을 받으러 가는 곳이 찾으러 가는 곳과 바로 옆으로 비슷하다\n",
      "\n",
      "Text #주거와 생활# 언니 지갑 받으러 가는곳 어딘지 봤어 응 그 종이에 써이 써있엉 찾으러 가는것 지슷한가 해서 물어본간뎅 그건 안봤나보군 힣 아 비슷해 바로 옆인가 그랬던거같아 어홍 다행이군 마쟈마쟈\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_12 \n",
      "\n",
      "Summary after \n",
      " 주식에 돈을 다 물려서 생활비도 거의 다 거지 되었다\n",
      "\n",
      "Target summary \n",
      " 주식에 돈이 다 물려서 생활비가 없다고 하니 비상금을 쓰라고 하니 그건 부모님 생신이나 어버이날에 써야 한다고 안된다고 한다\n",
      "\n",
      "Text #주거와 생활# 지금 주식에 돈 다 물려서 생활비 제로에 수렴함 흙흙 알바비도 거의다 거지돼써 신카 할부로 나가는제 이게 뭐람 울디마 바버야 집에 비상금 잇자나 앙대 그건 앵대 엄빠 생일 or 어버이날에 써야한다고\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_13 \n",
      "\n",
      "Summary after \n",
      " 다음에 폰 바꿀 때 뭘로 바꿔야 할지 추천해 주고 노트 10을 추천해 준다\n",
      "\n",
      "Target summary \n",
      " 다음에 핸드폰을 바꿀 때 무엇으로 바꿔야 할지에 대해 이야기하고 있다\n",
      "\n",
      "Text #주거와 생활# 님들 나 다음에 폰 바꿀 때 뭘로 바꿔야 할지 추천 좀 흠 좋을 것 같다 노트 10 나 20 어때 저는 갤럭시노트20 울트라가 좋은거 같아요 근데 가성비를 따져야 함 너무 비싸 하긴 ram이 12gb나 되는데 성능 보면 안비쌀수가 없지 울트라는 카메라가 튀어나온게 단점이라 추천 안해\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_14 \n",
      "\n",
      "Summary after \n",
      " 토요일에 12시에 끝나는데 홍대에 오면 46분 걸린다고 한다\n",
      "\n",
      "Target summary \n",
      " 토요일에 12시에 끝나는 데 이동에 걸리는 시간에 대해서 이야기한다\n",
      "\n",
      "Text #주거와 생활# 마쟈 나 토요일에 아마 12시에 끝나는데 언제 만날려 너 홍대오면 몇신데 46분 걸린대 고람 1시 쳐자다가 지금 일어났다 하 서울집에서 오는겨 아님 천안에서 오는겨 서울집이즤 홍대 가깝지 내가 퇴근할때 알려줄게 그치 뭐 2-30분이면\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_15 \n",
      "\n",
      "Summary after \n",
      " 택시가 오고는 있는데 아직 늦는다고 한다\n",
      "\n",
      "Target summary \n",
      " 차가 막혀서 택시가 늦게 도착해서 지각할 위기에 놓였다\n",
      "\n",
      "Text #주거와 생활# 택시타쏘 아니 아직 늦겠다 아직두 헐큰일났네 택시가오고는있는데늦넹 전화왔는데 막히나봐5분정도걸린뎅 여보도집에서바로나가야겠당 나쟈철역와쏘 나이제타고출발 언넝뛰어가\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_16 \n",
      "\n",
      "Summary after \n",
      " 공항 가는 버스를 5시 반에는 타야겠다고 하자 더 일찍 가야 하는 거 아니냐고 한다\n",
      "\n",
      "Target summary \n",
      " 공항버스를 타고 공항까지 걸리는 시간에 대해 이야기한다\n",
      "\n",
      "Text #주거와 생활# 공항가는버스 5시반에는 타야겠다 나두 더 일찍 가야되는거 아니야 공항까지 얼마나걸려 버스검색하니까 한시간반정도걸리네 흠 더일찍타야하나 한시간반씩이나걸려 비행기가 8시5분이니까 두시간 전부터 탑승수속할걸 난 7시50분뱅기야 껄껄 후 몇시에타야하지 4시 30분엔 타야할듯 난\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_17 \n",
      "\n",
      "Summary after \n",
      " 웨딩촬영을 할 때 갈 수 있으면 좋겠고 언제로 잡을 거냐고 묻자 평소에 갈 수 있으면 좋겠다고 한다\n",
      "\n",
      "Target summary \n",
      " 웨딩촬영에 오고 싶어 해서 날짜가 잡히면 알려주기로 한다\n",
      "\n",
      "Text #행사# 좋구나 웨딩촬영은 그것도 이제 알아봐야지 촬영할 때 나도 갈 수 있으면 좋겠당 그래주면 고맙징 언제로 잡을꺼야 평일 주말 음 아무래도 평일오후일거 같아 아 안돼 날짜 잡히면 알려줄게\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_18 \n",
      "\n",
      "Summary after \n",
      " 여의도 ifc몰에서 맛난 거 먹자고 연락하기로 했다\n",
      "\n",
      "Target summary \n",
      " 둘이 여의도 아이에프시몰에서 만나 맛있는 것을 먹기로 했다\n",
      "\n",
      "Text #행사# 맛난거 먹자 응 연락할겡 오키 나 가는 중 어딨어 여의도 ifc몰 오키 오 필터 써서 바꾸시오 응\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_19 \n",
      "\n",
      "Summary after \n",
      " 9일 뒤에 여행을 간다고 해서 갔다 와서 보기로 했다\n",
      "\n",
      "Target summary \n",
      " 갑자기 만난 사람이 9일 뒤에 여행을 간다고 해서 갔다가 와서 보자고 했다는 얘기를 한다\n",
      "\n",
      "Text #행사# 깩 나두 보고싶다 갑자기 만남 9일 뒤에 여행간데 그래서 갔다와서 보자했어 오오 잘쉬고오라해 8 8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_20 \n",
      "\n",
      "Summary after \n",
      " 혼자 노는 게 재미없어서 내일 대전으로 오라고 한다\n",
      "\n",
      "Target summary \n",
      " 목요일 저녁에 동탄에서 보던가 금요일 오전에 데이트를 하자고 한다\n",
      "\n",
      "Text #행사# 혼자 노는거 재미써여 재미업써영 놀자아 대전으로오시길 갈까 내일 근데 가면 올때 넘 우울인디 금욜 오전에 안바쁘시면 데이트 어떠십니깡 힣 목욜 저녁때 동탄에서 봐도대궁 다 좋습니당\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_21 \n",
      "\n",
      "Summary after \n",
      " 내일 같이 저녁을 먹자고 하는데 표정이 심각하다\n",
      "\n",
      "Target summary \n",
      " 지인과 같이 밥 먹으려 하는데 다음날 시험인데 밥 먹어도 괜찮기는 하겠다\n",
      "\n",
      "Text #행사# 내일 같이 저녇 녁 먹어도 되나염 녜에 그게 뭔소리야 표정왤케 심각해 같이 저녁먹자는데 나 저녁먹고 학원가야햄 읭 저녁 어디 정해둔 곳 있으면 그담날 시험인데 같이 밥 먹자고 너희 만나서 먹엉 아 같이 먹어도 괜찮긴 하겠다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_22 \n",
      "\n",
      "Summary after \n",
      " 저질의 상품을 팔아서 대형마트에서 대부분 저 브랜드 상품을 팔고 있다\n",
      "\n",
      "Target summary \n",
      " 대형마트에서 대부분 저 브랜드의 저질의 상품을 팔아서 갈 필요가 없고 가지 말아야겠고 인터넷에서 산다\n",
      "\n",
      "Text #상거래(쇼핑)# 아 저질의 상품을 팔다니 저긴 갈필요가없군 가지말자 대형마트에서 대부분 저 브랜드 팔아 에휴 그래서 난 인터넷에서 사\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_23 \n",
      "\n",
      "Summary after \n",
      " 어제 새벽에 너무 잠결에 핸드폰을 구매해서 오늘 아침에 핸드폰을 구매했다\n",
      "\n",
      "Target summary \n",
      " 어제 새벽에 너무 급하게 핸드폰을 구매해서 오늘 아침에 취소했다\n",
      "\n",
      "Text #상거래(쇼핑)# 어제 새벽에 나도 너무 잠결에 후다닥 처리햇나봐 핸드폰구매 우웅 그래서 오늘 아침에 후다닥 취소한듯 깜놀해써 그런거같어 그럴수이찌\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_24 \n",
      "\n",
      "Summary after \n",
      " 내가 산 슈펜 신발에 검정 스키니에 위에를 뭘 사야 잘 어울릴지 이야기한다\n",
      "\n",
      "Target summary \n",
      " 슈펜 신발과 검정 스키니에 어떤 옷이 어울릴지 이야기한다\n",
      "\n",
      "Text #상거래(쇼핑)# 내가산슈펜신발에 검정스키니에 위에를뭘사야잘어울릴까 그신발에블라우스가어울릴까 괜찮을가같은데 흰신발은 웬만하면 다 어울리잖 그렇치 잉\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_25 \n",
      "\n",
      "Summary after \n",
      " 이따가 현백에서 기다리겠다고 한다\n",
      "\n",
      "Target summary \n",
      " 피곤하지만 오늘 현대백화점에서 옷 구경하면서 기다린다고 한다\n",
      "\n",
      "Text #상거래(쇼핑)# 이따봐 현백에서 기다릴게 오잉 오늘 현백가는거여또 뀨 마쟈뀽 득템하자뀨 나 너무 피곤해 안된다 먼저가서 옷 구경하고닛어 난 옷구경 하고시픈거없어 알았어\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_26 \n",
      "\n",
      "Summary after \n",
      " 경품 이벤트 응모도 했고 1등 경품이 노트북이다\n",
      "\n",
      "Target summary \n",
      " 경품 이벤트로 1등은 노트북 2등은 노스페이스 패딩이 준비되어 있다\n",
      "\n",
      "Text #상거래(쇼핑)# 경품 이벤트 응모도 했오 엥 경품 이벤트는 뭐야 당첨되면 뭐주는데 그거 1등 경품이 노트북이고 2등이 노스페이스 패딩인가 소소한 기대 품고있다 헐 노트북 그림의떡이네 크게주네 팔백집 더맘에들어\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_27 \n",
      "\n",
      "Summary after \n",
      " 지갑은 안 왔는데 계속 진행 중이라 세 달이 걸릴 것 같다\n",
      "\n",
      "Target summary \n",
      " 지갑을 주문한지 이틀밖에 안되었으니 전화해서 물어볼 것도 없고 기다려보라고 한다\n",
      "\n",
      "Text #상거래(쇼핑)# 지갑은 안옴 그만 물어봐 이아저씨야 계속 진행중이야 세달걸리겄네 내가 전화해서 물어보까 뭘 전화해서 물어봐 주문한지 이틀됐는데 기달려라 쫌\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_28 \n",
      "\n",
      "Summary after \n",
      " 조식 가격이 너무 비싸서 내일 전화해서 취소할 것을 건의했다\n",
      "\n",
      "Target summary \n",
      " 조식을 취소하려고 했는데 취소가 안 된다고 했다고 해서 일단 전화를 해보기로 했다\n",
      "\n",
      "Text #상거래(쇼핑)# 잠등었능가 아직 - 큰일이야 조식 취소를 건의함 핫 넘 비싸성 그래서 내일 전화해서 취소해준댕 아침 잘 안먹어서 아깝댕 취소 안된다 그랬던거 같은디 그래 그래도 일단 전화해보자\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_29 \n",
      "\n",
      "Summary after \n",
      " 저번에 동영상으로 멋있게 글쓰던 게 파이였다\n",
      "\n",
      "Target summary \n",
      " 저번에 파이 만드는 동영상을 찍었던 파이는 손님 거였다\n",
      "\n",
      "Text #여가 생활# 저번에 동영상으로 멋잇게 글쓰던게 저 파이야 동영상 응 막 엄청 8배속으로 타임랩스로 아니 파이 만드는 짤 그거는 손님거엿엌 근데 내가 깔짝대면서 찍은고 아하 그 파이가 저 파이가 아니여서 다행이다 완성작 이거야 그때 그거는 헐 더 이뻐졋네\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_30 \n",
      "\n",
      "Summary after \n",
      " 새로운 것도 사야 하는데 너무 사는 것 같아서 오늘 저녁에 잘 들어보자고 한다\n",
      "\n",
      "Target summary \n",
      " 새로운 것도 사야 하니 그만 사기로 하고 오늘 저녁에 유튜브를 잘 들어보기로 했다\n",
      "\n",
      "Text #여가 생활# 근니까 우리 너무 사는것 같아 새로운것도 사야지 마자 그만 사야지 오늘 저녁에 잘 들어보자 우튜브 유트브 그래 옹\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_31 \n",
      "\n",
      "Summary after \n",
      " 원식이 다음 주 토요일에 출국해서 영상을 미리 찍은 것 같다고 한다\n",
      "\n",
      "Target summary \n",
      " 원식이가 토요일에 들어온다는 소식과 군인 신분인 차학연의 공식행사에 대해서 이야기하고 있다\n",
      "\n",
      "Text #여가 생활# 원식이 토요일인가 들어온댕 담주 오늘 출국해서 영상 미리 찍은거같다고 하긴하는뎅 담주토욜이면 24일이랑 상관없자너 그러개나말이다 뭐할거면 미리공지하고 그냥하지마 그냥 차학연너혼자해 빠수니들 보고싶을거아녀 근데 뭐 군인신분이니까 차학연혼자해 공식행사 이런건 못할겨 그래야만해\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_32 \n",
      "\n",
      "Summary after \n",
      " 케이크같이 반만 잘라도 케이크 같고 좋은 것 같다고 한다\n",
      "\n",
      "Target summary \n",
      " 상자를 원형으로 잘라서 붙이고 칠하면 괜찮을 거 같고 나뭇잎 펠트지로 만든 거를 바닥에 붙이려고 한다\n",
      "\n",
      "Text #여가 생활# 나톱들었다지금 톱 이거 반만잘라도 케이크같고 좋은거같아여 여기다가 상자를 원형으로 달라서 붙이고 칠하면 케이크될거같아유 내생각엔 이렇게만들고싶은데 나뭇잎 펠트지로 만든거 바닥에붙이고\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_33 \n",
      "\n",
      "Summary after \n",
      " 포샵만 한 시간 걸린다고 하자 수술이 아니라면 수술이라고 한다\n",
      "\n",
      "Target summary \n",
      " 포토샵이 한 시간이나 걸렸다면서 수술 아니냐고 한다\n",
      "\n",
      "Text #여가 생활# 포샵만 한시간 걸린다 인간 창조야 아니 무 수술아니가 그정도면 수술이래 미쳤나 만나면 때린다 캬캬캬캬 사진 보자\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_34 \n",
      "\n",
      "Summary after \n",
      " 선물해 준 것이 우울하고 문체가 특이해서 다음에 읽기로 했다\n",
      "\n",
      "Target summary \n",
      " 선물해 준 거 너무 우울한데 문체가 정말 특이하다며 다음에 한번 읽어보라고 말한다\n",
      "\n",
      "Text #여가 생활# 니가선물해주는거 해준거 개 우울함 근데 문체겁나특이해 나도읽어봉래 담에함읽어보셈 작가가 진자 개성대박인듯 먼가 난해한데 이미지가 떠오름\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "idx_35 \n",
      "\n",
      "Summary after \n",
      " 관리자들이 팬미팅을 찍지 말라고 잡는 중이다\n",
      "\n",
      "Target summary \n",
      " 팬미팅 관리자들이 사진 찍는 사람들에게 찍지 말라고 했다\n",
      "\n",
      "Text #여가 생활# 걸어다니고 통로에얼쩡이는 애들 머임 관리자들 팬미팅 찍지마라고 잡는중 아 아주 매의눈임 되게 다들 어려보여서 나도 찍다가 팬들인줄 걸려서 혼남 그래도 동영상 하나 건졌다\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(summaries_after_tuning)):\n",
    "    print('idx_{} '.format(i))\n",
    "  #  print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd166f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # step 50000\n",
    "# print(\"ROUGE 1 SCORE: \",rouge.compute(predictions=summaries_after_tuning, references=test_samples[\"Summary\"],\\\n",
    "#                                             rouge_types=[\"rouge1\"])[\"rouge1\"].mid)\n",
    "# print(\"ROUGE 2 SCORE: \",rouge.compute(predictions=summaries_after_tuning, references=test_samples[\"Summary\"], rouge_types=[\"rouge2\"])[\"rouge2\"].mid)\n",
    "# print(\"ROUGE L SCORE: \",rouge.compute(predictions=summaries_after_tuning, references=test_samples[\"Summary\"], rouge_types=[\"rougeL\"])[\"rougeL\"].mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11233f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
