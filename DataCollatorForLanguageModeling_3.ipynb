{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f322347",
   "metadata": {},
   "source": [
    "# DataCollatorForLanguageModeling_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830522ab",
   "metadata": {},
   "source": [
    "## 1. Import 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feacaaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a9f98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.21.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=a4017db3b710f497b8c6f48c01a7e3d055b50781e6d0024fed40b48f9e7fe026\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting datasets==1.0.2\n",
      "  Downloading datasets-1.0.2-py3-none-any.whl (1.8 MB)\n",
      "     |████████████████████████████████| 1.8 MB 6.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (0.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.26.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (2.0.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.21.4)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (6.0.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==1.0.2) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==1.0.2) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.16.0)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.14.0\n",
      "    Uninstalling datasets-1.14.0:\n",
      "      Successfully uninstalled datasets-1.14.0\n",
      "Successfully installed datasets-1.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting transformers==4.24.0\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "     |████████████████████████████████| 5.5 MB 6.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
      "     |████████████████████████████████| 182 kB 82.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "     |████████████████████████████████| 7.6 MB 62.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (1.21.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.24.0) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.24.0) (2.10)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.19\n",
      "    Uninstalling huggingface-hub-0.0.19:\n",
      "      Successfully uninstalled huggingface-hub-0.0.19\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.11.3\n",
      "    Uninstalling transformers-4.11.3:\n",
      "      Successfully uninstalled transformers-4.11.3\n",
      "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting transformer-utils\n",
      "  Downloading transformer_utils-0.1.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (0.11.2)\n",
      "Collecting colorcet\n",
      "  Downloading colorcet-3.0.1-py2.py3-none-any.whl (1.7 MB)\n",
      "     |████████████████████████████████| 1.7 MB 10.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (1.9.1+cu111)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.24.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from transformer-utils) (4.62.3)\n",
      "Collecting pyct>=0.4.4\n",
      "  Downloading pyct-0.4.8-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (3.4.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.9/site-packages (from seaborn->transformer-utils) (1.21.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->transformer-utils) (4.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (3.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->transformer-utils) (2021.11.10)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (8.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn->transformer-utils) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23->seaborn->transformer-utils) (2021.3)\n",
      "Collecting param>=1.7.0\n",
      "  Downloading param-1.12.2-py2.py3-none-any.whl (86 kB)\n",
      "     |████████████████████████████████| 86 kB 10.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers->transformer-utils) (2.0.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn->transformer-utils) (1.16.0)\n",
      "Installing collected packages: param, pyct, colorcet, transformer-utils\n",
      "Successfully installed colorcet-3.0.1 param-1.12.2 pyct-0.4.8 transformer-utils-0.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
      "     |████████████████████████████████| 1.9 MB 5.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.3)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
      "     |████████████████████████████████| 168 kB 86.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
      "     |████████████████████████████████| 182 kB 86.7 MB/s            \n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "     |████████████████████████████████| 62 kB 2.5 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "     |████████████████████████████████| 140 kB 91.1 MB/s            \n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=754ad454f5d66c8b7f387b71171d068fd96ca5d1fb7d0a198bb05ee43389a352\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built pathtools\n",
      "Installing collected packages: smmap, urllib3, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.7\n",
      "    Uninstalling urllib3-1.26.7:\n",
      "      Successfully uninstalled urllib3-1.26.7\n",
      "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.11.1 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 urllib3-1.26.13 wandb-0.13.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "!pip install datasets==1.0.2\n",
    "!pip install transformers==4.24.0\n",
    "!pip install transformer-utils\n",
    "!pip install packaging\n",
    "!pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3ab61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mecab 설치\n",
    "# !curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85985d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    LineByLineTextDataset,\n",
    "    EarlyStoppingCallback\n",
    "\n",
    ")\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import EncoderDecoderModel\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "#from konlpy.tag import Mecab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e0442a",
   "metadata": {},
   "source": [
    "## 2. 모델 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23420a1d",
   "metadata": {},
   "source": [
    "### 1) 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed769647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff99b8c3aefd4908aac7b297274dbaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5dd47c63af0400ab348191822999b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/682k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d813cd237a417c9148514fb1958ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf3bdb0074648fb99fb304e03efcaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1236f6ca105f4009a8f73da3d7b255c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoints = \"gogamza/kobart-base-v2\" #/MLM_pretrain_3ep_221121/checkpoint-33000\"#\"gogamza/kobart-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ded25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_count = 0\n",
    "# for i in model.parameters():\n",
    "#     total_count += 1\n",
    "# count= 0\n",
    "# for i in model.parameters():\n",
    "#     count += 1\n",
    "#     i.requires_grad = False\n",
    "#     if count == total_count-3:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "509e25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_params(model):\n",
    "    for par in model.parameters():\n",
    "        par.requires_grad = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5688f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_params(model.get_encoder()) ## freeze the encoder\n",
    "# dec_layers = model.get_decoder().layers\n",
    "# for i in range(2):\n",
    "#     freeze_params(dec_layers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daacf76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train params in Summarization Model : 159\n"
     ]
    }
   ],
   "source": [
    "# for i in model.parameters():\n",
    "#     print(i.requires_grad)\n",
    "train_p = [p for p in model.parameters() if p.requires_grad] \n",
    "print(f'Length of train params in Summarization Model : {len(train_p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729bf19",
   "metadata": {},
   "source": [
    "### 2) 데이터 불러오기 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a07c97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_textfile_path = \"data/train_text.csv\"\n",
    "val_textfile_path = \"data/val_text.csv\"\n",
    "\n",
    "with open(train_textfile_path, encoding=\"utf-8\") as f:\n",
    "            train_textlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]     \n",
    "\n",
    "with open(val_textfile_path, encoding=\"utf-8\") as f:\n",
    "            val_textlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c4076f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_textlines[0]\n",
    "del val_textlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02c7f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(zip(train_textlines), columns=['Text'])\n",
    "val_df = pd.DataFrame(zip(val_textlines), columns=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1fd64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True, drop=True)\n",
    "val_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cd7cf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우리 비행포함 15일...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕칫솔 또 사려나 싶...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우리 비행포함 15일...\n",
       "1  kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...\n",
       "2  아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...\n",
       "3  칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕칫솔 또 사려나 싶...\n",
       "4  잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829b15b",
   "metadata": {},
   "source": [
    "## 3. 데이터 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25b7eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab = Mecab()\n",
    "\n",
    "# def sentence_len_total(data):\n",
    "#     text_split_text = []\n",
    "#     # 반복문으로 Mecab 적용\n",
    "#     for text_sen in tqdm(data['Text'].iloc[range(0, len(data))]):\n",
    "#         text_split_text.append(mecab.morphs(text_sen))\n",
    "    \n",
    "#     temp = pd.DataFrame(zip(text_split_text), columns=['Text'])\n",
    "    \n",
    "#     # Mecab 적용 후 길이 출력\n",
    "#     text_len = temp.Text.map(len)\n",
    "    \n",
    "#     # text_len 사분위수 구하기    \n",
    "#     text_Q1 = text_len.quantile(.25)\n",
    "#     text_Q3 = text_len.quantile(.75)\n",
    "#     text_IQR = text_Q3 - text_Q1\n",
    "#     text_Q2 = text_len.quantile(.5)\n",
    "#     text_Q4 = text_len.quantile(1)\n",
    "#     text_threshold_len_left = text_Q1 - (1.5 * text_IQR)\n",
    "#     text_threshold_len_right = text_Q3 + (1.5 * text_IQR)\n",
    "\n",
    "    \n",
    "#     print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "#     print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "#     print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "#     print('텍스트의 왼쪽 울타리 범위 : {}'. format(text_threshold_len_left),\n",
    "#          '텍스트의 오른쪽 울타리 범위 : {}'. format(text_threshold_len_right))\n",
    "#     print('text_Q1 = {}'.format(text_Q1), 'headlines_Q1 = {}'.format(text_Q1))\n",
    "#     print('text_Q3 = {}'.format(text_Q3), 'headlines_Q3 = {}'.format(text_Q3))\n",
    "#     print('text_IQR = {}'.format(text_IQR), 'headlines_IQR = {}'.format(text_IQR))\n",
    "#     print('text_Q2 = {}'.format(text_Q2), 'headlines_Q2 = {}'.format(text_Q2))\n",
    "#     print('text_Q4 = {}'.format(text_Q4), 'headlines_Q4 = {}'.format(text_Q4))\n",
    "\n",
    "    \n",
    "#     plt.subplot(1,1,1)\n",
    "#     plt.boxplot(text_len)\n",
    "#     plt.title('text')\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.title('text')\n",
    "#     plt.hist(text_len, bins = 40)\n",
    "#     plt.xlabel('length of samples')\n",
    "#     plt.ylabel('number of samples')\n",
    "#     plt.show()\n",
    "    \n",
    "#     return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88c3b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_len_total(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63a6cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_len_total(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b93145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text\n",
      "0  그럼 날짜는 가격 큰 변동 없으면 6 28-7 13로 확정할까 우리 비행포함 15일...\n",
      "1  kf마스크만 5부제 하는거지 응 면마스크는 아무때나 사도될껀 면마스크말고 부직포 마...\n",
      "2  아 근데 케이크 업체들 봤는데 중앙동쪽 거기는 맛만있고 디자인은 그냥그런것같애 그러...\n",
      "3  칫솔사야하는데 쓱으로 살까 뭘 칫솔사는것까지 물어보시남 아 그 왕칫솔 또 사려나 싶...\n",
      "4  잠도안오네 얼릉 고구마츄 먹고싶단 그게 그렇게 맛있었어 아주 여보 빼이보릿 되버렸네...\n",
      "                                                Text\n",
      "0  웅 영업팀과장님이 보내줬는데 팀장님이 해줄지 모르겠다 저번에 부산갈때도 숙소로 엄청...\n",
      "1  너는 잘가라 회사 선택 잘해 알겠어 많이 힘들구나 나도 이제 이력서쓰고 영어도 해야...\n",
      "2  느낌상 대통령까지는 아니고 오시면 여사님정도오시지않을까 그러면서 샘 여기있었구낭 종...\n",
      "3  숨만수이 도 숨만쉬어도 100 이내 한달안에 일 무조건 해야대 아 딱한달 그냥 아무...\n",
      "4  목요일은 외근이구 금요일은 출장 금요일이 당진이양 아닝아닝 10일이 당진이야 그럼 ...\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())\n",
    "print(val_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bfefc6",
   "metadata": {},
   "source": [
    "## 4. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ebf89f",
   "metadata": {},
   "source": [
    "### 1) dataset으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f46838e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_pandas(train_df)\n",
    "val_data = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2696cd9",
   "metadata": {},
   "source": [
    "### 2) EDA 바탕으로 길이 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f7b71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 70# 5e128\n",
    "max_target = 70 # 5e 128\n",
    "batch_size = 4\n",
    "ignore_index = -100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba0679",
   "metadata": {},
   "source": [
    "### 3) 토큰화 함수 구현 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3cc2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ignored_data(inputs, max_len, ignore_index):\n",
    "    if len(inputs) < max_len:\n",
    "        pad = [ignore_index] *(max_len - len(inputs)) # ignore_index즉 -100으로 패딩을 만들 것인데 max_len - lne(inpu)\n",
    "        inputs = np.concatenate([inputs, pad])\n",
    "    else:\n",
    "        inputs = inputs[:max_len]\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8cc2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "    label_id= []\n",
    "    label_ids = []\n",
    "\n",
    "    inputs = [dialogue for dialogue in data_to_process['Text']]\n",
    "    model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n",
    "\n",
    "    for i in range(len(data_to_process['Text'])):\n",
    "        label_id.append(tokenizer.encode(data_to_process['Text'][i]))  \n",
    "    for i in range(len(data_to_process['Text'])):\n",
    "        label_id[i].append(tokenizer.eos_token_id)\n",
    "        label_ids.append(add_ignored_data(label_id[i], max_target, ignore_index))\n",
    "\n",
    "    model_inputs['labels'] = label_ids\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a53896f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d49da4be0194419aa77ee05f75e974b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/280 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9335e394bf4d519d64b2a7a2d5493b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenize_data = train_data.map(preprocess_data, batched = True, remove_columns=['Text'])\n",
    "val_tokenize_data = val_data.map(preprocess_data, batched = True, remove_columns=['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29223b2",
   "metadata": {},
   "source": [
    "## 5. 학습을 진행하기 위한 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d1236",
   "metadata": {},
   "source": [
    "### 1) config 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "691db2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "model.config.max_length = 70#128\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 2\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf1ef0",
   "metadata": {},
   "source": [
    "### 2) rounge 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bedef9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0acd1d4ed2423ea693560d3f1c2fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     print(\"labels_ids\",labels_ids)\n",
    "#     print(\"labels_ids[labels_ids == -100]\",labels_ids[labels_ids == -100])\n",
    "#     print(\"tokenizer.pad_token_id\",tokenizer.pad_token_id)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "    rouge_output2 = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    rouge_outputL = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rougeL\"])[\"rougeL\"].mid\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"rouge1_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge1_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge1_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "        \n",
    "        \"rouge2_precision\": round(rouge_output2.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output2.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output2.fmeasure, 4), \n",
    "        \n",
    "        \"rougeL_precision\": round(rouge_outputL.precision, 4),\n",
    "        \"rougeL_recall\": round(rouge_outputL.recall, 4),\n",
    "        \"rougeL_fmeasure\": round(rouge_outputL.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf9bf0",
   "metadata": {},
   "source": [
    "### 3) arguments 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50bf4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"MLM_pretrain_basev2_freezing\",\n",
    "    num_train_epochs=10,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,  # demo\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True, # 생성기능을 사용하고 싶다고 지정한다.\n",
    "    logging_dir=\"logs2\",\n",
    "    logging_steps=500,\n",
    "    save_total_limit=3,\n",
    "    #evaluation_strategy = \"steps\",# step별로 2버 loss가 오르는거 아니면 계속 반복하는듯\n",
    "   # load_best_model_at_end = True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0963155",
   "metadata": {},
   "source": [
    "### 4) data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f70f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e62f3f",
   "metadata": {},
   "source": [
    "### 5) train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d4babc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_tokenize_data,\n",
    "    eval_dataset=val_tokenize_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da652739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 279992\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 175000\n",
      "  Number of trainable parameters = 57501696\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /aiffel/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/Korean_Conversation_Summary/wandb/run-20221124_082034-nxo8gkka</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jx7789/huggingface/runs/nxo8gkka\" target=\"_blank\">MLM_pretrain_basev2_freezing</a></strong> to <a href=\"https://wandb.ai/jx7789/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16945' max='175000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 16945/175000 1:01:54 < 9:37:29, 4.56 it/s, Epoch 0.97/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.153500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.513200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.371700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>5.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>5.275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>5.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>5.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>5.177100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>5.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>5.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>5.122100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.085900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>5.093400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>5.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>5.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>5.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>5.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>5.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>5.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.978800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>4.973800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>4.954300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>4.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>4.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>4.988200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>4.944200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>4.922300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-1000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-1000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-1500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-1500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-2000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-2000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-2500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-2500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-3000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-3000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-3500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-3500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-4000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-4000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-4500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-4500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-5000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-5000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-5500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-5500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-6000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-6000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-6500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-6500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-7000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-7000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-7500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-7500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-8000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-8000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-8500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-8500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-9000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-9000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-9500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-9500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-10000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-10000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-10500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-10500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-11000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-11000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-11500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-11500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-12000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-12000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-12500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-12500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-13000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-13000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-13500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-13500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-14000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-14000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-14500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-14500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-15000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-15000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-15500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-15500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-16000\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-16000/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to MLM_pretrain_basev2_freezing/checkpoint-16500\n",
      "Configuration saved in MLM_pretrain_basev2_freezing/checkpoint-16500/config.json\n",
      "Model weights saved in MLM_pretrain_basev2_freezing/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in MLM_pretrain_basev2_freezing/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in MLM_pretrain_basev2_freezing/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [MLM_pretrain_basev2_freezing/checkpoint-15000] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046fe01",
   "metadata": {},
   "source": [
    "## 6. 학습 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(val_textfile_path, encoding=\"utf-8\") as f:\n",
    "            val_textlines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "        \n",
    "val_df = pd.DataFrame(zip(val_textlines), columns=['Text'])\n",
    "val_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753bdd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"Text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_target,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    \n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, num_beams=2,no_repeat_ngram_size=2, max_length=128,\n",
    "                            suppress_tokens= [234,23782,14338,240,199,198,161,116, 14338, 239], \n",
    "                             attention_mask=attention_mask, top_p=0.92)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)# 여기에 기본 kobart가져오기?\n",
    "import random\n",
    "from random import randrange\n",
    "ck_num = len(val_df)\n",
    "test_samples = val_df.select(range(0, ck_num, 1000))# 0, len(test_data), 200\n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1] # 여기에 체크포인트 가져오기 \n",
    "# 연구해봐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fe4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(summaries_after_tuning)):\n",
    "    print('idx_{} '.format(i))\n",
    "    print(\"Summary before \\n\", summaries_before_tuning[i])\n",
    "    print()\n",
    "    print(\"Summary after \\n\", summaries_after_tuning[i])\n",
    "    print()\n",
    "    print(\"Target summary \\n\", test_samples[\"Summary\"][i])\n",
    "    print()\n",
    "    print('Text', test_samples[\"Text\"][i])\n",
    "    print('-'*100)\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9f37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a139e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3de911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
